2018-07-19 14:27:43.920947: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-07-19 14:27:44.096016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 3.03GiB
2018-07-19 14:27:44.096046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
train_file:/home/syp/zwr/txtgen/examples/long_text/yahoo_data/yahoo.train.txt
valid_file:/home/syp/zwr/txtgen/examples/long_text/yahoo_data/yahoo.valid.txt
logdir:./log_dir/bsize32.epoch70.lr_c2warm16000/
step:100 source:(1, 66) loss:128.492
step:200 source:(1, 66) loss:117.832
step:300 source:(1, 45) loss:110.328
step:400 source:(1, 66) loss:86.7226
step:500 source:(1, 38) loss:88.5943
step:600 source:(1, 66) loss:94.9179
step:700 source:(1, 25) loss:81.361
step:800 source:(1, 66) loss:89.2212
step:900 source:(1, 66) loss:91.324
step:1000 source:(1, 29) loss:82.7386
step:1100 source:(1, 66) loss:86.1837
step:1200 source:(1, 38) loss:86.9617
step:1300 source:(1, 36) loss:87.5541
step:1400 source:(1, 66) loss:71.3897
step:1500 source:(1, 32) loss:89.606
step:1600 source:(1, 52) loss:83.0709
step:1700 source:(1, 66) loss:75.8777
step:1800 source:(1, 66) loss:91.6951
step:1900 source:(1, 47) loss:97.1244
step:2000 source:(1, 66) loss:69.26
step:2100 source:(1, 40) loss:93.6053
step:2200 source:(1, 23) loss:101.705
step:2300 source:(1, 59) loss:59.5527
step:2400 source:(1, 66) loss:72.6486
step:2500 source:(1, 66) loss:92.9808
step:2600 source:(1, 23) loss:59.9467
step:2700 source:(1, 27) loss:100.832
step:2800 source:(1, 63) loss:84.6638
step:2900 source:(1, 66) loss:85.9946
step:3000 source:(1, 65) loss:80.7601
step:3100 source:(1, 28) loss:85.271
step:3200 source:(1, 66) loss:84.0603
step:3300 source:(1, 46) loss:79.114
step:3400 source:(1, 66) loss:89.5897
step:3500 source:(1, 66) loss:90.4711
step:3600 source:(1, 66) loss:88.5096
step:3700 source:(1, 66) loss:70.0483
step:3800 source:(1, 66) loss:81.0552
step:3900 source:(1, 33) loss:86.1459
step:4000 source:(1, 66) loss:88.8683
step:4100 source:(1, 66) loss:93.0787
step:4200 source:(1, 66) loss:81.169
step:4300 source:(1, 66) loss:78.6377
step:4400 source:(1, 40) loss:91.765
step:4500 source:(1, 66) loss:94.0085
step:4600 source:(1, 29) loss:103.545
step:4700 source:(1, 66) loss:87.2609
step:4800 source:(1, 36) loss:79.0119
step:4900 source:(1, 66) loss:70.778
step:5000 source:(1, 28) loss:83.3005
step:5100 source:(1, 42) loss:67.4416
step:5200 source:(1, 66) loss:87.1358
step:5300 source:(1, 63) loss:86.7995
step:5400 source:(1, 34) loss:82.2792
step:5500 source:(1, 61) loss:88.4395
step:5600 source:(1, 66) loss:70.7334
step:5700 source:(1, 66) loss:74.3705
step:5800 source:(1, 30) loss:78.5132
step:5900 source:(1, 35) loss:88.1428
step:6000 source:(1, 66) loss:92.4274
step:6100 source:(1, 66) loss:71.9684
step:6200 source:(1, 24) loss:96.5173
step:6300 source:(1, 66) loss:72.2768
step:6400 source:(1, 66) loss:85.0551
step:6500 source:(1, 66) loss:64.0678
step:6600 source:(1, 66) loss:61.3573
step:6700 source:(1, 66) loss:77.4456
step:6800 source:(1, 66) loss:76.3172
step:6900 source:(1, 66) loss:62.2884
step:7000 source:(1, 61) loss:80.8569
step:7100 source:(1, 49) loss:92.8491
step:7200 source:(1, 50) loss:116.202
step:7300 source:(1, 34) loss:75.4485
step:7400 source:(1, 29) loss:78.1649
step:7500 source:(1, 22) loss:91.0346
step:7600 source:(1, 34) loss:82.9636
step:7700 source:(1, 66) loss:65.6635
step:7800 source:(1, 26) loss:84.6356
step:7900 source:(1, 51) loss:89.2038
step:8000 source:(1, 47) loss:66.8752
step:8100 source:(1, 44) loss:79.6915
step:8200 source:(1, 28) loss:77.8234
step:8300 source:(1, 66) loss:81.945
step:8400 source:(1, 66) loss:93.1217
step:8500 source:(1, 66) loss:88.4999
step:8600 source:(1, 45) loss:78.3973
step:8700 source:(1, 66) loss:69.0912
step:8800 source:(1, 36) loss:96.9691
step:8900 source:(1, 66) loss:61.1538
step:9000 source:(1, 66) loss:87.3181
step:9100 source:(1, 66) loss:69.6037
step:9200 source:(1, 66) loss:77.9027
step:9300 source:(1, 66) loss:80.6289
step:9400 source:(1, 66) loss:70.6082
step:9500 source:(1, 41) loss:87.0009
step:9600 source:(1, 33) loss:89.9301
step:9700 source:(1, 66) loss:72.2108
step:9800 source:(1, 64) loss:75.5515
step:9900 source:(1, 66) loss:70.4218
step:10000 source:(1, 66) loss:80.5056
step:10100 source:(1, 25) loss:90.8968
step:10200 source:(1, 32) loss:97.1638
step:10300 source:(1, 57) loss:85.1147
step:10400 source:(1, 66) loss:86.4655
step:10500 source:(1, 41) loss:66.3477
step:10600 source:(1, 24) loss:84.6886
step:10700 source:(1, 66) loss:78.2133
step:10800 source:(1, 33) loss:70.7464
step:10900 source:(1, 29) loss:76.3754
step:11000 source:(1, 66) loss:69.0194
step:11100 source:(1, 52) loss:80.2847
step:11200 source:(1, 44) loss:84.2262
step:11300 source:(1, 50) loss:80.6824
step:11400 source:(1, 36) loss:84.3331
step:11500 source:(1, 66) loss:90.0875
step:11600 source:(1, 66) loss:78.0354
step:11700 source:(1, 29) loss:81.5765
step:11800 source:(1, 48) loss:95.2902
step:11900 source:(1, 66) loss:83.2617
step:12000 source:(1, 66) loss:66.0189
step:12100 source:(1, 29) loss:80.702
step:12200 source:(1, 66) loss:67.9447
step:12300 source:(1, 66) loss:82.6198
step:12400 source:(1, 29) loss:97.2367
step:12500 source:(1, 66) loss:92.1209
step:12600 source:(1, 29) loss:72.5388
step:12700 source:(1, 46) loss:100.196
step:12800 source:(1, 44) loss:85.7004
step:12900 source:(1, 47) loss:103.029
step:13000 source:(1, 38) loss:66.4436
step:13100 source:(1, 66) loss:79.6451
step:13200 source:(1, 26) loss:82.6104
step:13300 source:(1, 33) loss:63.5327
step:13400 source:(1, 66) loss:70.6333
step:13500 source:(1, 66) loss:84.9381
step:13600 source:(1, 45) loss:75.3772
step:13700 source:(1, 25) loss:67.7811
step:13800 source:(1, 52) loss:69.2334
step:13900 source:(1, 66) loss:73.186
step:14000 source:(1, 66) loss:64.454
step:14100 source:(1, 66) loss:64.4199
step:14200 source:(1, 66) loss:76.5288
step:14300 source:(1, 59) loss:93.1882
step:14400 source:(1, 25) loss:86.5455
step:14500 source:(1, 56) loss:76.5217
step:14600 source:(1, 47) loss:62.9098
step:14700 source:(1, 53) loss:78.5499
step:14800 source:(1, 33) loss:83.3486
step:14900 source:(1, 66) loss:86.5201
step:15000 source:(1, 43) loss:76.3295
step:15100 source:(1, 43) loss:74.091
step:15200 source:(1, 40) loss:70.1301
step:15300 source:(1, 40) loss:78.1839
step:15400 source:(1, 66) loss:65.862
step:15500 source:(1, 66) loss:72.2637
step:15600 source:(1, 23) loss:77.9401
step:15700 source:(1, 66) loss:85.8388
step:15800 source:(1, 66) loss:101.311
step:15900 source:(1, 66) loss:78.7688
step:16000 source:(1, 41) loss:50.5584
step:16100 source:(1, 60) loss:73.3327
step:16200 source:(1, 36) loss:80.1754
step:16300 source:(1, 29) loss:87.6124
step:16400 source:(1, 56) loss:80.4537
step:16500 source:(1, 66) loss:82.421
step:16600 source:(1, 50) loss:64.1864
step:16700 source:(1, 66) loss:73.2371
step:16800 source:(1, 66) loss:86.3919
step:16900 source:(1, 66) loss:74.5502
step:17000 source:(1, 66) loss:86.1776
step:17100 source:(1, 57) loss:56.1679
step:17200 source:(1, 66) loss:77.8562
step:17300 source:(1, 26) loss:76.0316
step:17400 source:(1, 66) loss:83.4476
step:17500 source:(1, 66) loss:84.3663
step:17600 source:(1, 66) loss:89.4586
step:17700 source:(1, 38) loss:74.2334
step:17800 source:(1, 66) loss:90.0106
step:17900 source:(1, 66) loss:81.2441
step:18000 source:(1, 66) loss:85.7955
step:18100 source:(1, 66) loss:71.9182
step:18200 source:(1, 66) loss:86.4662
step:18300 source:(1, 48) loss:76.1129
step:18400 source:(1, 66) loss:61.7361
step:18500 source:(1, 66) loss:83.5457
step:18600 source:(1, 66) loss:75.8229
step:18700 source:(1, 48) loss:77.3422
step:18800 source:(1, 66) loss:62.2668
step:18900 source:(1, 66) loss:76.6612
step:19000 source:(1, 66) loss:68.4741
step:19100 source:(1, 66) loss:76.7621
step:19200 source:(1, 66) loss:71.9678
step:19300 source:(1, 28) loss:83.9307
step:19400 source:(1, 66) loss:82.5735
step:19500 source:(1, 66) loss:74.2888
step:19600 source:(1, 66) loss:74.9661
step:19700 source:(1, 26) loss:92.4059
step:19800 source:(1, 66) loss:82.3576
step:19900 source:(1, 66) loss:64.4043
step:20000 source:(1, 66) loss:78.4303
step:20100 source:(1, 60) loss:64.4659
step:20200 source:(1, 63) loss:89.5616
step:20300 source:(1, 66) loss:82.9239
step:20400 source:(1, 62) loss:75.9117
step:20500 source:(1, 42) loss:91.2573
step:20600 source:(1, 27) loss:84.1451
step:20700 source:(1, 54) loss:75.0813
step:20800 source:(1, 66) loss:70.1557
step:20900 source:(1, 54) loss:97.6283
step:21000 source:(1, 66) loss:87.3775
step:21100 source:(1, 66) loss:90.2102
step:21200 source:(1, 40) loss:89.7529
step:21300 source:(1, 55) loss:58.8037
step:21400 source:(1, 66) loss:80.3089
step:21500 source:(1, 66) loss:61.6922
step:21600 source:(1, 66) loss:85.8625
step:21700 source:(1, 66) loss:72.2055
step:21800 source:(1, 66) loss:81.3451
step:21900 source:(1, 33) loss:46.1843
step:22000 source:(1, 66) loss:65.0904
step:22100 source:(1, 66) loss:77.3575
step:22200 source:(1, 66) loss:82.7625
step:22300 source:(1, 56) loss:87.3438
step:22400 source:(1, 53) loss:60.0938
step:22500 source:(1, 24) loss:75.237
step:22600 source:(1, 66) loss:82.3256
step:22700 source:(1, 66) loss:81.9699
step:22800 source:(1, 55) loss:68.4945
step:22900 source:(1, 66) loss:72.3775
step:23000 source:(1, 66) loss:73.8551
step:23100 source:(1, 62) loss:78.3511
step:23200 source:(1, 66) loss:79.108
step:23300 source:(1, 56) loss:75.1141
step:23400 source:(1, 66) loss:76.084
step:23500 source:(1, 42) loss:91.5364
step:23600 source:(1, 44) loss:70.1698
step:23700 source:(1, 66) loss:77.0725
step:23800 source:(1, 66) loss:84.2299
step:23900 source:(1, 43) loss:74.8908
step:24000 source:(1, 66) loss:77.5506
step:24100 source:(1, 43) loss:74.2674
step:24200 source:(1, 27) loss:68.0889
step:24300 source:(1, 66) loss:85.4344
step:24400 source:(1, 50) loss:79.7824
step:24500 source:(1, 66) loss:81.1038
step:24600 source:(1, 66) loss:83.8171
step:24700 source:(1, 66) loss:65.5711
step:24800 source:(1, 66) loss:73.638
step:24900 source:(1, 60) loss:87.0366
step:25000 source:(1, 27) loss:83.0404
step:25100 source:(1, 66) loss:71.4633
step:25200 source:(1, 61) loss:68.41
step:25300 source:(1, 24) loss:87.524
step:25400 source:(1, 66) loss:78.299
step:25500 source:(1, 66) loss:69.5592
step:25600 source:(1, 66) loss:76.945
step:25700 source:(1, 66) loss:66.4758
step:25800 source:(1, 31) loss:73.2651
step:25900 source:(1, 40) loss:65.9308
step:26000 source:(1, 32) loss:74.9378
step:26100 source:(1, 47) loss:79.4414
step:26200 source:(1, 66) loss:63.3197
step:26300 source:(1, 51) loss:81.9756
step:26400 source:(1, 29) loss:61.9585
step:26500 source:(1, 49) loss:93.7911
step:26600 source:(1, 66) loss:64.4606
step:26700 source:(1, 66) loss:77.0173
step:26800 source:(1, 57) loss:79.5869
step:26900 source:(1, 56) loss:70.8152
step:27000 source:(1, 66) loss:57.144
step:27100 source:(1, 26) loss:47.2855
step:27200 source:(1, 66) loss:91.089
step:27300 source:(1, 66) loss:63.9672
step:27400 source:(1, 66) loss:73.9976
step:27500 source:(1, 56) loss:81.7581
step:27600 source:(1, 66) loss:75.0417
step:27700 source:(1, 66) loss:71.0509
step:27800 source:(1, 66) loss:60.8936
step:27900 source:(1, 66) loss:75.6039
step:28000 source:(1, 66) loss:83.3577
step:28100 source:(1, 66) loss:77.7883
step:28200 source:(1, 25) loss:77.4986
step:28300 source:(1, 51) loss:84.462
step:28400 source:(1, 64) loss:87.1608
step:28500 source:(1, 66) loss:92.9851
step:28600 source:(1, 59) loss:72.1129
step:28700 source:(1, 25) loss:74.8436
step:28800 source:(1, 36) loss:87.5586
step:28900 source:(1, 66) loss:75.189
step:29000 source:(1, 66) loss:106.831
step:29100 source:(1, 66) loss:65.6915
step:29200 source:(1, 66) loss:74.3062
step:29300 source:(1, 35) loss:81.3082
step:29400 source:(1, 28) loss:81.4525
step:29500 source:(1, 66) loss:78.7207
step:29600 source:(1, 29) loss:57.8475
step:29700 source:(1, 66) loss:77.0015
step:29800 source:(1, 66) loss:70.5404
step:29900 source:(1, 66) loss:75.2607
step:30000 source:(1, 66) loss:73.0243
step:30100 source:(1, 31) loss:74.7571
step:30200 source:(1, 50) loss:74.9854
step:30300 source:(1, 22) loss:81.2835
step:30400 source:(1, 66) loss:76.1548
step:30500 source:(1, 46) loss:73.708
step:30600 source:(1, 66) loss:70.4665
step:30700 source:(1, 53) loss:87.4945
step:30800 source:(1, 39) loss:46.2284
step:30900 source:(1, 37) loss:75.1762
step:31000 source:(1, 66) loss:70.0527
step:31100 source:(1, 39) loss:78.3919
step:31200 source:(1, 66) loss:76.7642
step:31300 source:(1, 66) loss:88.6102
step:31400 source:(1, 66) loss:75.8907
step:31500 source:(1, 66) loss:78.9333
step:31600 source:(1, 66) loss:62.1804
step:31700 source:(1, 66) loss:73.5649
step:31800 source:(1, 66) loss:80.6529
step:31900 source:(1, 32) loss:79.0991
step:32000 source:(1, 50) loss:74.1644
step:32100 source:(1, 66) loss:72.7449
step:32200 source:(1, 66) loss:68.2328
step:32300 source:(1, 66) loss:76.3243
step:32400 source:(1, 44) loss:62.0424
step:32500 source:(1, 44) loss:69.1946
step:32600 source:(1, 66) loss:67.1535
step:32700 source:(1, 66) loss:61.1807
step:32800 source:(1, 66) loss:73.0892
step:32900 source:(1, 44) loss:82.027
step:33000 source:(1, 30) loss:96.2496
step:33100 source:(1, 39) loss:74.3493
step:33200 source:(1, 56) loss:63.9223
step:33300 source:(1, 34) loss:72.4804
step:33400 source:(1, 66) loss:81.1567
step:33500 source:(1, 66) loss:59.3547
step:33600 source:(1, 56) loss:76.7017
step:33700 source:(1, 64) loss:83.6474
step:33800 source:(1, 34) loss:85.6721
step:33900 source:(1, 66) loss:90.705
step:34000 source:(1, 66) loss:78.6068
step:34100 source:(1, 66) loss:55.9872
step:34200 source:(1, 59) loss:74.3497
step:34300 source:(1, 66) loss:76.8699
step:34400 source:(1, 28) loss:70.62
step:34500 source:(1, 66) loss:73.4175
step:34600 source:(1, 66) loss:84.6913
step:34700 source:(1, 65) loss:67.91
step:34800 source:(1, 66) loss:84.1921
step:34900 source:(1, 66) loss:86.682
step:35000 source:(1, 66) loss:76.4932
step:35100 source:(1, 66) loss:92.5291
step:35200 source:(1, 29) loss:72.3569
step:35300 source:(1, 66) loss:81.7249
step:35400 source:(1, 66) loss:57.1079
step:35500 source:(1, 23) loss:62.9871
step:35600 source:(1, 58) loss:78.6648
step:35700 source:(1, 66) loss:86.2649
step:35800 source:(1, 66) loss:76.4382
step:35900 source:(1, 66) loss:69.2724
step:36000 source:(1, 66) loss:73.2225
step:36100 source:(1, 66) loss:65.3855
step:36200 source:(1, 66) loss:85.6288
step:36300 source:(1, 33) loss:64.3754
step:36400 source:(1, 40) loss:70.8749
step:36500 source:(1, 34) loss:79.0379
step:36600 source:(1, 29) loss:77.9629
step:36700 source:(1, 66) loss:86.1326
step:36800 source:(1, 66) loss:50.9485
step:36900 source:(1, 66) loss:78.9074
step:37000 source:(1, 66) loss:75.6588
step:37100 source:(1, 66) loss:64.3014
step:37200 source:(1, 33) loss:80.9467
step:37300 source:(1, 38) loss:76.312
step:37400 source:(1, 66) loss:66.0549
step:37500 source:(1, 66) loss:79.9316
step:37600 source:(1, 66) loss:82.3053
step:37700 source:(1, 44) loss:86.5362
step:37800 source:(1, 66) loss:62.9271
step:37900 source:(1, 28) loss:67.491
step:38000 source:(1, 66) loss:81.1009
step:38100 source:(1, 66) loss:81.818
step:38200 source:(1, 39) loss:75.0189
step:38300 source:(1, 66) loss:73.5583
step:38400 source:(1, 32) loss:77.0376
step:38500 source:(1, 46) loss:86.1086
step:38600 source:(1, 37) loss:75.5424
step:38700 source:(1, 66) loss:76.5874
step:38800 source:(1, 66) loss:59.0305
step:38900 source:(1, 23) loss:74.7649
step:39000 source:(1, 66) loss:80.5469
step:39100 source:(1, 53) loss:91.4899
step:39200 source:(1, 66) loss:99.3383
step:39300 source:(1, 38) loss:84.447
step:39400 source:(1, 40) loss:80.5548
step:39500 source:(1, 37) loss:77.4646
step:39600 source:(1, 66) loss:52.6222
step:39700 source:(1, 26) loss:69.244
step:39800 source:(1, 37) loss:64.261
step:39900 source:(1, 66) loss:89.973
step:40000 source:(1, 66) loss:69.7211
step:40100 source:(1, 65) loss:67.8317
step:40200 source:(1, 29) loss:78.6157
step:40300 source:(1, 66) loss:85.5147
step:40400 source:(1, 66) loss:79.5829
step:40500 source:(1, 36) loss:65.5725
step:40600 source:(1, 66) loss:80.7466
step:40700 source:(1, 42) loss:73.0728
step:40800 source:(1, 66) loss:83.8874
step:40900 source:(1, 66) loss:69.0888
step:41000 source:(1, 33) loss:64.392
step:41100 source:(1, 66) loss:60.2954
step:41200 source:(1, 66) loss:73.1909
step:41300 source:(1, 66) loss:81.3214
step:41400 source:(1, 54) loss:79.4059
step:41500 source:(1, 55) loss:58.7598
step:41600 source:(1, 66) loss:72.031
step:41700 source:(1, 66) loss:88.0789
step:41800 source:(1, 48) loss:73.0529
step:41900 source:(1, 66) loss:62.3612
step:42000 source:(1, 59) loss:76.23
step:42100 source:(1, 66) loss:67.549
step:42200 source:(1, 66) loss:87.1419
step:42300 source:(1, 66) loss:81.0518
step:42400 source:(1, 66) loss:75.7228
step:42500 source:(1, 28) loss:82.5891
step:42600 source:(1, 66) loss:65.8945
step:42700 source:(1, 66) loss:59.8397
step:42800 source:(1, 53) loss:80.9901
step:42900 source:(1, 56) loss:73.4176
step:43000 source:(1, 33) loss:69.6479
step:43100 source:(1, 66) loss:71.5371
step:43200 source:(1, 66) loss:86.6645
step:43300 source:(1, 66) loss:88.4506
step:43400 source:(1, 37) loss:63.9375
step:43500 source:(1, 32) loss:86.1728
step:43600 source:(1, 33) loss:86.8373
step:43700 source:(1, 66) loss:87.2157
step:43800 source:(1, 64) loss:82.6197
step:43900 source:(1, 54) loss:86.2809
step:44000 source:(1, 66) loss:63.7329
step:44100 source:(1, 66) loss:92.2299
step:44200 source:(1, 63) loss:78.587
step:44300 source:(1, 66) loss:66.596
step:44400 source:(1, 44) loss:73.3962
step:44500 source:(1, 53) loss:60.2879
step:44600 source:(1, 66) loss:66.5066
step:44700 source:(1, 64) loss:102.008
step:44800 source:(1, 50) loss:83.1094
step:44900 source:(1, 66) loss:78.1387
step:45000 source:(1, 66) loss:59.9448
step:45100 source:(1, 66) loss:72.0669
step:45200 source:(1, 44) loss:79.0366
step:45300 source:(1, 22) loss:75.9186
step:45400 source:(1, 66) loss:88.6776
step:45500 source:(1, 66) loss:76.416
step:45600 source:(1, 59) loss:82.7521
step:45700 source:(1, 48) loss:84.4029
step:45800 source:(1, 37) loss:80.9
step:45900 source:(1, 26) loss:79.0074
step:46000 source:(1, 66) loss:69.204
step:46100 source:(1, 66) loss:60.5566
step:46200 source:(1, 66) loss:74.9098
step:46300 source:(1, 55) loss:51.831
step:46400 source:(1, 66) loss:84.2321
step:46500 source:(1, 66) loss:62.035
step:46600 source:(1, 46) loss:77.2178
step:46700 source:(1, 66) loss:74.1855
step:46800 source:(1, 35) loss:83.7757
step:46900 source:(1, 66) loss:69.9417
step:47000 source:(1, 66) loss:68.9012
step:47100 source:(1, 66) loss:80.1857
step:47200 source:(1, 48) loss:72.2874
step:47300 source:(1, 66) loss:73.8565
step:47400 source:(1, 66) loss:71.2678
step:47500 source:(1, 26) loss:68.8457
step:47600 source:(1, 47) loss:82.1175
step:47700 source:(1, 66) loss:93.0784
step:47800 source:(1, 66) loss:64.5225
step:47900 source:(1, 38) loss:76.1534
step:48000 source:(1, 66) loss:73.0533
step:48100 source:(1, 36) loss:77.9323
step:48200 source:(1, 27) loss:74.9717
step:48300 source:(1, 66) loss:44.9077
step:48400 source:(1, 66) loss:74.6006
step:48500 source:(1, 66) loss:73.723
step:48600 source:(1, 66) loss:69.7406
step:48700 source:(1, 66) loss:95.4131
step:48800 source:(1, 37) loss:63.2217
step:48900 source:(1, 66) loss:69.2989
step:49000 source:(1, 66) loss:70.5455
step:49100 source:(1, 66) loss:94.4074
step:49200 source:(1, 59) loss:83.5235
step:49300 source:(1, 62) loss:91.0014
step:49400 source:(1, 49) loss:81.826
step:49500 source:(1, 47) loss:68.2472
step:49600 source:(1, 66) loss:74.5224
step:49700 source:(1, 44) loss:79.7199
step:49800 source:(1, 66) loss:95.3614
step:49900 source:(1, 66) loss:96.7997
step:50000 source:(1, 46) loss:73.9059
step:50100 source:(1, 54) loss:99.6722
step:50200 source:(1, 24) loss:69.8906
step:50300 source:(1, 30) loss:80.5819
step:50400 source:(1, 49) loss:73.5457
step:50500 source:(1, 66) loss:78.0078
step:50600 source:(1, 60) loss:83.4044
step:50700 source:(1, 66) loss:68.9066
step:50800 source:(1, 66) loss:75.5586
step:50900 source:(1, 53) loss:69.3044
step:51000 source:(1, 66) loss:72.9613
step:51100 source:(1, 55) loss:89.8981
step:51200 source:(1, 22) loss:84.5937
step:51300 source:(1, 60) loss:52.2048
step:51400 source:(1, 66) loss:70.3829
step:51500 source:(1, 63) loss:85.1115
step:51600 source:(1, 66) loss:68.8778
step:51700 source:(1, 66) loss:67.9304
step:51800 source:(1, 28) loss:59.8227
step:51900 source:(1, 66) loss:71.8928
step:52000 source:(1, 31) loss:94.3301
step:52100 source:(1, 66) loss:78.7353
step:52200 source:(1, 54) loss:80.8539
step:52300 source:(1, 60) loss:90.4066
step:52400 source:(1, 66) loss:79.7861
step:52500 source:(1, 33) loss:69.2637
step:52600 source:(1, 44) loss:81.0981
step:52700 source:(1, 66) loss:72.3683
step:52800 source:(1, 27) loss:69.8734
step:52900 source:(1, 66) loss:59.458
step:53000 source:(1, 53) loss:80.9131
step:53100 source:(1, 61) loss:81.5695
step:53200 source:(1, 66) loss:68.1316
step:53300 source:(1, 38) loss:81.3902
step:53400 source:(1, 43) loss:78.0219
step:53500 source:(1, 66) loss:78.5044
step:53600 source:(1, 66) loss:79.1758
step:53700 source:(1, 66) loss:75.2128
step:53800 source:(1, 33) loss:81.5327
step:53900 source:(1, 66) loss:67.4789
step:54000 source:(1, 31) loss:87.8897
step:54100 source:(1, 66) loss:74.0579
step:54200 source:(1, 66) loss:72.9198
step:54300 source:(1, 66) loss:78.1368
step:54400 source:(1, 62) loss:80.763
step:54500 source:(1, 66) loss:61.1078
step:54600 source:(1, 43) loss:77.8761
step:54700 source:(1, 66) loss:81.7449
step:54800 source:(1, 66) loss:81.359
step:54900 source:(1, 26) loss:78.9297
step:55000 source:(1, 31) loss:75.2158
step:55100 source:(1, 43) loss:85.6537
step:55200 source:(1, 66) loss:84.5872
step:55300 source:(1, 40) loss:72.5485
step:55400 source:(1, 66) loss:84.2207
step:55500 source:(1, 66) loss:101.551
step:55600 source:(1, 66) loss:67.7828
step:55700 source:(1, 22) loss:98.1807
step:55800 source:(1, 66) loss:86.9088
step:55900 source:(1, 28) loss:84.4011
step:56000 source:(1, 66) loss:88.9105
step:56100 source:(1, 66) loss:68.3973
step:56200 source:(1, 66) loss:84.4555
step:56300 source:(1, 22) loss:77.3938
step:56400 source:(1, 35) loss:92.1928
step:56500 source:(1, 66) loss:75.2689
step:56600 source:(1, 49) loss:83.6137
step:56700 source:(1, 66) loss:71.4073
step:56800 source:(1, 66) loss:95.2258
step:56900 source:(1, 44) loss:74.4394
step:57000 source:(1, 66) loss:68.6445
step:57100 source:(1, 66) loss:60.0723
step:57200 source:(1, 24) loss:76.4429
step:57300 source:(1, 25) loss:57.4667
step:57400 source:(1, 48) loss:98.2632
step:57500 source:(1, 57) loss:69.0104
step:57600 source:(1, 39) loss:57.0898
step:57700 source:(1, 66) loss:70.7691
step:57800 source:(1, 34) loss:62.7351
step:57900 source:(1, 66) loss:85.4102
step:58000 source:(1, 24) loss:68.6236
step:58100 source:(1, 53) loss:63.2753
step:58200 source:(1, 66) loss:74.8001
step:58300 source:(1, 48) loss:61.9425
step:58400 source:(1, 66) loss:78.3595
step:58500 source:(1, 59) loss:82.5056
step:58600 source:(1, 52) loss:71.0793
step:58700 source:(1, 48) loss:83.3148
step:58800 source:(1, 43) loss:88.8073
step:58900 source:(1, 66) loss:74.5882
step:59000 source:(1, 39) loss:71.0148
step:59100 source:(1, 66) loss:79.2981
step:59200 source:(1, 66) loss:65.5851
step:59300 source:(1, 66) loss:72.5573
step:59400 source:(1, 66) loss:58.1577
step:59500 source:(1, 35) loss:99.1001
step:59600 source:(1, 66) loss:89.8473
step:59700 source:(1, 32) loss:89.861
step:59800 source:(1, 66) loss:73.7307
step:59900 source:(1, 66) loss:85.9033
step:60000 source:(1, 66) loss:73.0949
step:60100 source:(1, 30) loss:91.5606
step:60200 source:(1, 41) loss:70.2521
step:60300 source:(1, 66) loss:93.227
step:60400 source:(1, 23) loss:64.6837
step:60500 source:(1, 43) loss:80.7355
step:60600 source:(1, 66) loss:83.6982
step:60700 source:(1, 43) loss:71.511
step:60800 source:(1, 54) loss:85.9803
step:60900 source:(1, 66) loss:72.9788
step:61000 source:(1, 54) loss:72.9476
step:61100 source:(1, 59) loss:76.0358
step:61200 source:(1, 66) loss:74.0963
step:61300 source:(1, 32) loss:88.4668
step:61400 source:(1, 41) loss:76.6412
step:61500 source:(1, 41) loss:65.1323
step:61600 source:(1, 66) loss:74.045
step:61700 source:(1, 25) loss:70.7119
step:61800 source:(1, 62) loss:86.609
step:61900 source:(1, 36) loss:74.9059
step:62000 source:(1, 66) loss:73.038
step:62100 source:(1, 43) loss:92.676
step:62200 source:(1, 66) loss:65.965
step:62300 source:(1, 35) loss:96.215
step:62400 source:(1, 66) loss:96.9569
step:62500 source:(1, 66) loss:77.8413
step:62600 source:(1, 48) loss:78.5843
step:62700 source:(1, 66) loss:47.341
step:62800 source:(1, 51) loss:85.9214
step:62900 source:(1, 66) loss:68.7388
step:63000 source:(1, 66) loss:73.434
step:63100 source:(1, 45) loss:83.8061
step:63200 source:(1, 33) loss:58.8329
step:63300 source:(1, 27) loss:72.2276
step:63400 source:(1, 27) loss:69.0895
step:63500 source:(1, 44) loss:67.9715
step:63600 source:(1, 66) loss:86.0776
step:63700 source:(1, 53) loss:70.1098
step:63800 source:(1, 61) loss:72.7254
step:63900 source:(1, 66) loss:75.8225
step:64000 source:(1, 66) loss:95.6526
step:64100 source:(1, 40) loss:77.1244
step:64200 source:(1, 66) loss:65.0764
step:64300 source:(1, 66) loss:71.309
step:64400 source:(1, 32) loss:91.4495
step:64500 source:(1, 66) loss:58.152
step:64600 source:(1, 46) loss:49.489
step:64700 source:(1, 57) loss:95.2533
step:64800 source:(1, 66) loss:85.3452
step:64900 source:(1, 66) loss:57.0032
step:65000 source:(1, 66) loss:62.1283
step:65100 source:(1, 66) loss:75.7336
step:65200 source:(1, 51) loss:76.7255
step:65300 source:(1, 66) loss:89.885
step:65400 source:(1, 66) loss:66.1906
step:65500 source:(1, 51) loss:64.3616
step:65600 source:(1, 66) loss:78.6565
step:65700 source:(1, 66) loss:77.7756
step:65800 source:(1, 66) loss:77.5863
step:65900 source:(1, 36) loss:84.8851
step:66000 source:(1, 57) loss:64.0842
step:66100 source:(1, 66) loss:67.0595
step:66200 source:(1, 66) loss:70.2045
step:66300 source:(1, 50) loss:62.1746
step:66400 source:(1, 66) loss:89.0167
step:66500 source:(1, 66) loss:56.8567
step:66600 source:(1, 27) loss:71.0671
step:66700 source:(1, 66) loss:87.1901
step:66800 source:(1, 66) loss:74.8284
step:66900 source:(1, 66) loss:80.5591
step:67000 source:(1, 34) loss:87.7008
step:67100 source:(1, 59) loss:72.7919
step:67200 source:(1, 32) loss:76.7904
step:67300 source:(1, 66) loss:84.4087
step:67400 source:(1, 64) loss:75.4417
step:67500 source:(1, 66) loss:95.1224
step:67600 source:(1, 66) loss:69.6287
step:67700 source:(1, 66) loss:66.2082
step:67800 source:(1, 66) loss:66.5375
step:67900 source:(1, 36) loss:78.1756
step:68000 source:(1, 66) loss:59.0044
step:68100 source:(1, 56) loss:71.2685
step:68200 source:(1, 66) loss:66.3906
step:68300 source:(1, 43) loss:95.9671
step:68400 source:(1, 39) loss:98.4647
step:68500 source:(1, 35) loss:79.8511
step:68600 source:(1, 30) loss:83.032
step:68700 source:(1, 66) loss:57.7095
step:68800 source:(1, 26) loss:55.0217
step:68900 source:(1, 66) loss:89.6936
step:69000 source:(1, 66) loss:84.6322
step:69100 source:(1, 41) loss:89.4425
step:69200 source:(1, 66) loss:80.8743
step:69300 source:(1, 62) loss:72.0554
step:69400 source:(1, 44) loss:79.4661
step:69500 source:(1, 35) loss:73.9284
step:69600 source:(1, 50) loss:86.7393
step:69700 source:(1, 66) loss:77.5141
step:69800 source:(1, 40) loss:82.3285
step:69900 source:(1, 66) loss:73.7431
step:70000 source:(1, 66) loss:77.7172
step:70100 source:(1, 36) loss:74.3139
step:70200 source:(1, 54) loss:93.709
step:70300 source:(1, 50) loss:63.2329
step:70400 source:(1, 50) loss:71.2632
step:70500 source:(1, 66) loss:70.3017
step:70600 source:(1, 66) loss:81.8983
step:70700 source:(1, 66) loss:70.1627
step:70800 source:(1, 35) loss:62.3728
step:70900 source:(1, 53) loss:78.1415
step:71000 source:(1, 36) loss:71.4365
step:71100 source:(1, 66) loss:87.3603
step:71200 source:(1, 66) loss:68.9025
step:71300 source:(1, 66) loss:61.2364
step:71400 source:(1, 66) loss:101.116
step:71500 source:(1, 66) loss:76.6445
step:71600 source:(1, 27) loss:94.6575
step:71700 source:(1, 66) loss:81.4108
step:71800 source:(1, 22) loss:68.2207
step:71900 source:(1, 66) loss:66.3057
step:72000 source:(1, 48) loss:81.3868
step:72100 source:(1, 66) loss:83.7491
step:72200 source:(1, 44) loss:75.5182
step:72300 source:(1, 30) loss:63.2128
step:72400 source:(1, 66) loss:88.8937
step:72500 source:(1, 51) loss:57.3234
step:72600 source:(1, 66) loss:79.1258
step:72700 source:(1, 66) loss:62.5514
step:72800 source:(1, 66) loss:66.6484
step:72900 source:(1, 66) loss:86.9926
step:73000 source:(1, 66) loss:74.0762
step:73100 source:(1, 66) loss:83.6697
step:73200 source:(1, 66) loss:76.1118
step:73300 source:(1, 66) loss:85.2572
step:73400 source:(1, 66) loss:83.8879
step:73500 source:(1, 66) loss:78.5315
step:73600 source:(1, 36) loss:71.9743
step:73700 source:(1, 35) loss:92.179
step:73800 source:(1, 66) loss:77.512
step:73900 source:(1, 66) loss:90.7977
step:74000 source:(1, 66) loss:73.0025
step:74100 source:(1, 58) loss:62.1389
step:74200 source:(1, 66) loss:89.8272
step:74300 source:(1, 36) loss:84.9314
step:74400 source:(1, 66) loss:64.9483
step:74500 source:(1, 66) loss:81.1775
step:74600 source:(1, 66) loss:67.9931
step:74700 source:(1, 66) loss:80.6165
step:74800 source:(1, 35) loss:75.7834
step:74900 source:(1, 66) loss:58.7295
step:75000 source:(1, 55) loss:81.5654
step:75100 source:(1, 66) loss:82.0347
step:75200 source:(1, 66) loss:88.0764
step:75300 source:(1, 66) loss:72.2187
step:75400 source:(1, 34) loss:67.7143
step:75500 source:(1, 33) loss:84.2084
step:75600 source:(1, 66) loss:75.5964
step:75700 source:(1, 22) loss:70.7507
step:75800 source:(1, 57) loss:71.6239
step:75900 source:(1, 66) loss:65.8903
step:76000 source:(1, 66) loss:73.4322
step:76100 source:(1, 54) loss:87.1604
step:76200 source:(1, 66) loss:78.6633
step:76300 source:(1, 25) loss:75.5065
step:76400 source:(1, 66) loss:71.7072
step:76500 source:(1, 35) loss:76.659
step:76600 source:(1, 66) loss:75.8747
step:76700 source:(1, 66) loss:58.5973
step:76800 source:(1, 66) loss:67.8746
step:76900 source:(1, 30) loss:80.6148
step:77000 source:(1, 25) loss:72.6376
step:77100 source:(1, 66) loss:82.4447
step:77200 source:(1, 59) loss:75.1624
step:77300 source:(1, 66) loss:68.1209
step:77400 source:(1, 44) loss:69.7147
step:77500 source:(1, 30) loss:82.3346
step:77600 source:(1, 50) loss:73.5905
step:77700 source:(1, 66) loss:62.4874
step:77800 source:(1, 36) loss:71.6055
step:77900 source:(1, 66) loss:79.8505
step:78000 source:(1, 66) loss:56.4122
step:78100 source:(1, 64) loss:65.2666
step:78200 source:(1, 23) loss:92.1609
step:78300 source:(1, 66) loss:81.8804
step:78400 source:(1, 66) loss:63.4965
step:78500 source:(1, 41) loss:84.9287
step:78600 source:(1, 66) loss:69.3703
step:78700 source:(1, 62) loss:73.1532
step:78800 source:(1, 66) loss:60.0118
step:78900 source:(1, 66) loss:77.8781
step:79000 source:(1, 59) loss:97.1918
step:79100 source:(1, 44) loss:75.2782
step:79200 source:(1, 66) loss:66.1564
step:79300 source:(1, 30) loss:64.6385
step:79400 source:(1, 59) loss:85.0637
step:79500 source:(1, 64) loss:71.3178
step:79600 source:(1, 38) loss:60.384
step:79700 source:(1, 66) loss:78.3799
step:79800 source:(1, 41) loss:77.6351
step:79900 source:(1, 49) loss:74.6111
step:80000 source:(1, 43) loss:64.9636
step:80100 source:(1, 66) loss:78.4181
step:80200 source:(1, 66) loss:57.8678
step:80300 source:(1, 34) loss:95.76
step:80400 source:(1, 35) loss:59.6256
step:80500 source:(1, 66) loss:88.1035
step:80600 source:(1, 43) loss:59.3197
step:80700 source:(1, 48) loss:81.7062
step:80800 source:(1, 66) loss:68.3586
step:80900 source:(1, 31) loss:90.0408
step:81000 source:(1, 66) loss:73.6415
step:81100 source:(1, 66) loss:75.5835
step:81200 source:(1, 24) loss:83.8584
step:81300 source:(1, 64) loss:79.2107
step:81400 source:(1, 66) loss:60.5883
step:81500 source:(1, 66) loss:65.5858
step:81600 source:(1, 63) loss:72.1139
step:81700 source:(1, 29) loss:68.0507
step:81800 source:(1, 66) loss:81.3347
step:81900 source:(1, 24) loss:75.6624
step:82000 source:(1, 39) loss:87.2682
step:82100 source:(1, 56) loss:64.3665
step:82200 source:(1, 66) loss:78.4485
step:82300 source:(1, 66) loss:79.7575
step:82400 source:(1, 23) loss:78.161
step:82500 source:(1, 66) loss:63.6329
step:82600 source:(1, 66) loss:81.1926
step:82700 source:(1, 58) loss:64.7664
step:82800 source:(1, 49) loss:85.0274
step:82900 source:(1, 27) loss:78.3023
step:83000 source:(1, 52) loss:79.4427
step:83100 source:(1, 66) loss:60.9152
step:83200 source:(1, 66) loss:72.7132
step:83300 source:(1, 36) loss:72.324
step:83400 source:(1, 66) loss:60.4251
step:83500 source:(1, 66) loss:76.1654
step:83600 source:(1, 41) loss:70.5611
step:83700 source:(1, 59) loss:67.8713
step:83800 source:(1, 66) loss:73.2516
step:83900 source:(1, 66) loss:85.6507
step:84000 source:(1, 66) loss:74.1741
step:84100 source:(1, 33) loss:70.4206
step:84200 source:(1, 66) loss:82.2392018-07-19 17:59:09.828760: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-19 17:59:09.828835: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-19 17:59:09.829329: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-19 17:59:09.829376: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-19 18:18:00.175513: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-19 18:18:00.175602: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]

step:84300 source:(1, 66) loss:80.5961
step:84400 source:(1, 41) loss:70.233
step:84500 source:(1, 66) loss:81.4498
step:84600 source:(1, 24) loss:87.877
step:84700 source:(1, 66) loss:81.375
step:84800 source:(1, 66) loss:87.0327
step:84900 source:(1, 66) loss:99.362
step:85000 source:(1, 66) loss:78.4519
step:85100 source:(1, 66) loss:58.3068
step:85200 source:(1, 60) loss:82.3316
step:85300 source:(1, 51) loss:80.2855
step:85400 source:(1, 42) loss:74.4093
step:85500 source:(1, 62) loss:60.7949
step:85600 source:(1, 65) loss:81.6055
step:85700 source:(1, 66) loss:80.5915
step:85800 source:(1, 66) loss:87.5657
step:85900 source:(1, 52) loss:55.6785
step:86000 source:(1, 66) loss:70.5279
step:86100 source:(1, 53) loss:74.1151
step:86200 source:(1, 66) loss:80.4267
step:86300 source:(1, 26) loss:82.0588
step:86400 source:(1, 66) loss:65.9953
step:86500 source:(1, 66) loss:67.0932
step:86600 source:(1, 40) loss:97.0782
step:86700 source:(1, 32) loss:91.4017
step:86800 source:(1, 23) loss:80.74
step:86900 source:(1, 66) loss:71.2792
step:87000 source:(1, 38) loss:81.3086
step:87100 source:(1, 66) loss:81.2965
step:87200 source:(1, 66) loss:72.6358
step:87300 source:(1, 23) loss:65.1455
step:87400 source:(1, 66) loss:73.6605
step:87500 source:(1, 66) loss:78.532
step:87600 source:(1, 66) loss:85.8207
step:87700 source:(1, 66) loss:92.2968
step:87800 source:(1, 66) loss:62.5647
step:87900 source:(1, 66) loss:66.6639
step:88000 source:(1, 30) loss:82.0331
step:88100 source:(1, 66) loss:72.9316
step:88200 source:(1, 66) loss:83.0477
step:88300 source:(1, 66) loss:58.5678
step:88400 source:(1, 59) loss:75.3777
step:88500 source:(1, 66) loss:95.9752
step:88600 source:(1, 45) loss:86.9816
step:88700 source:(1, 66) loss:61.4758
step:88800 source:(1, 66) loss:80.4225
step:88900 source:(1, 66) loss:93.0162
step:89000 source:(1, 33) loss:72.9718
step:89100 source:(1, 66) loss:58.8797
step:89200 source:(1, 44) loss:72.8752
step:89300 source:(1, 66) loss:55.5096
step:89400 source:(1, 23) loss:66.7309
step:89500 source:(1, 66) loss:73.6457
step:89600 source:(1, 66) loss:81.5365
step:89700 source:(1, 66) loss:79.4523
step:89800 source:(1, 58) loss:69.7919
step:89900 source:(1, 66) loss:90.578
step:90000 source:(1, 30) loss:74.3702
step:90100 source:(1, 66) loss:54.1562
step:90200 source:(1, 37) loss:65.457
step:90300 source:(1, 66) loss:67.7267
step:90400 source:(1, 66) loss:63.6733
step:90500 source:(1, 51) loss:78.1731
step:90600 source:(1, 46) loss:88.5502
step:90700 source:(1, 66) loss:85.9256
step:90800 source:(1, 66) loss:78.8897
step:90900 source:(1, 66) loss:58.105
step:91000 source:(1, 66) loss:76.273
step:91100 source:(1, 66) loss:52.014
step:91200 source:(1, 66) loss:62.3233
step:91300 source:(1, 38) loss:83.7724
step:91400 source:(1, 66) loss:62.5308
step:91500 source:(1, 25) loss:72.6081
step:91600 source:(1, 66) loss:77.6673
step:91700 source:(1, 66) loss:74.6879
step:91800 source:(1, 66) loss:66.9887
step:91900 source:(1, 57) loss:70.0979
step:92000 source:(1, 39) loss:84.019
step:92100 source:(1, 66) loss:60.854
step:92200 source:(1, 59) loss:90.9288
step:92300 source:(1, 66) loss:58.6074
step:92400 source:(1, 66) loss:86.0587
step:92500 source:(1, 59) loss:93.6981
step:92600 source:(1, 66) loss:69.3456
step:92700 source:(1, 66) loss:68.836
step:92800 source:(1, 32) loss:63.4852
step:92900 source:(1, 65) loss:56.2967
step:93000 source:(1, 22) loss:83.4096
step:93100 source:(1, 22) loss:83.6035
step:93200 source:(1, 66) loss:75.0325
step:93300 source:(1, 66) loss:71.5162
step:93400 source:(1, 66) loss:68.6767
step:93500 source:(1, 53) loss:90.3797
step:93600 source:(1, 35) loss:62.7693
step:93700 source:(1, 66) loss:82.1852
step:93800 source:(1, 58) loss:71.0316
step:93900 source:(1, 66) loss:89.088
step:94000 source:(1, 66) loss:81.7041
step:94100 source:(1, 59) loss:84.3744
step:94200 source:(1, 66) loss:75.8392
step:94300 source:(1, 66) loss:52.4758
step:94400 source:(1, 66) loss:78.7296
step:94500 source:(1, 48) loss:67.9575
step:94600 source:(1, 45) loss:85.3654
step:94700 source:(1, 66) loss:63.9065
step:94800 source:(1, 55) loss:77.4169
step:94900 source:(1, 66) loss:70.4718
step:95000 source:(1, 23) loss:79.5191
step:95100 source:(1, 48) loss:70.8001
step:95200 source:(1, 63) loss:73.6144
step:95300 source:(1, 35) loss:73.921
step:95400 source:(1, 66) loss:60.8891
step:95500 source:(1, 52) loss:57.6307
step:95600 source:(1, 66) loss:75.0021
step:95700 source:(1, 66) loss:69.9799
step:95800 source:(1, 29) loss:83.4619
step:95900 source:(1, 44) loss:92.639
step:96000 source:(1, 40) loss:63.5578
step:96100 source:(1, 66) loss:69.1191
step:96200 source:(1, 66) loss:74.6309
step:96300 source:(1, 55) loss:71.2579
step:96400 source:(1, 66) loss:60.4763
step:96500 source:(1, 66) loss:69.1498
step:96600 source:(1, 66) loss:69.4294
step:96700 source:(1, 66) loss:69.1325
step:96800 source:(1, 66) loss:66.9347
step:96900 source:(1, 52) loss:77.1995
step:97000 source:(1, 66) loss:92.6016
step:97100 source:(1, 66) loss:87.556
step:97200 source:(1, 66) loss:74.9795
step:97300 source:(1, 66) loss:97.0913
step:97400 source:(1, 66) loss:70.9711
step:97500 source:(1, 66) loss:78.2319
step:97600 source:(1, 39) loss:73.6346
step:97700 source:(1, 66) loss:64.0485
step:97800 source:(1, 66) loss:74.8668
step:97900 source:(1, 66) loss:61.699
step:98000 source:(1, 49) loss:76.6277
step:98100 source:(1, 30) loss:89.8319
step:98200 source:(1, 66) loss:64.6112
step:98300 source:(1, 66) loss:75.7131
step:98400 source:(1, 23) loss:87.3313
step:98500 source:(1, 27) loss:76.7
step:98600 source:(1, 32) loss:68.3455
step:98700 source:(1, 66) loss:56.2529
step:98800 source:(1, 66) loss:89.4906
step:98900 source:(1, 33) loss:60.5962
step:99000 source:(1, 24) loss:77.1947
step:99100 source:(1, 66) loss:87.5744
step:99200 source:(1, 33) loss:65.9337
step:99300 source:(1, 66) loss:98.4854
step:99400 source:(1, 27) loss:87.5405
step:99500 source:(1, 66) loss:82.6001
step:99600 source:(1, 37) loss:79.3006
step:99700 source:(1, 58) loss:60.6203
step:99800 source:(1, 22) loss:48.1347
step:99900 source:(1, 66) loss:76.3898
step:100000 source:(1, 58) loss:73.5153
epoch:0 eval_bleu:54.122304916381836
the 0 epoch, highest bleu 54.122305
step:100100 source:(1, 66) loss:89.8046
step:100200 source:(1, 66) loss:71.8968
step:100300 source:(1, 45) loss:74.575
step:100400 source:(1, 66) loss:74.5835
step:100500 source:(1, 38) loss:67.6794
step:100600 source:(1, 66) loss:74.0515
step:100700 source:(1, 25) loss:60.9984
step:100800 source:(1, 66) loss:64.8023
step:100900 source:(1, 66) loss:61.9778
step:101000 source:(1, 29) loss:57.6526
step:101100 source:(1, 66) loss:93.1881
step:101200 source:(1, 38) loss:91.415
step:101300 source:(1, 36) loss:75.7549
step:101400 source:(1, 66) loss:60.1042
step:101500 source:(1, 32) loss:73.0489
step:101600 source:(1, 52) loss:62.0801
step:101700 source:(1, 66) loss:73.7515
step:101800 source:(1, 66) loss:75.504
step:101900 source:(1, 47) loss:87.0606
step:102000 source:(1, 66) loss:71.4058
step:102100 source:(1, 40) loss:83.5361
step:102200 source:(1, 23) loss:100.833
step:102300 source:(1, 59) loss:67.9607
step:102400 source:(1, 66) loss:76.306
step:102500 source:(1, 66) loss:99.2269
step:102600 source:(1, 23) loss:49.5401
step:102700 source:(1, 27) loss:89.9298
step:102800 source:(1, 63) loss:76.2345
step:102900 source:(1, 66) loss:77.5229
step:103000 source:(1, 65) loss:65.0073
step:103100 source:(1, 28) loss:82.1335
step:103200 source:(1, 66) loss:75.7968
step:103300 source:(1, 46) loss:85.7234
step:103400 source:(1, 66) loss:82.5966
step:103500 source:(1, 66) loss:74.9872
step:103600 source:(1, 66) loss:77.8053
step:103700 source:(1, 66) loss:65.5587
step:103800 source:(1, 66) loss:81.5887
step:103900 source:(1, 33) loss:87.1566
step:104000 source:(1, 66) loss:79.7562
step:104100 source:(1, 66) loss:79.1966
step:104200 source:(1, 66) loss:67.0813
step:104300 source:(1, 66) loss:60.7643
step:104400 source:(1, 40) loss:73.9924
step:104500 source:(1, 66) loss:79.5587
step:104600 source:(1, 29) loss:74.759
step:104700 source:(1, 66) loss:97.6208
step:104800 source:(1, 36) loss:71.4468
step:104900 source:(1, 66) loss:70.2247
step:105000 source:(1, 28) loss:61.7813
step:105100 source:(1, 42) loss:66.6756
step:105200 source:(1, 66) loss:63.0904
step:105300 source:(1, 63) loss:76.3379
step:105400 source:(1, 34) loss:80.4847
step:105500 source:(1, 61) loss:77.616
step:105600 source:(1, 66) loss:66.8397
step:105700 source:(1, 66) loss:91.3039
step:105800 source:(1, 30) loss:68.3559
step:105900 source:(1, 35) loss:61.2876
step:106000 source:(1, 66) loss:78.5749
step:106100 source:(1, 66) loss:73.7032
step:106200 source:(1, 24) loss:100.581
step:106300 source:(1, 66) loss:69.7136
step:106400 source:(1, 66) loss:82.9739
step:106500 source:(1, 66) loss:73.4585
step:106600 source:(1, 66) loss:62.2718
step:106700 source:(1, 66) loss:81.0448
step:106800 source:(1, 66) loss:87.0539
step:106900 source:(1, 66) loss:71.8898
step:107000 source:(1, 61) loss:68.3186
step:107100 source:(1, 49) loss:77.0772
step:107200 source:(1, 50) loss:109.663
step:107300 source:(1, 34) loss:67.0977
step:107400 source:(1, 29) loss:68.1442
step:107500 source:(1, 22) loss:82.0832
step:107600 source:(1, 34) loss:76.4416
step:107700 source:(1, 66) loss:56.8997
step:107800 source:(1, 26) loss:78.0112
step:107900 source:(1, 51) loss:99.4948
step:108000 source:(1, 47) loss:70.4061
step:108100 source:(1, 44) loss:76.3599
step:108200 source:(1, 28) loss:86.4879
step:108300 source:(1, 66) loss:77.0881
step:108400 source:(1, 66) loss:55.6922
step:108500 source:(1, 66) loss:71.3651
step:108600 source:(1, 45) loss:56.4183
step:108700 source:(1, 66) loss:73.4661
step:108800 source:(1, 36) loss:85.4763
step:108900 source:(1, 66) loss:64.9427
step:109000 source:(1, 66) loss:88.2774
step:109100 source:(1, 66) loss:56.6627
step:109200 source:(1, 66) loss:69.3708
step:109300 source:(1, 66) loss:78.5541
step:109400 source:(1, 66) loss:83.9758
step:109500 source:(1, 41) loss:62.5532
step:109600 source:(1, 33) loss:87.0677
step:109700 source:(1, 66) loss:70.3449
step:109800 source:(1, 64) loss:71.2406
step:109900 source:(1, 66) loss:78.2272
step:110000 source:(1, 66) loss:71.6507
step:110100 source:(1, 25) loss:70.105
step:110200 source:(1, 32) loss:77.9065
step:110300 source:(1, 57) loss:71.9765
step:110400 source:(1, 66) loss:84.1641
step:110500 source:(1, 41) loss:67.8419
step:110600 source:(1, 24) loss:76.9789
step:110700 source:(1, 66) loss:85.342
step:110800 source:(1, 33) loss:73.9501
step:110900 source:(1, 29) loss:77.5381
step:111000 source:(1, 66) loss:65.1624
step:111100 source:(1, 52) loss:63.3763
step:111200 source:(1, 44) loss:84.5173
step:111300 source:(1, 50) loss:79.4026
step:111400 source:(1, 36) loss:71.8822
step:111500 source:(1, 66) loss:69.448
step:111600 source:(1, 66) loss:76.5209
step:111700 source:(1, 29) loss:68.7846
step:111800 source:(1, 48) loss:65.5506
step:111900 source:(1, 66) loss:65.9108
step:112000 source:(1, 66) loss:57.1732
step:112100 source:(1, 29) loss:75.1475
step:112200 source:(1, 66) loss:69.5957
step:112300 source:(1, 66) loss:66.9413
step:112400 source:(1, 29) loss:87.4767
step:112500 source:(1, 66) loss:82.799
step:112600 source:(1, 29) loss:73.2732
step:112700 source:(1, 46) loss:80.9557
step:112800 source:(1, 44) loss:84.4924
step:112900 source:(1, 47) loss:81.8491
step:113000 source:(1, 38) loss:78.4139
step:113100 source:(1, 66) loss:78.1688
step:113200 source:(1, 26) loss:77.72
step:113300 source:(1, 33) loss:66.6986
step:113400 source:(1, 66) loss:68.7814
step:113500 source:(1, 66) loss:80.6054
step:113600 source:(1, 45) loss:70.044
step:113700 source:(1, 25) loss:71.1347
step:113800 source:(1, 52) loss:71.0254
step:113900 source:(1, 66) loss:68.7426
step:114000 source:(1, 66) loss:66.0591
step:114100 source:(1, 66) loss:77.9628
step:114200 source:(1, 66) loss:56.5193
step:114300 source:(1, 59) loss:68.0915
step:114400 source:(1, 25) loss:81.5278
step:114500 source:(1, 56) loss:77.3845
step:114600 source:(1, 47) loss:78.4322
step:114700 source:(1, 53) loss:71.453
step:114800 source:(1, 33) loss:62.1859
step:114900 source:(1, 66) loss:85.7072
step:115000 source:(1, 43) loss:84.9257
step:115100 source:(1, 43) loss:64.6207
step:115200 source:(1, 40) loss:73.9748
step:115300 source:(1, 40) loss:77.5064
step:115400 source:(1, 66) loss:57.256
step:115500 source:(1, 66) loss:70.8982
step:115600 source:(1, 23) loss:69.9132
step:115700 source:(1, 66) loss:77.45
step:115800 source:(1, 66) loss:106.074
step:115900 source:(1, 66) loss:69.1987
step:116000 source:(1, 41) loss:60.2694
step:116100 source:(1, 60) loss:65.6144
step:116200 source:(1, 36) loss:95.4905
step:116300 source:(1, 29) loss:81.0726
step:116400 source:(1, 56) loss:86.3882
step:116500 source:(1, 66) loss:79.8013
step:116600 source:(1, 50) loss:89.1218
step:116700 source:(1, 66) loss:72.5345
step:116800 source:(1, 66) loss:83.548
step:116900 source:(1, 66) loss:70.3981
step:117000 source:(1, 66) loss:98.0465
step:117100 source:(1, 57) loss:75.6755
step:117200 source:(1, 66) loss:74.7146
step:117300 source:(1, 26) loss:70.1562
step:117400 source:(1, 66) loss:69.3808
step:117500 source:(1, 66) loss:71.4146
step:117600 source:(1, 66) loss:76.7897
step:117700 source:(1, 38) loss:72.568
step:117800 source:(1, 66) loss:81.1722
step:117900 source:(1, 66) loss:79.3264
step:118000 source:(1, 66) loss:77.2581
step:118100 source:(1, 66) loss:73.0631
step:118200 source:(1, 66) loss:75.689
step:118300 source:(1, 48) loss:70.9863
step:118400 source:(1, 66) loss:76.8402
step:118500 source:(1, 66) loss:71.8448
step:118600 source:(1, 66) loss:77.9896
step:118700 source:(1, 48) loss:60.3244
step:118800 source:(1, 66) loss:58.8636
step:118900 source:(1, 66) loss:84.2563
step:119000 source:(1, 66) loss:76.0291
step:119100 source:(1, 66) loss:71.0936
step:119200 source:(1, 66) loss:70.9321
step:119300 source:(1, 28) loss:77.7237
step:119400 source:(1, 66) loss:76.6171
step:119500 source:(1, 66) loss:82.1463
step:119600 source:(1, 66) loss:69.5864
step:119700 source:(1, 26) loss:73.7364
step:119800 source:(1, 66) loss:62.074
step:119900 source:(1, 66) loss:63.7459
step:120000 source:(1, 66) loss:89.9383
step:120100 source:(1, 60) loss:56.7776
step:120200 source:(1, 63) loss:90.4238
step:120300 source:(1, 66) loss:67.5592
step:120400 source:(1, 62) loss:82.7582
step:120500 source:(1, 42) loss:86.539
step:120600 source:(1, 27) loss:79.3034
step:120700 source:(1, 54) loss:68.7799
step:120800 source:(1, 66) loss:76.0864
step:120900 source:(1, 54) loss:92.9623
step:121000 source:(1, 66) loss:83.3478
step:121100 source:(1, 66) loss:77.8948
step:121200 source:(1, 40) loss:89.7881
step:121300 source:(1, 55) loss:50.6367
step:121400 source:(1, 66) loss:97.7607
step:121500 source:(1, 66) loss:81.9511
step:121600 source:(1, 66) loss:94.7867
step:121700 source:(1, 66) loss:63.7962
step:121800 source:(1, 66) loss:79.1331
step:121900 source:(1, 33) loss:69.4848
step:122000 source:(1, 66) loss:57.9994
step:122100 source:(1, 66) loss:71.8633
step:122200 source:(1, 66) loss:67.0626
step:122300 source:(1, 56) loss:79.9429
step:122400 source:(1, 53) loss:62.1984
step:122500 source:(1, 24) loss:80.0999
step:122600 source:(1, 66) loss:70.2195
step:122700 source:(1, 66) loss:95.2197
step:122800 source:(1, 55) loss:70.1642
step:122900 source:(1, 66) loss:81.95
step:123000 source:(1, 66) loss:76.8898
step:123100 source:(1, 62) loss:71.8134
step:123200 source:(1, 66) loss:92.7623
step:123300 source:(1, 56) loss:81.4078
step:123400 source:(1, 66) loss:60.4516
step:123500 source:(1, 42) loss:88.2706
step:123600 source:(1, 44) loss:66.5267
step:123700 source:(1, 66) loss:60.2903
step:123800 source:(1, 66) loss:88.6817
step:123900 source:(1, 43) loss:64.3144
step:124000 source:(1, 66) loss:88.9145
step:124100 source:(1, 43) loss:61.1374
step:124200 source:(1, 27) loss:78.0576
step:124300 source:(1, 66) loss:66.7544
step:124400 source:(1, 50) loss:68.389
step:124500 source:(1, 66) loss:64.138
step:124600 source:(1, 66) loss:73.1759
step:124700 source:(1, 66) loss:56.6033
step:124800 source:(1, 66) loss:71.7692
step:124900 source:(1, 60) loss:69.2243
step:125000 source:(1, 27) loss:83.6935
step:125100 source:(1, 66) loss:72.808
step:125200 source:(1, 61) loss:69.3858
step:125300 source:(1, 24) loss:90.6896
step:125400 source:(1, 66) loss:84.7442
step:125500 source:(1, 66) loss:88.8638
step:125600 source:(1, 66) loss:76.0023
step:125700 source:(1, 66) loss:59.6362
step:125800 source:(1, 31) loss:73.0269
step:125900 source:(1, 40) loss:81.2114
step:126000 source:(1, 32) loss:79.6324
step:126100 source:(1, 47) loss:84.6213
step:126200 source:(1, 66) loss:76.4471
step:126300 source:(1, 51) loss:80.7678
step:126400 source:(1, 29) loss:83.4045
step:126500 source:(1, 49) loss:82.7301
step:126600 source:(1, 66) loss:80.5844
step:126700 source:(1, 66) loss:63.5996
step:126800 source:(1, 57) loss:72.9086
step:126900 source:(1, 56) loss:91.2066
step:127000 source:(1, 66) loss:51.5433
step:127100 source:(1, 26) loss:47.6946
step:127200 source:(1, 66) loss:92.2876
step:127300 source:(1, 66) loss:76.5668
step:127400 source:(1, 66) loss:90.0622
step:127500 source:(1, 56) loss:71.8893
step:127600 source:(1, 66) loss:71.3841
step:127700 source:(1, 66) loss:79.2505
step:127800 source:(1, 66) loss:59.0087
step:127900 source:(1, 66) loss:73.3984
step:128000 source:(1, 66) loss:81.1636
step:128100 source:(1, 66) loss:72.2239
step:128200 source:(1, 25) loss:83.1534
step:128300 source:(1, 51) loss:64.2715
step:128400 source:(1, 64) loss:65.7543
step:128500 source:(1, 66) loss:77.1929
step:128600 source:(1, 59) loss:79.6153
step:128700 source:(1, 25) loss:82.6801
step:128800 source:(1, 36) loss:80.1356
step:128900 source:(1, 66) loss:78.8982
step:129000 source:(1, 66) loss:87.3389
step:129100 source:(1, 66) loss:65.8158
step:129200 source:(1, 66) loss:59.9604
step:129300 source:(1, 35) loss:86.7038
step:129400 source:(1, 28) loss:77.772
step:129500 source:(1, 66) loss:75.9094
step:129600 source:(1, 29) loss:60.7307
step:129700 source:(1, 66) loss:76.1732
step:129800 source:(1, 66) loss:67.1308
step:129900 source:(1, 66) loss:75.5444
step:130000 source:(1, 66) loss:78.2121
step:130100 source:(1, 31) loss:76.1643
step:130200 source:(1, 50) loss:76.0926
step:130300 source:(1, 22) loss:74.3488
step:130400 source:(1, 66) loss:84.3885
step:130500 source:(1, 46) loss:59.5403
step:130600 source:(1, 66) loss:62.2699
step:130700 source:(1, 53) loss:85.1319
step:130800 source:(1, 39) loss:51.6839
step:130900 source:(1, 37) loss:77.9675
step:131000 source:(1, 66) loss:75.9827
step:131100 source:(1, 39) loss:91.4088
step:131200 source:(1, 66) loss:70.2986
step:131300 source:(1, 66) loss:88.3242
step:131400 source:(1, 66) loss:72.7561
step:131500 source:(1, 66) loss:84.4928
step:131600 source:(1, 66) loss:71.0388
step:131700 source:(1, 66) loss:87.2131
step:131800 source:(1, 66) loss:77.3704
step:131900 source:(1, 32) loss:79.3315
step:132000 source:(1, 50) loss:78.0902
step:132100 source:(1, 66) loss:77.9513
step:132200 source:(1, 66) loss:78.9186
step:132300 source:(1, 66) loss:56.9739
step:132400 source:(1, 44) loss:60.8615
step:132500 source:(1, 44) loss:72.0217
step:132600 source:(1, 66) loss:62.5289
step:132700 source:(1, 66) loss:65.8651
step:132800 source:(1, 66) loss:69.1762
step:132900 source:(1, 44) loss:80.4133
step:133000 source:(1, 30) loss:101.406
step:133100 source:(1, 39) loss:59.2445
step:133200 source:(1, 56) loss:58.6946
step:133300 source:(1, 34) loss:66.2096
step:133400 source:(1, 66) loss:72.3047
step:133500 source:(1, 66) loss:64.1906
step:133600 source:(1, 56) loss:61.8622
step:133700 source:(1, 64) loss:70.6111
step:133800 source:(1, 34) loss:86.7243
step:133900 source:(1, 66) loss:99.5011
step:134000 source:(1, 66) loss:71.1634
step:134100 source:(1, 66) loss:53.7079
step:134200 source:(1, 59) loss:68.7983
step:134300 source:(1, 66) loss:74.9848
step:134400 source:(1, 28) loss:71.0268
step:134500 source:(1, 66) loss:56.691
step:134600 source:(1, 66) loss:105.875
step:134700 source:(1, 65) loss:61.3551
step:134800 source:(1, 66) loss:93.5685
step:134900 source:(1, 66) loss:69.6956
step:135000 source:(1, 66) loss:85.6836
step:135100 source:(1, 66) loss:88.3906
step:135200 source:(1, 29) loss:71.6195
step:135300 source:(1, 66) loss:64.2712
step:135400 source:(1, 66) loss:70.7059
step:135500 source:(1, 23) loss:54.6498
step:135600 source:(1, 58) loss:84.5046
step:135700 source:(1, 66) loss:72.441
step:135800 source:(1, 66) loss:77.4008
step:135900 source:(1, 66) loss:72.9539
step:136000 source:(1, 66) loss:70.7862
step:136100 source:(1, 66) loss:63.006
step:136200 source:(1, 66) loss:95.7596
step:136300 source:(1, 33) loss:64.9864
step:136400 source:(1, 40) loss:58.6173
step:136500 source:(1, 34) loss:76.3105
step:136600 source:(1, 29) loss:88.0913
step:136700 source:(1, 66) loss:73.7442
step:136800 source:(1, 66) loss:64.0402
step:136900 source:(1, 66) loss:80.983
step:137000 source:(1, 66) loss:76.2092
step:137100 source:(1, 66) loss:59.3634
step:137200 source:(1, 33) loss:83.4756
step:137300 source:(1, 38) loss:79.3101
step:137400 source:(1, 66) loss:74.6802
step:137500 source:(1, 66) loss:73.1195
step:137600 source:(1, 66) loss:73.8045
step:137700 source:(1, 44) loss:82.3752
step:137800 source:(1, 66) loss:88.633
step:137900 source:(1, 28) loss:70.2937
step:138000 source:(1, 66) loss:98.5383
step:138100 source:(1, 66) loss:80.1179
step:138200 source:(1, 39) loss:68.0624
step:138300 source:(1, 66) loss:64.6792
step:138400 source:(1, 32) loss:71.1871
step:138500 source:(1, 46) loss:81.1327
step:138600 source:(1, 37) loss:100.042
step:138700 source:(1, 66) loss:78.9756
step:138800 source:(1, 66) loss:73.7137
step:138900 source:(1, 23) loss:77.8523
step:139000 source:(1, 66) loss:65.5748
step:139100 source:(1, 53) loss:78.0865
step:139200 source:(1, 66) loss:87.5231
step:139300 source:(1, 38) loss:86.6664
step:139400 source:(1, 40) loss:81.2218
step:139500 source:(1, 37) loss:78.9938
step:139600 source:(1, 66) loss:62.4317
step:139700 source:(1, 26) loss:72.9206
step:139800 source:(1, 37) loss:74.4094
step:139900 source:(1, 66) loss:82.7939
step:140000 source:(1, 66) loss:75.9992
step:140100 source:(1, 65) loss:68.7821
step:140200 source:(1, 29) loss:99.2635
step:140300 source:(1, 66) loss:77.4169
step:140400 source:(1, 66) loss:83.4957
step:140500 source:(1, 36) loss:55.1959
step:140600 source:(1, 66) loss:71.8498
step:140700 source:(1, 42) loss:82.6511
step:140800 source:(1, 66) loss:87.6364
step:140900 source:(1, 66) loss:67.5302
step:141000 source:(1, 33) loss:68.7148
step:141100 source:(1, 66) loss:67.9476
step:141200 source:(1, 66) loss:85.3682
step:141300 source:(1, 66) loss:68.6777
step:141400 source:(1, 54) loss:101.213
step:141500 source:(1, 55) loss:55.0606
step:141600 source:(1, 66) loss:80.573
step:141700 source:(1, 66) loss:89.619
step:141800 source:(1, 48) loss:76.9939
step:141900 source:(1, 66) loss:60.8371
step:142000 source:(1, 59) loss:82.8396
step:142100 source:(1, 66) loss:61.4542
step:142200 source:(1, 66) loss:76.2001
step:142300 source:(1, 66) loss:76.6694
step:142400 source:(1, 66) loss:62.0344
step:142500 source:(1, 28) loss:69.7714
step:142600 source:(1, 66) loss:77.0559
step:142700 source:(1, 66) loss:67.0543
step:142800 source:(1, 53) loss:71.6806
step:142900 source:(1, 56) loss:92.7371
step:143000 source:(1, 33) loss:75.8603
step:143100 source:(1, 66) loss:66.9858
step:143200 source:(1, 66) loss:61.8273
step:143300 source:(1, 66) loss:74.8791
step:143400 source:(1, 37) loss:54.8233
step:143500 source:(1, 32) loss:66.4014
step:143600 source:(1, 33) loss:63.7984
step:143700 source:(1, 66) loss:80.5254
step:143800 source:(1, 64) loss:84.2914
step:143900 source:(1, 54) loss:72.2305
step:144000 source:(1, 66) loss:71.3593
step:144100 source:(1, 66) loss:70.7789
step:144200 source:(1, 63) loss:74.5605
step:144300 source:(1, 66) loss:70.5248
step:144400 source:(1, 44) loss:72.3766
step:144500 source:(1, 53) loss:54.3362
step:144600 source:(1, 66) loss:68.6526
step:144700 source:(1, 64) loss:76.2765
step:144800 source:(1, 50) loss:78.7583
step:144900 source:(1, 66) loss:80.127
step:145000 source:(1, 66) loss:72.6469
step:145100 source:(1, 66) loss:77.1886
step:145200 source:(1, 44) loss:74.1332
step:145300 source:(1, 22) loss:71.9164
step:145400 source:(1, 66) loss:65.4438
step:145500 source:(1, 66) loss:76.1057
step:145600 source:(1, 59) loss:76.3793
step:145700 source:(1, 48) loss:44.0471
step:145800 source:(1, 37) loss:75.2012
step:145900 source:(1, 26) loss:74.7867
step:146000 source:(1, 66) loss:64.7498
step:146100 source:(1, 66) loss:60.0175
step:146200 source:(1, 66) loss:73.2053
step:146300 source:(1, 55) loss:68.5678
step:146400 source:(1, 66) loss:69.7772
step:146500 source:(1, 66) loss:73.5537
step:146600 source:(1, 46) loss:59.9388
step:146700 source:(1, 66) loss:83.3134
step:146800 source:(1, 35) loss:89.1224
step:146900 source:(1, 66) loss:61.8368
step:147000 source:(1, 66) loss:75.3536
step:147100 source:(1, 66) loss:77.49
step:147200 source:(1, 48) loss:64.9739
step:147300 source:(1, 66) loss:82.6167
step:147400 source:(1, 66) loss:66.2379
step:147500 source:(1, 26) loss:67.1567
step:147600 source:(1, 47) loss:93.9766
step:147700 source:(1, 66) loss:89.6442
step:147800 source:(1, 66) loss:74.1165
step:147900 source:(1, 38) loss:74.2456
step:148000 source:(1, 66) loss:68.259
step:148100 source:(1, 36) loss:83.0008
step:148200 source:(1, 27) loss:69.0828
step:148300 source:(1, 66) loss:66.8554
step:148400 source:(1, 66) loss:76.9335
step:148500 source:(1, 66) loss:75.8026
step:148600 source:(1, 66) loss:77.0183
step:148700 source:(1, 66) loss:69.7557
step:148800 source:(1, 37) loss:57.545
step:148900 source:(1, 66) loss:53.8363
step:149000 source:(1, 66) loss:77.5984
step:149100 source:(1, 66) loss:79.0411
step:149200 source:(1, 59) loss:60.5233
step:149300 source:(1, 62) loss:103.476
step:149400 source:(1, 49) loss:83.7287
step:149500 source:(1, 47) loss:65.53
step:149600 source:(1, 66) loss:82.7253
step:149700 source:(1, 44) loss:83.8975
step:149800 source:(1, 66) loss:72.1028
step:149900 source:(1, 66) loss:97.9771
step:150000 source:(1, 46) loss:75.1396
step:150100 source:(1, 54) loss:78.4495
step:150200 source:(1, 24) loss:73.4586
step:150300 source:(1, 30) loss:68.454
step:150400 source:(1, 49) loss:78.8292
step:150500 source:(1, 66) loss:82.3806
step:150600 source:(1, 60) loss:91.1495
step:150700 source:(1, 66) loss:80.2459
step:150800 source:(1, 66) loss:81.8128
step:150900 source:(1, 53) loss:66.1186
step:151000 source:(1, 66) loss:63.0752
step:151100 source:(1, 55) loss:82.0401
step:151200 source:(1, 22) loss:84.673
step:151300 source:(1, 60) loss:68.8146
step:151400 source:(1, 66) loss:68.1376
step:151500 source:(1, 63) loss:84.0505
step:151600 source:(1, 66) loss:80.5894
step:151700 source:(1, 66) loss:79.1213
step:151800 source:(1, 28) loss:59.7793
step:151900 source:(1, 66) loss:74.0132
step:152000 source:(1, 31) loss:77.8832
step:152100 source:(1, 66) loss:81.7674
step:152200 source:(1, 54) loss:80.0585
step:152300 source:(1, 60) loss:82.7564
step:152400 source:(1, 66) loss:69.5269
step:152500 source:(1, 33) loss:67.4138
step:152600 source:(1, 44) loss:82.2584
step:152700 source:(1, 66) loss:86.7262
step:152800 source:(1, 27) loss:68.1102
step:152900 source:(1, 66) loss:74.1869
step:153000 source:(1, 53) loss:73.1583
step:153100 source:(1, 61) loss:75.5307
step:153200 source:(1, 66) loss:76.0734
step:153300 source:(1, 38) loss:87.504
step:153400 source:(1, 43) loss:69.1632
step:153500 source:(1, 66) loss:87.2992
step:153600 source:(1, 66) loss:88.354
step:153700 source:(1, 66) loss:70.9691
step:153800 source:(1, 33) loss:68.3339
step:153900 source:(1, 66) loss:51.7211
step:154000 source:(1, 31) loss:83.1077
step:154100 source:(1, 66) loss:85.4088
step:154200 source:(1, 66) loss:48.5379
step:154300 source:(1, 66) loss:70.8278
step:154400 source:(1, 62) loss:72.3786
step:154500 source:(1, 66) loss:70.7901
step:154600 source:(1, 43) loss:74.5299
step:154700 source:(1, 66) loss:78.2228
step:154800 source:(1, 66) loss:86.3971
step:154900 source:(1, 26) loss:82.3162
step:155000 source:(1, 31) loss:71.4111
step:155100 source:(1, 43) loss:78.1823
step:155200 source:(1, 66) loss:77.0068
step:155300 source:(1, 40) loss:72.1092
step:155400 source:(1, 66) loss:89.256
step:155500 source:(1, 66) loss:89.2945
step:155600 source:(1, 66) loss:78.4551
step:155700 source:(1, 22) loss:94.4081
step:155800 source:(1, 66) loss:102.869
step:155900 source:(1, 28) loss:73.5733
step:156000 source:(1, 66) loss:84.0109
step:156100 source:(1, 66) loss:81.6996
step:156200 source:(1, 66) loss:72.0323
step:156300 source:(1, 22) loss:73.1023
step:156400 source:(1, 35) loss:74.8466
step:156500 source:(1, 66) loss:76.9767
step:156600 source:(1, 49) loss:82.7735
step:156700 source:(1, 66) loss:72.599
step:156800 source:(1, 66) loss:94.0277
step:156900 source:(1, 44) loss:71.811
step:157000 source:(1, 66) loss:63.5613
step:157100 source:(1, 66) loss:64.2379
step:157200 source:(1, 24) loss:76.6629
step:157300 source:(1, 25) loss:64.6695
step:157400 source:(1, 48) loss:69.3285
step:157500 source:(1, 57) loss:60.4429
step:157600 source:(1, 39) loss:76.8823
step:157700 source:(1, 66) loss:74.3634
step:157800 source:(1, 34) loss:61.6845
step:157900 source:(1, 66) loss:87.3893
step:158000 source:(1, 24) loss:60.4895
step:158100 source:(1, 53) loss:78.6819
step:158200 source:(1, 66) loss:81.5199
step:158300 source:(1, 48) loss:59.9389
step:158400 source:(1, 66) loss:77.8097
step:158500 source:(1, 59) loss:86.4478
step:158600 source:(1, 52) loss:61.1648
step:158700 source:(1, 48) loss:79.2014
step:158800 source:(1, 43) loss:75.5229
step:158900 source:(1, 66) loss:81.8656
step:159000 source:(1, 39) loss:77.9902
step:159100 source:(1, 66) loss:74.561
step:159200 source:(1, 66) loss:80.498
step:159300 source:(1, 66) loss:67.6389
step:159400 source:(1, 66) loss:73.1178
step:159500 source:(1, 35) loss:112.534
step:159600 source:(1, 66) loss:70.2137
step:159700 source:(1, 32) loss:78.8485
step:159800 source:(1, 66) loss:63.6188
step:159900 source:(1, 66) loss:64.4296
step:160000 source:(1, 66) loss:77.3369
step:160100 source:(1, 30) loss:74.0441
step:160200 source:(1, 41) loss:60.639
step:160300 source:(1, 66) loss:84.3243
step:160400 source:(1, 23) loss:70.7193
step:160500 source:(1, 43) loss:79.7531
step:160600 source:(1, 66) loss:91.4786
step:160700 source:(1, 43) loss:92.9355
step:160800 source:(1, 54) loss:71.2086
step:160900 source:(1, 66) loss:72.8385
step:161000 source:(1, 54) loss:67.9317
step:161100 source:(1, 59) loss:67.487
step:161200 source:(1, 66) loss:80.8385
step:161300 source:(1, 32) loss:67.2547
step:161400 source:(1, 41) loss:78.4691
step:161500 source:(1, 41) loss:83.375
step:161600 source:(1, 66) loss:79.8144
step:161700 source:(1, 25) loss:79.182
step:161800 source:(1, 62) loss:80.3397
step:161900 source:(1, 36) loss:91.2236
step:162000 source:(1, 66) loss:68.3122
step:162100 source:(1, 43) loss:75.7449
step:162200 source:(1, 66) loss:82.1879
step:162300 source:(1, 35) loss:67.7595
step:162400 source:(1, 66) loss:76.1864
step:162500 source:(1, 66) loss:74.7151
step:162600 source:(1, 48) loss:75.0769
step:162700 source:(1, 66) loss:45.8884
step:162800 source:(1, 51) loss:51.2379
step:162900 source:(1, 66) loss:59.3494
step:163000 source:(1, 66) loss:88.713
step:163100 source:(1, 45) loss:78.9932
step:163200 source:(1, 33) loss:59.1304
step:163300 source:(1, 27) loss:73.7769
step:163400 source:(1, 27) loss:73.3446
step:163500 source:(1, 44) loss:83.6635
step:163600 source:(1, 66) loss:72.8231
step:163700 source:(1, 53) loss:73.6576
step:163800 source:(1, 61) loss:77.4266
step:163900 source:(1, 66) loss:73.5208
step:164000 source:(1, 66) loss:81.9231
step:164100 source:(1, 40) loss:69.7515
step:164200 source:(1, 66) loss:79.1285
step:164300 source:(1, 66) loss:66.8587
step:164400 source:(1, 32) loss:83.228
step:164500 source:(1, 66) loss:60.8277
step:164600 source:(1, 46) loss:49.2427
step:164700 source:(1, 57) loss:90.9855
step:164800 source:(1, 66) loss:76.2856
step:164900 source:(1, 66) loss:77.7899
step:165000 source:(1, 66) loss:60.4906
step:165100 source:(1, 66) loss:73.5375
step:165200 source:(1, 51) loss:65.6287
step:165300 source:(1, 66) loss:80.5418
step:165400 source:(1, 66) loss:66.3791
step:165500 source:(1, 51) loss:67.2058
step:165600 source:(1, 66) loss:85.4574
step:165700 source:(1, 66) loss:80.7514
step:165800 source:(1, 66) loss:69.817
step:165900 source:(1, 36) loss:77.0143
step:166000 source:(1, 57) loss:70.6262
step:166100 source:(1, 66) loss:77.8356
step:166200 source:(1, 66) loss:73.3122
step:166300 source:(1, 50) loss:65.7644
step:166400 source:(1, 66) loss:94.3335
step:166500 source:(1, 66) loss:59.9284
step:166600 source:(1, 27) loss:85.6911
step:166700 source:(1, 66) loss:68.2537
step:166800 source:(1, 66) loss:62.9959
step:166900 source:(1, 66) loss:70.4608
step:167000 source:(1, 34) loss:79.0506
step:167100 source:(1, 59) loss:83.1784
step:167200 source:(1, 32) loss:73.9515
step:167300 source:(1, 66) loss:74.8233
step:167400 source:(1, 64) loss:65.4886
step:167500 source:(1, 66) loss:85.0746
step:167600 source:(1, 66) loss:77.4042
step:167700 source:(1, 66) loss:68.201
step:167800 source:(1, 66) loss:57.4738
step:167900 source:(1, 36) loss:72.2795
step:168000 source:(1, 66) loss:59.7327
step:168100 source:(1, 56) loss:87.5912
step:168200 source:(1, 66) loss:78.774
step:168300 source:(1, 43) loss:84.5249
step:168400 source:(1, 39) loss:88.8311
step:168500 source:(1, 35) loss:70.5045
step:168600 source:(1, 30) loss:80.206
step:168700 source:(1, 66) loss:69.2073
step:168800 source:(1, 26) loss:53.2285
step:168900 source:(1, 66) loss:81.0914
step:169000 source:(1, 66) loss:88.7023
step:169100 source:(1, 41) loss:86.4754
step:169200 source:(1, 66) loss:75.0179
step:169300 source:(1, 62) loss:63.3794
step:169400 source:(1, 44) loss:76.7787
step:169500 source:(1, 35) loss:66.8366
step:169600 source:(1, 50) loss:91.7188
step:169700 source:(1, 66) loss:58.5334
step:169800 source:(1, 40) loss:83.813
step:169900 source:(1, 66) loss:74.7808
step:170000 source:(1, 66) loss:72.0533
step:170100 source:(1, 36) loss:55.2005
step:170200 source:(1, 54) loss:84.3746
step:170300 source:(1, 50) loss:55.8437
step:170400 source:(1, 50) loss:58.6458
step:170500 source:(1, 66) loss:75.1665
step:170600 source:(1, 66) loss:74.0191
step:170700 source:(1, 66) loss:76.6334
step:170800 source:(1, 35) loss:77.4077
step:170900 source:(1, 53) loss:87.7595
step:171000 source:(1, 36) loss:73.8633
step:171100 source:(1, 66) loss:97.1946
step:171200 source:(1, 66) loss:88.9238
step:171300 source:(1, 66) loss:78.4694
step:171400 source:(1, 66) loss:105.935
step:171500 source:(1, 66) loss:71.9381
step:171600 source:(1, 27) loss:96.6894
step:171700 source:(1, 66) loss:81.2874
step:171800 source:(1, 22) loss:71.5246
step:171900 source:(1, 66) loss:78.1883
step:172000 source:(1, 48) loss:78.8827
step:172100 source:(1, 66) loss:68.3462
step:172200 source:(1, 44) loss:95.4424
step:172300 source:(1, 30) loss:80.495
step:172400 source:(1, 66) loss:83.5988
step:172500 source:(1, 51) loss:55.0909
step:172600 source:(1, 66) loss:82.4021
step:172700 source:(1, 66) loss:92.4301
step:172800 source:(1, 66) loss:76.3144
step:172900 source:(1, 66) loss:82.3826
step:173000 source:(1, 66) loss:87.2048
step:173100 source:(1, 66) loss:92.4822
step:173200 source:(1, 66) loss:72.3051
step:173300 source:(1, 66) loss:101.036
step:173400 source:(1, 66) loss:78.4468
step:173500 source:(1, 66) loss:96.8999
step:173600 source:(1, 36) loss:69.7964
step:173700 source:(1, 35) loss:88.0967
step:173800 source:(1, 66) loss:60.0027
step:173900 source:(1, 66) loss:83.349
step:174000 source:(1, 66) loss:90.331
step:174100 source:(1, 58) loss:85.6935
step:174200 source:(1, 66) loss:80.5432
step:174300 source:(1, 36) loss:80.6106
step:174400 source:(1, 66) loss:62.605
step:174500 source:(1, 66) loss:76.2751
step:174600 source:(1, 66) loss:75.8644
step:174700 source:(1, 66) loss:78.8447
step:174800 source:(1, 35) loss:72.0513
step:174900 source:(1, 66) loss:68.2087
step:175000 source:(1, 55) loss:88.0164
step:175100 source:(1, 66) loss:84.8913
step:175200 source:(1, 66) loss:70.1341
step:175300 source:(1, 66) loss:67.9772
step:175400 source:(1, 34) loss:83.2491
step:175500 source:(1, 33) loss:71.3938
step:175600 source:(1, 66) loss:75.7162
step:175700 source:(1, 22) loss:69.2804
step:175800 source:(1, 57) loss:78.3785
step:175900 source:(1, 66) loss:75.8468
step:176000 source:(1, 66) loss:70.1938
step:176100 source:(1, 54) loss:73.2702
step:176200 source:(1, 66) loss:88.728
step:176300 source:(1, 25) loss:87.4836
step:176400 source:(1, 66) loss:60.4947
step:176500 source:(1, 35) loss:84.2758
step:176600 source:(1, 66) loss:70.8124
step:176700 source:(1, 66) loss:71.9091
step:176800 source:(1, 66) loss:65.1692
step:176900 source:(1, 30) loss:71.0105
step:177000 source:(1, 25) loss:70.6205
step:177100 source:(1, 66) loss:74.0831
step:177200 source:(1, 59) loss:79.07
step:177300 source:(1, 66) loss:65.7261
step:177400 source:(1, 44) loss:73.364
step:177500 source:(1, 30) loss:74.1765
step:177600 source:(1, 50) loss:80.7665
step:177700 source:(1, 66) loss:83.2464
step:177800 source:(1, 36) loss:74.4279
step:177900 source:(1, 66) loss:70.597
step:178000 source:(1, 66) loss:51.6726
step:178100 source:(1, 64) loss:73.7068
step:178200 source:(1, 23) loss:83.9184
step:178300 source:(1, 66) loss:79.0275
step:178400 source:(1, 66) loss:60.665
step:178500 source:(1, 41) loss:78.4677
step:178600 source:(1, 66) loss:64.1002
step:178700 source:(1, 62) loss:79.1927
step:178800 source:(1, 66) loss:62.1564
step:178900 source:(1, 66) loss:86.4667
step:179000 source:(1, 59) loss:59.9528
step:179100 source:(1, 44) loss:66.7215
step:179200 source:(1, 66) loss:72.5249
step:179300 source:(1, 30) loss:71.9386
step:179400 source:(1, 59) loss:90.6985
step:179500 source:(1, 64) loss:60.0213
step:179600 source:(1, 38) loss:77.8819
step:179700 source:(1, 66) loss:81.5914
step:179800 source:(1, 41) loss:86.7589
step:179900 source:(1, 49) loss:93.6908
step:180000 source:(1, 43) loss:78.2397
step:180100 source:(1, 66) loss:67.78
step:180200 source:(1, 66) loss:70.273
step:180300 source:(1, 34) loss:91.3836
step:180400 source:(1, 35) loss:61.2525
step:180500 source:(1, 66) loss:77.4104
step:180600 source:(1, 43) loss:78.7514
step:180700 source:(1, 48) loss:75.1465
step:180800 source:(1, 66) loss:61.4721
step:180900 source:(1, 31) loss:90.5866
step:181000 source:(1, 66) loss:78.6875
step:181100 source:(1, 66) loss:79.5908
step:181200 source:(1, 24) loss:76.389
step:181300 source:(1, 64) loss:96.3448
step:181400 source:(1, 66) loss:81.6498
step:181500 source:(1, 66) loss:69.2146
step:181600 source:(1, 63) loss:79.0069
step:181700 source:(1, 29) loss:75.7665
step:181800 source:(1, 66) loss:79.2863
step:181900 source:(1, 24) loss:82.0662
step:182000 source:(1, 39) loss:94.0299
step:182100 source:(1, 56) loss:76.9154
step:182200 source:(1, 66) loss:73.3581
step:182300 source:(1, 66) loss:71.2373
step:182400 source:(1, 23) loss:74.52582018-07-19 21:16:04.243420: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-19 21:16:04.243507: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-19 21:31:51.912651: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-19 21:31:51.912719: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-19 21:31:51.912747: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]

step:182500 source:(1, 66) loss:56.8867
step:182600 source:(1, 66) loss:75.5399
step:182700 source:(1, 58) loss:66.3896
step:182800 source:(1, 49) loss:75.8717
step:182900 source:(1, 27) loss:74.7764
step:183000 source:(1, 52) loss:67.5844
step:183100 source:(1, 66) loss:52.0385
step:183200 source:(1, 66) loss:87.7956
step:183300 source:(1, 36) loss:68.2129
step:183400 source:(1, 66) loss:70.4
step:183500 source:(1, 66) loss:54.693
step:183600 source:(1, 41) loss:72.6934
step:183700 source:(1, 59) loss:69.8307
step:183800 source:(1, 66) loss:80.1135
step:183900 source:(1, 66) loss:75.5939
step:184000 source:(1, 66) loss:75.4367
step:184100 source:(1, 33) loss:82.9666
step:184200 source:(1, 66) loss:67.2156
step:184300 source:(1, 66) loss:85.9208
step:184400 source:(1, 41) loss:79.4947
step:184500 source:(1, 66) loss:77.3489
step:184600 source:(1, 24) loss:94.7884
step:184700 source:(1, 66) loss:77.6988
step:184800 source:(1, 66) loss:84.3976
step:184900 source:(1, 66) loss:92.7591
step:185000 source:(1, 66) loss:64.444
step:185100 source:(1, 66) loss:61.8847
step:185200 source:(1, 60) loss:64.7783
step:185300 source:(1, 51) loss:78.6198
step:185400 source:(1, 42) loss:62.21
step:185500 source:(1, 62) loss:62.6088
step:185600 source:(1, 65) loss:72.2261
step:185700 source:(1, 66) loss:68.7991
step:185800 source:(1, 66) loss:77.0534
step:185900 source:(1, 52) loss:59.5038
step:186000 source:(1, 66) loss:69.764
step:186100 source:(1, 53) loss:73.1193
step:186200 source:(1, 66) loss:84.4654
step:186300 source:(1, 26) loss:70.0494
step:186400 source:(1, 66) loss:57.4055
step:186500 source:(1, 66) loss:84.8113
step:186600 source:(1, 40) loss:85.9171
step:186700 source:(1, 32) loss:69.9865
step:186800 source:(1, 23) loss:70.8048
step:186900 source:(1, 66) loss:73.8625
step:187000 source:(1, 38) loss:69.9355
step:187100 source:(1, 66) loss:81.9743
step:187200 source:(1, 66) loss:66.8361
step:187300 source:(1, 23) loss:68.8057
step:187400 source:(1, 66) loss:67.1128
step:187500 source:(1, 66) loss:82.665
step:187600 source:(1, 66) loss:81.6107
step:187700 source:(1, 66) loss:68.6361
step:187800 source:(1, 66) loss:85.5117
step:187900 source:(1, 66) loss:79.4668
step:188000 source:(1, 30) loss:78.5629
step:188100 source:(1, 66) loss:63.2086
step:188200 source:(1, 66) loss:97.7456
step:188300 source:(1, 66) loss:73.2567
step:188400 source:(1, 59) loss:79.2541
step:188500 source:(1, 66) loss:63.3775
step:188600 source:(1, 45) loss:65.1981
step:188700 source:(1, 66) loss:68.2738
step:188800 source:(1, 66) loss:77.7965
step:188900 source:(1, 66) loss:88.5387
step:189000 source:(1, 33) loss:63.4934
step:189100 source:(1, 66) loss:70.8591
step:189200 source:(1, 44) loss:66.571
step:189300 source:(1, 66) loss:82.7919
step:189400 source:(1, 23) loss:78.2387
step:189500 source:(1, 66) loss:67.0238
step:189600 source:(1, 66) loss:71.7582
step:189700 source:(1, 66) loss:66.1922
step:189800 source:(1, 58) loss:75.5081
step:189900 source:(1, 66) loss:89.1426
step:190000 source:(1, 30) loss:75.1864
step:190100 source:(1, 66) loss:68.8928
step:190200 source:(1, 37) loss:73.7378
step:190300 source:(1, 66) loss:71.9901
step:190400 source:(1, 66) loss:77.8499
step:190500 source:(1, 51) loss:72.7146
step:190600 source:(1, 46) loss:73.2141
step:190700 source:(1, 66) loss:73.4921
step:190800 source:(1, 66) loss:66.6148
step:190900 source:(1, 66) loss:55.1924
step:191000 source:(1, 66) loss:53.3853
step:191100 source:(1, 66) loss:58.2249
step:191200 source:(1, 66) loss:85.107
step:191300 source:(1, 38) loss:78.6392
step:191400 source:(1, 66) loss:93.4088
step:191500 source:(1, 25) loss:58.8278
step:191600 source:(1, 66) loss:77.4808
step:191700 source:(1, 66) loss:78.9279
step:191800 source:(1, 66) loss:75.6015
step:191900 source:(1, 57) loss:70.6957
step:192000 source:(1, 39) loss:82.7718
step:192100 source:(1, 66) loss:57.3585
step:192200 source:(1, 59) loss:87.9807
step:192300 source:(1, 66) loss:74.5609
step:192400 source:(1, 66) loss:87.2073
step:192500 source:(1, 59) loss:84.1983
step:192600 source:(1, 66) loss:59.8355
step:192700 source:(1, 66) loss:89.4323
step:192800 source:(1, 32) loss:63.6391
step:192900 source:(1, 65) loss:75.8017
step:193000 source:(1, 22) loss:71.7094
step:193100 source:(1, 22) loss:79.9426
step:193200 source:(1, 66) loss:84.3481
step:193300 source:(1, 66) loss:80.8012
step:193400 source:(1, 66) loss:103.943
step:193500 source:(1, 53) loss:88.0263
step:193600 source:(1, 35) loss:70.5175
step:193700 source:(1, 66) loss:77.8589
step:193800 source:(1, 58) loss:64.1587
step:193900 source:(1, 66) loss:72.6473
step:194000 source:(1, 66) loss:75.4965
step:194100 source:(1, 59) loss:87.9819
step:194200 source:(1, 66) loss:68.1555
step:194300 source:(1, 66) loss:74.9463
step:194400 source:(1, 66) loss:75.0901
step:194500 source:(1, 48) loss:70.4632
step:194600 source:(1, 45) loss:80.958
step:194700 source:(1, 66) loss:58.6581
step:194800 source:(1, 55) loss:71.4776
step:194900 source:(1, 66) loss:61.9942
step:195000 source:(1, 23) loss:76.0259
step:195100 source:(1, 48) loss:75.7251
step:195200 source:(1, 63) loss:58.649
step:195300 source:(1, 35) loss:74.6642
step:195400 source:(1, 66) loss:77.8922
step:195500 source:(1, 52) loss:57.9471
step:195600 source:(1, 66) loss:74.3366
step:195700 source:(1, 66) loss:81.9188
step:195800 source:(1, 29) loss:89.6946
step:195900 source:(1, 44) loss:90.7144
step:196000 source:(1, 40) loss:64.9619
step:196100 source:(1, 66) loss:72.0746
step:196200 source:(1, 66) loss:70.7401
step:196300 source:(1, 55) loss:80.8329
step:196400 source:(1, 66) loss:86.2369
step:196500 source:(1, 66) loss:64.1529
step:196600 source:(1, 66) loss:87.0826
step:196700 source:(1, 66) loss:71.0165
step:196800 source:(1, 66) loss:55.8713
step:196900 source:(1, 52) loss:51.2547
step:197000 source:(1, 66) loss:71.4733
step:197100 source:(1, 66) loss:75.3633
step:197200 source:(1, 66) loss:81.392
step:197300 source:(1, 66) loss:87.6624
step:197400 source:(1, 66) loss:80.3
step:197500 source:(1, 66) loss:69.2648
step:197600 source:(1, 39) loss:89.5234
step:197700 source:(1, 66) loss:60.3209
step:197800 source:(1, 66) loss:72.2458
step:197900 source:(1, 66) loss:72.7243
step:198000 source:(1, 49) loss:73.3948
step:198100 source:(1, 30) loss:72.7919
step:198200 source:(1, 66) loss:66.5646
step:198300 source:(1, 66) loss:86.3415
step:198400 source:(1, 23) loss:77.7733
step:198500 source:(1, 27) loss:82.8908
step:198600 source:(1, 32) loss:65.7097
step:198700 source:(1, 66) loss:57.6503
step:198800 source:(1, 66) loss:79.8327
step:198900 source:(1, 33) loss:70.4422
step:199000 source:(1, 24) loss:72.4754
step:199100 source:(1, 66) loss:66.5535
step:199200 source:(1, 33) loss:66.5029
step:199300 source:(1, 66) loss:82.5195
step:199400 source:(1, 27) loss:87.6902
step:199500 source:(1, 66) loss:90.5436
step:199600 source:(1, 37) loss:78.1281
step:199700 source:(1, 58) loss:57.5374
step:199800 source:(1, 22) loss:47.3076
step:199900 source:(1, 66) loss:68.4735
step:200000 source:(1, 58) loss:72.4935
epoch:1 eval_bleu:55.81960082054138
the 1 epoch, highest bleu 55.819601
step:200100 source:(1, 66) loss:80.9414
step:200200 source:(1, 66) loss:75.7822
step:200300 source:(1, 45) loss:92.783
step:200400 source:(1, 66) loss:77.2685
step:200500 source:(1, 38) loss:62.941
step:200600 source:(1, 66) loss:66.0384
step:200700 source:(1, 25) loss:57.9234
step:200800 source:(1, 66) loss:73.9128
step:200900 source:(1, 66) loss:71.2642
step:201000 source:(1, 29) loss:68.5279
step:201100 source:(1, 66) loss:72.7701
step:201200 source:(1, 38) loss:91.7328
step:201300 source:(1, 36) loss:73.0349
step:201400 source:(1, 66) loss:62.7523
step:201500 source:(1, 32) loss:75.9198
step:201600 source:(1, 52) loss:66.8322
step:201700 source:(1, 66) loss:79.6907
step:201800 source:(1, 66) loss:77.3185
step:201900 source:(1, 47) loss:91.5142
step:202000 source:(1, 66) loss:66.2662
step:202100 source:(1, 40) loss:98.7495
step:202200 source:(1, 23) loss:89.0887
step:202300 source:(1, 59) loss:61.1186
step:202400 source:(1, 66) loss:75.4397
step:202500 source:(1, 66) loss:72.5959
step:202600 source:(1, 23) loss:56.2519
step:202700 source:(1, 27) loss:92.9802
step:202800 source:(1, 63) loss:69.685
step:202900 source:(1, 66) loss:77.8255
step:203000 source:(1, 65) loss:65.9583
step:203100 source:(1, 28) loss:85.0425
step:203200 source:(1, 66) loss:83.1836
step:203300 source:(1, 46) loss:75.5392
step:203400 source:(1, 66) loss:82.9996
step:203500 source:(1, 66) loss:76.7889
step:203600 source:(1, 66) loss:82.4408
step:203700 source:(1, 66) loss:64.1986
step:203800 source:(1, 66) loss:74.1527
step:203900 source:(1, 33) loss:71.9388
step:204000 source:(1, 66) loss:78.1758
step:204100 source:(1, 66) loss:94.1006
step:204200 source:(1, 66) loss:63.1003
step:204300 source:(1, 66) loss:66.9985
step:204400 source:(1, 40) loss:65.9406
step:204500 source:(1, 66) loss:79.2828
step:204600 source:(1, 29) loss:66.8771
step:204700 source:(1, 66) loss:96.5077
step:204800 source:(1, 36) loss:71.4596
step:204900 source:(1, 66) loss:64.0651
step:205000 source:(1, 28) loss:69.3337
step:205100 source:(1, 42) loss:74.432
step:205200 source:(1, 66) loss:73.215
step:205300 source:(1, 63) loss:79.3417
step:205400 source:(1, 34) loss:72.4771
step:205500 source:(1, 61) loss:76.3129
step:205600 source:(1, 66) loss:58.4637
step:205700 source:(1, 66) loss:76.1672
step:205800 source:(1, 30) loss:73.0385
step:205900 source:(1, 35) loss:71.1625
step:206000 source:(1, 66) loss:59.1735
step:206100 source:(1, 66) loss:70.7669
step:206200 source:(1, 24) loss:79.8773
step:206300 source:(1, 66) loss:74.282
step:206400 source:(1, 66) loss:86.237
step:206500 source:(1, 66) loss:79.9404
step:206600 source:(1, 66) loss:66.4895
step:206700 source:(1, 66) loss:92.1749
step:206800 source:(1, 66) loss:75.9903
step:206900 source:(1, 66) loss:78.8823
step:207000 source:(1, 61) loss:70.1301
step:207100 source:(1, 49) loss:72.6815
step:207200 source:(1, 50) loss:102.516
step:207300 source:(1, 34) loss:68.4175
step:207400 source:(1, 29) loss:65.8564
step:207500 source:(1, 22) loss:81.1035
step:207600 source:(1, 34) loss:76.4462
step:207700 source:(1, 66) loss:54.4953
step:207800 source:(1, 26) loss:71.8521
step:207900 source:(1, 51) loss:81.8951
step:208000 source:(1, 47) loss:66.1853
step:208100 source:(1, 44) loss:68.7533
step:208200 source:(1, 28) loss:81.5648
step:208300 source:(1, 66) loss:76.5607
step:208400 source:(1, 66) loss:67.3544
step:208500 source:(1, 66) loss:66.4621
step:208600 source:(1, 45) loss:75.4223
step:208700 source:(1, 66) loss:69.9474
step:208800 source:(1, 36) loss:77.8647
step:208900 source:(1, 66) loss:69.5184
step:209000 source:(1, 66) loss:83.1611
step:209100 source:(1, 66) loss:76.2889
step:209200 source:(1, 66) loss:72.4934
step:209300 source:(1, 66) loss:53.3204
step:209400 source:(1, 66) loss:65.6567
step:209500 source:(1, 41) loss:73.7335
step:209600 source:(1, 33) loss:79.0854
step:209700 source:(1, 66) loss:76.6277
step:209800 source:(1, 64) loss:50.2028
step:209900 source:(1, 66) loss:68.3614
step:210000 source:(1, 66) loss:78.7158
step:210100 source:(1, 25) loss:78.5282
step:210200 source:(1, 32) loss:90.3601
step:210300 source:(1, 57) loss:70.9538
step:210400 source:(1, 66) loss:83.7194
step:210500 source:(1, 41) loss:88.5296
step:210600 source:(1, 24) loss:83.3099
step:210700 source:(1, 66) loss:59.3791
step:210800 source:(1, 33) loss:73.4529
step:210900 source:(1, 29) loss:74.2032
step:211000 source:(1, 66) loss:72.9989
step:211100 source:(1, 52) loss:80.4871
step:211200 source:(1, 44) loss:75.0603
step:211300 source:(1, 50) loss:93.0847
step:211400 source:(1, 36) loss:61.9796
step:211500 source:(1, 66) loss:69.1336
step:211600 source:(1, 66) loss:84.4814
step:211700 source:(1, 29) loss:66.9335
step:211800 source:(1, 48) loss:77.8487
step:211900 source:(1, 66) loss:82.2997
step:212000 source:(1, 66) loss:60.7515
step:212100 source:(1, 29) loss:75.9894
step:212200 source:(1, 66) loss:59.3043
step:212300 source:(1, 66) loss:65.4645
step:212400 source:(1, 29) loss:81.9078
step:212500 source:(1, 66) loss:71.3361
step:212600 source:(1, 29) loss:60.0656
step:212700 source:(1, 46) loss:96.9582
step:212800 source:(1, 44) loss:84.9888
step:212900 source:(1, 47) loss:92.9703
step:213000 source:(1, 38) loss:66.8071
step:213100 source:(1, 66) loss:77.6231
step:213200 source:(1, 26) loss:63.8328
step:213300 source:(1, 33) loss:57.2612
step:213400 source:(1, 66) loss:56.8868
step:213500 source:(1, 66) loss:95.5988
step:213600 source:(1, 45) loss:74.0035
step:213700 source:(1, 25) loss:73.3112
step:213800 source:(1, 52) loss:75.1428
step:213900 source:(1, 66) loss:61.6366
step:214000 source:(1, 66) loss:75.354
step:214100 source:(1, 66) loss:58.4247
step:214200 source:(1, 66) loss:62.4459
step:214300 source:(1, 59) loss:73.5144
step:214400 source:(1, 25) loss:81.0435
step:214500 source:(1, 56) loss:84.5115
step:214600 source:(1, 47) loss:74.6173
step:214700 source:(1, 53) loss:66.8131
step:214800 source:(1, 33) loss:72.6825
step:214900 source:(1, 66) loss:95.8812
step:215000 source:(1, 43) loss:76.3209
step:215100 source:(1, 43) loss:62.8567
step:215200 source:(1, 40) loss:76.853
step:215300 source:(1, 40) loss:73.2736
step:215400 source:(1, 66) loss:63.1849
step:215500 source:(1, 66) loss:58.7096
step:215600 source:(1, 23) loss:76.6345
step:215700 source:(1, 66) loss:67.386
step:215800 source:(1, 66) loss:83.6686
step:215900 source:(1, 66) loss:63.5166
step:216000 source:(1, 41) loss:51.9423
step:216100 source:(1, 60) loss:68.6161
step:216200 source:(1, 36) loss:74.9767
step:216300 source:(1, 29) loss:69.6049
step:216400 source:(1, 56) loss:91.506
step:216500 source:(1, 66) loss:90.2321
step:216600 source:(1, 50) loss:74.7531
step:216700 source:(1, 66) loss:69.2155
step:216800 source:(1, 66) loss:86.4404
step:216900 source:(1, 66) loss:67.7798
step:217000 source:(1, 66) loss:89.7987
step:217100 source:(1, 57) loss:79.1206
step:217200 source:(1, 66) loss:95.8884
step:217300 source:(1, 26) loss:72.4646
step:217400 source:(1, 66) loss:70.9896
step:217500 source:(1, 66) loss:60.3786
step:217600 source:(1, 66) loss:80.8734
step:217700 source:(1, 38) loss:57.984
step:217800 source:(1, 66) loss:74.4165
step:217900 source:(1, 66) loss:63.2699
step:218000 source:(1, 66) loss:68.3937
step:218100 source:(1, 66) loss:72.0161
step:218200 source:(1, 66) loss:97.9982
step:218300 source:(1, 48) loss:88.1498
step:218400 source:(1, 66) loss:60.0793
step:218500 source:(1, 66) loss:87.2891
step:218600 source:(1, 66) loss:64.0719
step:218700 source:(1, 48) loss:84.9257
step:218800 source:(1, 66) loss:52.9979
step:218900 source:(1, 66) loss:79.5956
step:219000 source:(1, 66) loss:80.4068
step:219100 source:(1, 66) loss:85.5733
step:219200 source:(1, 66) loss:73.1249
step:219300 source:(1, 28) loss:75.5516
step:219400 source:(1, 66) loss:66.0447
step:219500 source:(1, 66) loss:77.5479
step:219600 source:(1, 66) loss:72.9437
step:219700 source:(1, 26) loss:86.8709
step:219800 source:(1, 66) loss:74.4872
step:219900 source:(1, 66) loss:61.5165
step:220000 source:(1, 66) loss:77.2802
step:220100 source:(1, 60) loss:65.3723
step:220200 source:(1, 63) loss:75.5028
step:220300 source:(1, 66) loss:56.6927
step:220400 source:(1, 62) loss:86.7498
step:220500 source:(1, 42) loss:84.9162
step:220600 source:(1, 27) loss:82.0922
step:220700 source:(1, 54) loss:69.7422
step:220800 source:(1, 66) loss:57.9162
step:220900 source:(1, 54) loss:83.3267
step:221000 source:(1, 66) loss:86.5032
step:221100 source:(1, 66) loss:74.2655
step:221200 source:(1, 40) loss:75.7342
step:221300 source:(1, 55) loss:49.4552
step:221400 source:(1, 66) loss:60.2629
step:221500 source:(1, 66) loss:67.805
step:221600 source:(1, 66) loss:96.4272
step:221700 source:(1, 66) loss:84.4165
step:221800 source:(1, 66) loss:80.0175
step:221900 source:(1, 33) loss:54.8155
step:222000 source:(1, 66) loss:81.1812
step:222100 source:(1, 66) loss:71.5489
step:222200 source:(1, 66) loss:70.5216
step:222300 source:(1, 56) loss:86.1803
step:222400 source:(1, 53) loss:65.6777
step:222500 source:(1, 24) loss:74.4932
step:222600 source:(1, 66) loss:61.7972
step:222700 source:(1, 66) loss:97.7834
step:222800 source:(1, 55) loss:81.3585
step:222900 source:(1, 66) loss:68.2069
step:223000 source:(1, 66) loss:90.7154
step:223100 source:(1, 62) loss:59.5603
step:223200 source:(1, 66) loss:67.6607
step:223300 source:(1, 56) loss:82.8816
step:223400 source:(1, 66) loss:69.2772
step:223500 source:(1, 42) loss:88.2053
step:223600 source:(1, 44) loss:80.9991
step:223700 source:(1, 66) loss:75.1083
step:223800 source:(1, 66) loss:95.8767
step:223900 source:(1, 43) loss:57.7058
step:224000 source:(1, 66) loss:73.554
step:224100 source:(1, 43) loss:70.7259
step:224200 source:(1, 27) loss:67.2294
step:224300 source:(1, 66) loss:85.951
step:224400 source:(1, 50) loss:73.4217
step:224500 source:(1, 66) loss:80.9383
step:224600 source:(1, 66) loss:84.5827
step:224700 source:(1, 66) loss:62.7876
step:224800 source:(1, 66) loss:69.2046
step:224900 source:(1, 60) loss:63.2486
step:225000 source:(1, 27) loss:78.4192
step:225100 source:(1, 66) loss:62.1036
step:225200 source:(1, 61) loss:69.7704
step:225300 source:(1, 24) loss:89.8181
step:225400 source:(1, 66) loss:86.1748
step:225500 source:(1, 66) loss:79.3417
step:225600 source:(1, 66) loss:71.184
step:225700 source:(1, 66) loss:64.13
step:225800 source:(1, 31) loss:77.4653
step:225900 source:(1, 40) loss:83.9497
step:226000 source:(1, 32) loss:85.5575
step:226100 source:(1, 47) loss:67.5658
step:226200 source:(1, 66) loss:62.9403
step:226300 source:(1, 51) loss:66.6858
step:226400 source:(1, 29) loss:77.5484
step:226500 source:(1, 49) loss:77.2428
step:226600 source:(1, 66) loss:86.0351
step:226700 source:(1, 66) loss:99.0734
step:226800 source:(1, 57) loss:79.538
step:226900 source:(1, 56) loss:81.5226
step:227000 source:(1, 66) loss:64.6929
step:227100 source:(1, 26) loss:42.1098
step:227200 source:(1, 66) loss:97.3596
step:227300 source:(1, 66) loss:80.1382
step:227400 source:(1, 66) loss:77.4941
step:227500 source:(1, 56) loss:63.0689
step:227600 source:(1, 66) loss:78.8142
step:227700 source:(1, 66) loss:86.8547
step:227800 source:(1, 66) loss:63.9908
step:227900 source:(1, 66) loss:68.5625
step:228000 source:(1, 66) loss:64.5361
step:228100 source:(1, 66) loss:65.9296
step:228200 source:(1, 25) loss:81.2723
step:228300 source:(1, 51) loss:65.2671
step:228400 source:(1, 64) loss:67.9745
step:228500 source:(1, 66) loss:90.0715
step:228600 source:(1, 59) loss:74.0002
step:228700 source:(1, 25) loss:77.674
step:228800 source:(1, 36) loss:85.1136
step:228900 source:(1, 66) loss:78.8512
step:229000 source:(1, 66) loss:97.9479
step:229100 source:(1, 66) loss:78.4986
step:229200 source:(1, 66) loss:54.8348
step:229300 source:(1, 35) loss:79.9856
step:229400 source:(1, 28) loss:81.873
step:229500 source:(1, 66) loss:70.178
step:229600 source:(1, 29) loss:56.9888
step:229700 source:(1, 66) loss:85.3932
step:229800 source:(1, 66) loss:61.7342
step:229900 source:(1, 66) loss:76.0341
step:230000 source:(1, 66) loss:66.4911
step:230100 source:(1, 31) loss:80.6036
step:230200 source:(1, 50) loss:65.9957
step:230300 source:(1, 22) loss:75.5183
step:230400 source:(1, 66) loss:78.9946
step:230500 source:(1, 46) loss:93.2072
step:230600 source:(1, 66) loss:72.2857
step:230700 source:(1, 53) loss:73.525
step:230800 source:(1, 39) loss:44.3256
step:230900 source:(1, 37) loss:68.4005
step:231000 source:(1, 66) loss:83.6798
step:231100 source:(1, 39) loss:90.6285
step:231200 source:(1, 66) loss:64.4735
step:231300 source:(1, 66) loss:84.9742
step:231400 source:(1, 66) loss:77.5249
step:231500 source:(1, 66) loss:58.6942
step:231600 source:(1, 66) loss:72.6608
step:231700 source:(1, 66) loss:75.1446
step:231800 source:(1, 66) loss:76.7683
step:231900 source:(1, 32) loss:69.8094
step:232000 source:(1, 50) loss:79.6411
step:232100 source:(1, 66) loss:88.1085
step:232200 source:(1, 66) loss:76.6036
step:232300 source:(1, 66) loss:62.7085
step:232400 source:(1, 44) loss:63.7642
step:232500 source:(1, 44) loss:68.1614
step:232600 source:(1, 66) loss:58.272
step:232700 source:(1, 66) loss:66.5935
step:232800 source:(1, 66) loss:70.8821
step:232900 source:(1, 44) loss:65.9576
step:233000 source:(1, 30) loss:89.5128
step:233100 source:(1, 39) loss:56.8293
step:233200 source:(1, 56) loss:84.6827
step:233300 source:(1, 34) loss:60.7687
step:233400 source:(1, 66) loss:78.8706
step:233500 source:(1, 66) loss:71.6261
step:233600 source:(1, 56) loss:67.7278
step:233700 source:(1, 64) loss:78.4404
step:233800 source:(1, 34) loss:61.2142
step:233900 source:(1, 66) loss:88.7251
step:234000 source:(1, 66) loss:76.8973
step:234100 source:(1, 66) loss:65.3394
step:234200 source:(1, 59) loss:71.2726
step:234300 source:(1, 66) loss:71.8789
step:234400 source:(1, 28) loss:60.1824
step:234500 source:(1, 66) loss:71.1434
step:234600 source:(1, 66) loss:97.1162
step:234700 source:(1, 65) loss:70.9094
step:234800 source:(1, 66) loss:77.3531
step:234900 source:(1, 66) loss:72.3111
step:235000 source:(1, 66) loss:80.1085
step:235100 source:(1, 66) loss:96.0129
step:235200 source:(1, 29) loss:65.961
step:235300 source:(1, 66) loss:76.1243
step:235400 source:(1, 66) loss:61.6755
step:235500 source:(1, 23) loss:57.1751
step:235600 source:(1, 58) loss:71.9516
step:235700 source:(1, 66) loss:83.0487
step:235800 source:(1, 66) loss:70.6933
step:235900 source:(1, 66) loss:75.625
step:236000 source:(1, 66) loss:77.6078
step:236100 source:(1, 66) loss:69.786
step:236200 source:(1, 66) loss:87.1171
step:236300 source:(1, 33) loss:72.9807
step:236400 source:(1, 40) loss:65.4603
step:236500 source:(1, 34) loss:76.6094
step:236600 source:(1, 29) loss:82.1869
step:236700 source:(1, 66) loss:81.8793
step:236800 source:(1, 66) loss:71.7987
step:236900 source:(1, 66) loss:77.6347
step:237000 source:(1, 66) loss:71.5601
step:237100 source:(1, 66) loss:65.8869
step:237200 source:(1, 33) loss:83.8794
step:237300 source:(1, 38) loss:80.4189
step:237400 source:(1, 66) loss:80.5503
step:237500 source:(1, 66) loss:57.7368
step:237600 source:(1, 66) loss:90.2602
step:237700 source:(1, 44) loss:57.7382
step:237800 source:(1, 66) loss:84.5573
step:237900 source:(1, 28) loss:62.3229
step:238000 source:(1, 66) loss:87.2856
step:238100 source:(1, 66) loss:80.9511
step:238200 source:(1, 39) loss:62.689
step:238300 source:(1, 66) loss:92.5419
step:238400 source:(1, 32) loss:70.5742
step:238500 source:(1, 46) loss:58.9251
step:238600 source:(1, 37) loss:96.1963
step:238700 source:(1, 66) loss:80.2217
step:238800 source:(1, 66) loss:65.0956
step:238900 source:(1, 23) loss:72.1737
step:239000 source:(1, 66) loss:78.8583
step:239100 source:(1, 53) loss:68.7801
step:239200 source:(1, 66) loss:69.0372
step:239300 source:(1, 38) loss:82.2676
step:239400 source:(1, 40) loss:74.8935
step:239500 source:(1, 37) loss:80.719
step:239600 source:(1, 66) loss:77.1042
step:239700 source:(1, 26) loss:64.6943
step:239800 source:(1, 37) loss:74.92
step:239900 source:(1, 66) loss:72.9155
step:240000 source:(1, 66) loss:69.1567
step:240100 source:(1, 65) loss:76.6027
step:240200 source:(1, 29) loss:89.6866
step:240300 source:(1, 66) loss:69.785
step:240400 source:(1, 66) loss:80.2574
step:240500 source:(1, 36) loss:65.2186
step:240600 source:(1, 66) loss:76.7599
step:240700 source:(1, 42) loss:77.0133
step:240800 source:(1, 66) loss:65.987
step:240900 source:(1, 66) loss:63.1394
step:241000 source:(1, 33) loss:57.3042
step:241100 source:(1, 66) loss:62.7954
step:241200 source:(1, 66) loss:70.61532018-07-19 23:10:45.199800: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-19 23:10:45.199895: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-19 23:10:45.199932: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
/home/syp/miniconda2/envs/newtf/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)

step:241300 source:(1, 66) loss:72.5905
step:241400 source:(1, 54) loss:76.7747
step:241500 source:(1, 55) loss:54.2223
step:241600 source:(1, 66) loss:68.4383
step:241700 source:(1, 66) loss:89.9385
step:241800 source:(1, 48) loss:79.1729
step:241900 source:(1, 66) loss:58.0419
step:242000 source:(1, 59) loss:76.9312
step:242100 source:(1, 66) loss:71.8875
step:242200 source:(1, 66) loss:85.9743
step:242300 source:(1, 66) loss:66.8436
step:242400 source:(1, 66) loss:73.2659
step:242500 source:(1, 28) loss:75.287
step:242600 source:(1, 66) loss:69.0312
step:242700 source:(1, 66) loss:60.2656
step:242800 source:(1, 53) loss:63.2463
step:242900 source:(1, 56) loss:86.7073
step:243000 source:(1, 33) loss:68.0945
step:243100 source:(1, 66) loss:80.369
step:243200 source:(1, 66) loss:63.7343
step:243300 source:(1, 66) loss:73.4772
step:243400 source:(1, 37) loss:54.5727
step:243500 source:(1, 32) loss:74.0565
step:243600 source:(1, 33) loss:70.8367
step:243700 source:(1, 66) loss:69.2328
step:243800 source:(1, 64) loss:81.7767
step:243900 source:(1, 54) loss:71.3949
step:244000 source:(1, 66) loss:67.1991
step:244100 source:(1, 66) loss:87.6343
step:244200 source:(1, 63) loss:77.3432
step:244300 source:(1, 66) loss:79.205
step:244400 source:(1, 44) loss:85.8074
step:244500 source:(1, 53) loss:72.6047
step:244600 source:(1, 66) loss:69.2697
step:244700 source:(1, 64) loss:100.039
step:244800 source:(1, 50) loss:65.886
step:244900 source:(1, 66) loss:73.0282
step:245000 source:(1, 66) loss:54.6637
step:245100 source:(1, 66) loss:82.615
step:245200 source:(1, 44) loss:73.8179
step:245300 source:(1, 22) loss:72.3694
step:245400 source:(1, 66) loss:70.475
step:245500 source:(1, 66) loss:76.1746
step:245600 source:(1, 59) loss:73.0416
step:245700 source:(1, 48) loss:54.1942
step:245800 source:(1, 37) loss:78.1496
step:245900 source:(1, 26) loss:76.3061
step:246000 source:(1, 66) loss:74.12
step:246100 source:(1, 66) loss:62.7377
step:246200 source:(1, 66) loss:59.7748
step:246300 source:(1, 55) loss:52.8752
step:246400 source:(1, 66) loss:68.3788
step:246500 source:(1, 66) loss:71.781
step:246600 source:(1, 46) loss:67.1181
step:246700 source:(1, 66) loss:78.5987
step:246800 source:(1, 35) loss:96.7017
step:246900 source:(1, 66) loss:54.1378
step:247000 source:(1, 66) loss:79.2315
step:247100 source:(1, 66) loss:79.7229
step:247200 source:(1, 48) loss:54.3316
step:247300 source:(1, 66) loss:79.6829
step:247400 source:(1, 66) loss:65.6653
step:247500 source:(1, 26) loss:77.1448
step:247600 source:(1, 47) loss:81.1459
step:247700 source:(1, 66) loss:93.239
step:247800 source:(1, 66) loss:76.2108
step:247900 source:(1, 38) loss:85.8478
step:248000 source:(1, 66) loss:71.7461
step:248100 source:(1, 36) loss:75.1052
step:248200 source:(1, 27) loss:70.3054
step:248300 source:(1, 66) loss:45.4928
step:248400 source:(1, 66) loss:59.716
step:248500 source:(1, 66) loss:80.5622
step:248600 source:(1, 66) loss:70.8246
step:248700 source:(1, 66) loss:82.0569
step:248800 source:(1, 37) loss:65.612
step:248900 source:(1, 66) loss:57.9112
step:249000 source:(1, 66) loss:77.7367
step:249100 source:(1, 66) loss:82.4046
step:249200 source:(1, 59) loss:56.2966
step:249300 source:(1, 62) loss:87.9045
step:249400 source:(1, 49) loss:87.8928
step:249500 source:(1, 47) loss:62.7891
step:249600 source:(1, 66) loss:91.0863
step:249700 source:(1, 44) loss:84.3469
step:249800 source:(1, 66) loss:82.2115
step:249900 source:(1, 66) loss:92.4701
step:250000 source:(1, 46) loss:66.3773
reach max steps:250000 loss:66.3773193359375
reached max training steps
epoch:2 eval_bleu:56.23665452003479
the 2 epoch, highest bleu 56.236655
saving model for max training steps
2018-07-20 10:23:25.345162: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-07-20 10:23:25.513169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.81GiB
2018-07-20 10:23:25.513215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-07-20 12:26:03.907215: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-07-20 12:26:04.040268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:02:00.0
totalMemory: 7.92GiB freeMemory: 3.03GiB
2018-07-20 12:26:04.040299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
2018-07-20 13:12:23.389957: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 13:12:23.390189: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 13:12:23.390218: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 13:12:23.391432: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 13:12:23.391534: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 13:30:02.014802: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 13:30:02.014875: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 13:30:02.014907: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
train_file:/home/syp/zwr/txtgen/examples/long_text/yahoo_data/yahoo.train.txt
valid_file:/home/syp/zwr/txtgen/examples/long_text/yahoo_data/yahoo.valid.txt
logdir:./log_dir/bsize32.epoch70.lr_c2warm16000/
step:250500 source:(1, 38) loss:69.7987
step:251000 source:(1, 29) loss:71.1525
step:251500 source:(1, 32) loss:77.2246
step:252000 source:(1, 66) loss:59.5778
step:252500 source:(1, 66) loss:102.063
step:253000 source:(1, 65) loss:65.8486
step:253500 source:(1, 66) loss:71.9438
step:254000 source:(1, 66) loss:84.5585
step:254500 source:(1, 66) loss:88.3602
step:255000 source:(1, 28) loss:77.5686
step:255500 source:(1, 61) loss:79.5099
step:256000 source:(1, 66) loss:70.0646
step:256500 source:(1, 66) loss:71.1873
step:257000 source:(1, 61) loss:73.6445
step:257500 source:(1, 22) loss:81.0169
step:258000 source:(1, 47) loss:63.2519
step:258500 source:(1, 66) loss:74.9874
step:259000 source:(1, 66) loss:91.543
step:259500 source:(1, 41) loss:81.2327
step:260000 source:(1, 66) loss:79.5011
step:260500 source:(1, 41) loss:79.9851
step:261000 source:(1, 66) loss:61.8277
step:261500 source:(1, 66) loss:75.7922
step:262000 source:(1, 66) loss:62.3657
step:262500 source:(1, 66) loss:68.5566
step:263000 source:(1, 38) loss:71.3558
step:263500 source:(1, 66) loss:91.0813
step:264000 source:(1, 66) loss:75.6957
step:264500 source:(1, 56) loss:79.2885
step:265000 source:(1, 43) loss:71.4069
step:265500 source:(1, 66) loss:71.4383
step:266000 source:(1, 41) loss:48.5069
step:266500 source:(1, 66) loss:92.4654
step:267000 source:(1, 66) loss:87.3791
step:267500 source:(1, 66) loss:66.6607
step:268000 source:(1, 66) loss:66.3703
step:268500 source:(1, 66) loss:70.1821
step:269000 source:(1, 66) loss:81.6359
step:269500 source:(1, 66) loss:63.5085
step:270000 source:(1, 66) loss:96.5387
step:270500 source:(1, 42) loss:87.9171
step:271000 source:(1, 66) loss:86.1747
step:271500 source:(1, 66) loss:68.4993
step:272000 source:(1, 66) loss:78.2998
step:272500 source:(1, 24) loss:77.2627
step:273000 source:(1, 66) loss:72.7377
step:273500 source:(1, 42) loss:96.9617
step:274000 source:(1, 66) loss:83.6962
step:274500 source:(1, 66) loss:80.3455
step:275000 source:(1, 27) loss:67.5206
step:275500 source:(1, 66) loss:66.0283
step:276000 source:(1, 32) loss:70.698
step:276500 source:(1, 49) loss:75.6167
step:277000 source:(1, 66) loss:84.5205
step:277500 source:(1, 56) loss:77.5838
step:278000 source:(1, 66) loss:76.2192
step:278500 source:(1, 66) loss:89.6968
step:279000 source:(1, 66) loss:82.5276
step:279500 source:(1, 66) loss:72.0646
step:280000 source:(1, 66) loss:77.8022
step:280500 source:(1, 46) loss:75.5007
step:281000 source:(1, 66) loss:68.2587
step:281500 source:(1, 66) loss:83.9208
step:282000 source:(1, 50) loss:77.0672
step:282500 source:(1, 44) loss:80.0549
step:283000 source:(1, 30) loss:98.746
step:283500 source:(1, 66) loss:63.301
step:284000 source:(1, 66) loss:79.0249
step:284500 source:(1, 66) loss:91.8635
step:285000 source:(1, 66) loss:70.9509
step:285500 source:(1, 23) loss:66.0135
step:286000 source:(1, 66) loss:56.6913
step:286500 source:(1, 34) loss:67.4966
step:287000 source:(1, 66) loss:76.0628
step:287500 source:(1, 66) loss:63.2534
step:288000 source:(1, 66) loss:72.3446
step:288500 source:(1, 46) loss:72.2629
step:289000 source:(1, 66) loss:78.7706
step:289500 source:(1, 37) loss:81.2446
step:290000 source:(1, 66) loss:81.8984
step:290500 source:(1, 36) loss:69.7793
step:291000 source:(1, 33) loss:57.8893
step:291500 source:(1, 55) loss:49.1649
step:292000 source:(1, 59) loss:74.1653
step:292500 source:(1, 28) loss:76.3419
step:293000 source:(1, 33) loss:71.197
step:293500 source:(1, 32) loss:78.6392
step:294000 source:(1, 66) loss:68.8919
step:294500 source:(1, 53) loss:58.7264
step:295000 source:(1, 66) loss:70.236
step:295500 source:(1, 66) loss:66.5131
step:296000 source:(1, 66) loss:74.4751
step:296500 source:(1, 66) loss:81.9852
step:297000 source:(1, 66) loss:78.6647
step:297500 source:(1, 26) loss:72.7498
step:298000 source:(1, 66) loss:69.4624
step:298500 source:(1, 66) loss:76.4254
step:299000 source:(1, 66) loss:64.9406
step:299500 source:(1, 47) loss:55.4667
step:300000 source:(1, 46) loss:66.749
step:300500 source:(1, 66) loss:78.3161
step:301000 source:(1, 66) loss:72.4482
step:301500 source:(1, 63) loss:74.2552
step:302000 source:(1, 31) loss:91.2785
step:302500 source:(1, 33) loss:71.569
step:303000 source:(1, 53) loss:60.7181
step:303500 source:(1, 66) loss:87.6761
step:304000 source:(1, 31) loss:78.9509
step:304500 source:(1, 66) loss:78.7761
step:305000 source:(1, 31) loss:70.7753
step:305500 source:(1, 66) loss:68.342
step:306000 source:(1, 66) loss:95.1227
step:306500 source:(1, 66) loss:89.8261
step:307000 source:(1, 66) loss:56.981
step:307500 source:(1, 57) loss:77.256
step:308000 source:(1, 24) loss:60.2039
step:308500 source:(1, 59) loss:82.0789
step:309000 source:(1, 39) loss:80.739
step:309500 source:(1, 35) loss:95.6742
step:310000 source:(1, 66) loss:81.1929
step:310500 source:(1, 43) loss:63.2067
step:311000 source:(1, 54) loss:79.028
step:311500 source:(1, 41) loss:58.1405
step:312000 source:(1, 66) loss:73.6524
step:312500 source:(1, 66) loss:77.9073
step:313000 source:(1, 66) loss:76.2276
step:313500 source:(1, 44) loss:68.8169
step:314000 source:(1, 66) loss:81.8169
step:314500 source:(1, 66) loss:60.653
step:315000 source:(1, 66) loss:67.5403
step:315500 source:(1, 51) loss:70.876
step:316000 source:(1, 57) loss:80.7497
step:316500 source:(1, 66) loss:72.3632
step:317000 source:(1, 34) loss:83.45
step:317500 source:(1, 66) loss:79.0783
step:318000 source:(1, 66) loss:69.9133
step:318500 source:(1, 35) loss:75.9381
step:319000 source:(1, 66) loss:80.1431
step:319500 source:(1, 35) loss:69.0856
step:320000 source:(1, 66) loss:77.0937
step:320500 source:(1, 66) loss:76.7726
step:321000 source:(1, 36) loss:61.4306
step:321500 source:(1, 66) loss:64.6382
step:322000 source:(1, 48) loss:81.0512
step:322500 source:(1, 51) loss:62.4619
step:323000 source:(1, 66) loss:78.4947
step:323500 source:(1, 66) loss:84.2421
step:324000 source:(1, 66) loss:77.0018
step:324500 source:(1, 66) loss:96.7461
step:325000 source:(1, 55) loss:68.2783
step:325500 source:(1, 33) loss:79.1967
step:326000 source:(1, 66) loss:74.1402
step:326500 source:(1, 35) loss:81.6768
step:327000 source:(1, 25) loss:66.6879
step:327500 source:(1, 30) loss:71.3366
step:328000 source:(1, 66) loss:59.7842
step:328500 source:(1, 41) loss:69.5611
step:329000 source:(1, 59) loss:71.4157
step:329500 source:(1, 64) loss:71.7843
step:330000 source:(1, 43) loss:74.6679
step:330500 source:(1, 66) loss:72.6418
step:331000 source:(1, 66) loss:64.7038
step:331500 source:(1, 66) loss:68.3306
step:332000 source:(1, 39) loss:85.097
step:332500 source:(1, 66) loss:71.8232
step:333000 source:(1, 52) loss:75.018
step:333500 source:(1, 66) loss:78.2319
step:334000 source:(1, 66) loss:80.1036
step:334500 source:(1, 66) loss:94.289
step:335000 source:(1, 66) loss:70.949
step:335500 source:(1, 62) loss:51.2426
step:336000 source:(1, 66) loss:68.8986
step:336500 source:(1, 66) loss:70.3525
step:337000 source:(1, 38) loss:66.1293
step:337500 source:(1, 66) loss:75.0611
step:338000 source:(1, 30) loss:75.9475
step:338500 source:(1, 66) loss:62.4807
step:339000 source:(1, 33) loss:63.0129
step:339500 source:(1, 66) loss:79.2137
step:340000 source:(1, 30) loss:69.6724
step:340500 source:(1, 51) loss:86.9426
step:341000 source:(1, 66) loss:81.0023
step:341500 source:(1, 25) loss:59.8261
step:342000 source:(1, 39) loss:78.6919
step:342500 source:(1, 59) loss:79.0755
step:343000 source:(1, 22) loss:77.706
step:343500 source:(1, 53) loss:57.4909
step:344000 source:(1, 66) loss:62.4683
step:344500 source:(1, 48) loss:70.3984
step:345000 source:(1, 23) loss:74.9187
step:345500 source:(1, 52) loss:90.9029
step:346000 source:(1, 40) loss:62.6141
step:346500 source:(1, 66) loss:70.4314
step:347000 source:(1, 66) loss:86.0963
step:347500 source:(1, 66) loss:90.8085
step:348000 source:(1, 49) loss:68.3243
step:348500 source:(1, 27) loss:87.2884
step:349000 source:(1, 24) loss:69.503
step:349500 source:(1, 66) loss:86.3062
step:350000 source:(1, 58) loss:68.5088
epoch:0 eval_bleu:56.50355815887451
the 0 epoch, highest bleu 56.503558
2018-07-20 16:30:37.695151: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 16:30:37.695222: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 16:30:37.696913: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 16:30:37.696961: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 16:46:10.225946: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 16:46:10.226016: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 16:46:10.226047: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:350500 source:(1, 38) loss:73.5467
step:351000 source:(1, 29) loss:61.1778
step:351500 source:(1, 32) loss:56.3086
step:352000 source:(1, 66) loss:72.2096
step:352500 source:(1, 66) loss:76.0916
step:353000 source:(1, 65) loss:75.8449
step:353500 source:(1, 66) loss:80.1027
step:354000 source:(1, 66) loss:76.0294
step:354500 source:(1, 66) loss:88.3811
step:355000 source:(1, 28) loss:64.6685
step:355500 source:(1, 61) loss:69.2141
step:356000 source:(1, 66) loss:79.1503
step:356500 source:(1, 66) loss:68.8898
step:357000 source:(1, 61) loss:77.981
step:357500 source:(1, 22) loss:84.9034
step:358000 source:(1, 47) loss:64.0307
step:358500 source:(1, 66) loss:66.8353
step:359000 source:(1, 66) loss:87.7201
step:359500 source:(1, 41) loss:74.8606
step:360000 source:(1, 66) loss:86.1415
step:360500 source:(1, 41) loss:71.0004
step:361000 source:(1, 66) loss:59.9982
step:361500 source:(1, 66) loss:67.2521
step:362000 source:(1, 66) loss:61.5895
step:362500 source:(1, 66) loss:65.9335
step:363000 source:(1, 38) loss:76.974
step:363500 source:(1, 66) loss:82.6725
step:364000 source:(1, 66) loss:72.9438
step:364500 source:(1, 56) loss:75.7393
step:365000 source:(1, 43) loss:69.166
step:365500 source:(1, 66) loss:69.2597
step:366000 source:(1, 41) loss:62.2494
step:366500 source:(1, 66) loss:95.0286
step:367000 source:(1, 66) loss:65.6332
step:367500 source:(1, 66) loss:69.9705
step:368000 source:(1, 66) loss:76.7758
step:368500 source:(1, 66) loss:75.9918
step:369000 source:(1, 66) loss:78.1522
step:369500 source:(1, 66) loss:69.0163
step:370000 source:(1, 66) loss:76.248
step:370500 source:(1, 42) loss:85.0966
step:371000 source:(1, 66) loss:80.1338
step:371500 source:(1, 66) loss:66.319
step:372000 source:(1, 66) loss:80.4491
step:372500 source:(1, 24) loss:85.0637
step:373000 source:(1, 66) loss:76.9049
step:373500 source:(1, 42) loss:84.8407
step:374000 source:(1, 66) loss:78.0795
step:374500 source:(1, 66) loss:87.4269
step:375000 source:(1, 27) loss:74.9912
step:375500 source:(1, 66) loss:80.2054
step:376000 source:(1, 32) loss:73.7321
step:376500 source:(1, 49) loss:70.3064
step:377000 source:(1, 66) loss:56.5092
step:377500 source:(1, 56) loss:70.5681
step:378000 source:(1, 66) loss:84.5569
step:378500 source:(1, 66) loss:82.8797
step:379000 source:(1, 66) loss:90.5058
step:379500 source:(1, 66) loss:78.1323
step:380000 source:(1, 66) loss:89.9248
step:380500 source:(1, 46) loss:78.3105
step:381000 source:(1, 66) loss:63.9759
step:381500 source:(1, 66) loss:60.7561
step:382000 source:(1, 50) loss:87.0007
step:382500 source:(1, 44) loss:82.892
step:383000 source:(1, 30) loss:95.7959
step:383500 source:(1, 66) loss:80.2685
step:384000 source:(1, 66) loss:74.6192
step:384500 source:(1, 66) loss:87.0762
step:385000 source:(1, 66) loss:67.3802
step:385500 source:(1, 23) loss:62.5455
step:386000 source:(1, 66) loss:69.2026
step:386500 source:(1, 34) loss:72.5973
step:387000 source:(1, 66) loss:64.3754
step:387500 source:(1, 66) loss:74.3414
step:388000 source:(1, 66) loss:87.7422
step:388500 source:(1, 46) loss:80.4425
step:389000 source:(1, 66) loss:80.2596
step:389500 source:(1, 37) loss:77.9834
step:390000 source:(1, 66) loss:76.8576
step:390500 source:(1, 36) loss:68.3398
step:391000 source:(1, 33) loss:64.2264
step:391500 source:(1, 55) loss:51.0691
step:392000 source:(1, 59) loss:74.6425
step:392500 source:(1, 28) loss:71.3066
step:393000 source:(1, 33) loss:77.9999
step:393500 source:(1, 32) loss:63.6309
step:394000 source:(1, 66) loss:70.4486
step:394500 source:(1, 53) loss:66.35
step:395000 source:(1, 66) loss:64.8487
step:395500 source:(1, 66) loss:72.7483
step:396000 source:(1, 66) loss:75.7366
step:396500 source:(1, 66) loss:70.5342
step:397000 source:(1, 66) loss:83.4027
step:397500 source:(1, 26) loss:68.5428
step:398000 source:(1, 66) loss:64.0313
step:398500 source:(1, 66) loss:81.977
step:399000 source:(1, 66) loss:78.9584
step:399500 source:(1, 47) loss:68.8915
step:400000 source:(1, 46) loss:63.8192
step:400500 source:(1, 66) loss:75.7468
step:401000 source:(1, 66) loss:63.2628
step:401500 source:(1, 63) loss:82.8932
step:402000 source:(1, 31) loss:82.937
step:402500 source:(1, 33) loss:69.7452
step:403000 source:(1, 53) loss:83.8606
step:403500 source:(1, 66) loss:73.6896
step:404000 source:(1, 31) loss:89.3375
step:404500 source:(1, 66) loss:62.9995
step:405000 source:(1, 31) loss:53.7465
step:405500 source:(1, 66) loss:90.6703
step:406000 source:(1, 66) loss:80.408
step:406500 source:(1, 66) loss:100.206
step:407000 source:(1, 66) loss:73.1579
step:407500 source:(1, 57) loss:72.696
step:408000 source:(1, 24) loss:72.6693
step:408500 source:(1, 59) loss:81.7184
step:409000 source:(1, 39) loss:65.5919
step:409500 source:(1, 35) loss:85.4067
step:410000 source:(1, 66) loss:65.894
step:410500 source:(1, 43) loss:66.7585
step:411000 source:(1, 54) loss:88.4507
step:411500 source:(1, 41) loss:63.357
step:412000 source:(1, 66) loss:57.5994
step:412500 source:(1, 66) loss:85.2816
step:413000 source:(1, 66) loss:83.2476
step:413500 source:(1, 44) loss:90.8602
step:414000 source:(1, 66) loss:66.5571
step:414500 source:(1, 66) loss:62.3814
step:415000 source:(1, 66) loss:73.0036
step:415500 source:(1, 51) loss:73.3946
step:416000 source:(1, 57) loss:64.2598
step:416500 source:(1, 66) loss:79.6572
step:417000 source:(1, 34) loss:73.5227
step:417500 source:(1, 66) loss:83.2908
step:418000 source:(1, 66) loss:62.7034
step:418500 source:(1, 35) loss:90.8646
step:419000 source:(1, 66) loss:83.4315
step:419500 source:(1, 35) loss:65.1467
step:420000 source:(1, 66) loss:77.503
step:420500 source:(1, 66) loss:68.4304
step:421000 source:(1, 36) loss:60.0346
step:421500 source:(1, 66) loss:81.6287
step:422000 source:(1, 48) loss:83.6346
step:422500 source:(1, 51) loss:54.7526
step:423000 source:(1, 66) loss:76.3507
step:423500 source:(1, 66) loss:62.8287
step:424000 source:(1, 66) loss:82.4597
step:424500 source:(1, 66) loss:62.0546
step:425000 source:(1, 55) loss:65.5459
step:425500 source:(1, 33) loss:72.5525
step:426000 source:(1, 66) loss:72.5604
step:426500 source:(1, 35) loss:65.604
step:427000 source:(1, 25) loss:64.6704
step:427500 source:(1, 30) loss:76.939
step:428000 source:(1, 66) loss:65.8024
step:428500 source:(1, 41) loss:81.6455
step:429000 source:(1, 59) loss:63.2218
step:429500 source:(1, 64) loss:80.5167
step:430000 source:(1, 43) loss:65.1239
step:430500 source:(1, 66) loss:70.6015
step:431000 source:(1, 66) loss:84.0051
step:431500 source:(1, 66) loss:43.8045
step:432000 source:(1, 39) loss:86.901
step:432500 source:(1, 66) loss:49.0775
step:433000 source:(1, 52) loss:80.9973
step:433500 source:(1, 66) loss:80.7431
step:434000 source:(1, 66) loss:95.297
step:434500 source:(1, 66) loss:77.7739
step:435000 source:(1, 66) loss:87.5018
step:435500 source:(1, 62) loss:66.4078
step:436000 source:(1, 66) loss:60.7152
step:436500 source:(1, 66) loss:80.1172
step:437000 source:(1, 38) loss:61.7636
step:437500 source:(1, 66) loss:66.2772
step:438000 source:(1, 30) loss:85.3618
step:438500 source:(1, 66) loss:70.4961
step:439000 source:(1, 33) loss:71.4277
step:439500 source:(1, 66) loss:64.2885
step:440000 source:(1, 30) loss:76.7898
step:440500 source:(1, 51) loss:76.6123
step:441000 source:(1, 66) loss:57.5883
step:441500 source:(1, 25) loss:57.6512
step:442000 source:(1, 39) loss:74.6046
step:442500 source:(1, 59) loss:103.287
step:443000 source:(1, 22) loss:60.6774
step:443500 source:(1, 53) loss:89.7009
step:444000 source:(1, 66) loss:69.5166
step:444500 source:(1, 48) loss:69.5395
step:445000 source:(1, 23) loss:74.8447
step:445500 source:(1, 52) loss:89.0058
step:446000 source:(1, 40) loss:67.1507
step:446500 source:(1, 66) loss:86.5712
step:447000 source:(1, 66) loss:76.7082
step:447500 source:(1, 66) loss:89.3618
step:448000 source:(1, 49) loss:63.0235
step:448500 source:(1, 27) loss:75.4073
step:449000 source:(1, 24) loss:69.3824
step:449500 source:(1, 66) loss:77.5923
step:450000 source:(1, 58) loss:81.2934
epoch:1 eval_bleu:52.422648668289185
2018-07-20 19:28:07.454709: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 19:28:07.454721: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 19:28:07.454784: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 19:28:07.455800: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 19:43:46.830784: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 19:43:46.830872: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 19:43:46.830905: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:450500 source:(1, 38) loss:71.8375
step:451000 source:(1, 29) loss:52.144
step:451500 source:(1, 32) loss:78.4146
step:452000 source:(1, 66) loss:74.9205
step:452500 source:(1, 66) loss:73.9569
step:453000 source:(1, 65) loss:65.8639
step:453500 source:(1, 66) loss:73.7539
step:454000 source:(1, 66) loss:66.2989
step:454500 source:(1, 66) loss:87.4375
step:455000 source:(1, 28) loss:71.1004
step:455500 source:(1, 61) loss:82.3082
step:456000 source:(1, 66) loss:76.8878
step:456500 source:(1, 66) loss:80.9341
step:457000 source:(1, 61) loss:72.6787
step:457500 source:(1, 22) loss:80.9048
step:458000 source:(1, 47) loss:66.1505
step:458500 source:(1, 66) loss:76.0369
step:459000 source:(1, 66) loss:93.5215
step:459500 source:(1, 41) loss:83.8053
step:460000 source:(1, 66) loss:68.9712
step:460500 source:(1, 41) loss:69.6936
step:461000 source:(1, 66) loss:65.3934
step:461500 source:(1, 66) loss:68.4073
step:462000 source:(1, 66) loss:73.0717
step:462500 source:(1, 66) loss:84.6398
step:463000 source:(1, 38) loss:64.1977
step:463500 source:(1, 66) loss:83.5428
step:464000 source:(1, 66) loss:94.851
step:464500 source:(1, 56) loss:67.0237
step:465000 source:(1, 43) loss:68.0962
step:465500 source:(1, 66) loss:66.8012
step:466000 source:(1, 41) loss:63.5837
step:466500 source:(1, 66) loss:85.327
step:467000 source:(1, 66) loss:75.0313
step:467500 source:(1, 66) loss:62.9723
step:468000 source:(1, 66) loss:79.1612
step:468500 source:(1, 66) loss:78.7984
step:469000 source:(1, 66) loss:83.7759
step:469500 source:(1, 66) loss:82.7903
step:470000 source:(1, 66) loss:76.8695
step:470500 source:(1, 42) loss:86.2751
step:471000 source:(1, 66) loss:87.4846
step:471500 source:(1, 66) loss:85.2729
step:472000 source:(1, 66) loss:82.9874
step:472500 source:(1, 24) loss:78.0043
step:473000 source:(1, 66) loss:74.6791
step:473500 source:(1, 42) loss:90.5628
step:474000 source:(1, 66) loss:97.2912
step:474500 source:(1, 66) loss:61.2417
step:475000 source:(1, 27) loss:73.2341
step:475500 source:(1, 66) loss:80.6889
step:476000 source:(1, 32) loss:74.4801
step:476500 source:(1, 49) loss:76.0556
step:477000 source:(1, 66) loss:65.6939
step:477500 source:(1, 56) loss:73.4362
step:478000 source:(1, 66) loss:67.7914
step:478500 source:(1, 66) loss:69.7565
step:479000 source:(1, 66) loss:74.5767
step:479500 source:(1, 66) loss:76.8334
step:480000 source:(1, 66) loss:62.6111
step:480500 source:(1, 46) loss:88.6554
step:481000 source:(1, 66) loss:64.3533
step:481500 source:(1, 66) loss:67.4903
step:482000 source:(1, 50) loss:75.6794
step:482500 source:(1, 44) loss:72.8206
step:483000 source:(1, 30) loss:86.0663
step:483500 source:(1, 66) loss:61.1136
step:484000 source:(1, 66) loss:70.0809
step:484500 source:(1, 66) loss:60.7344
step:485000 source:(1, 66) loss:66.1677
step:485500 source:(1, 23) loss:61.5538
step:486000 source:(1, 66) loss:76.0206
step:486500 source:(1, 34) loss:74.2957
step:487000 source:(1, 66) loss:71.7959
step:487500 source:(1, 66) loss:69.5398
step:488000 source:(1, 66) loss:69.0181
step:488500 source:(1, 46) loss:59.847
step:489000 source:(1, 66) loss:81.4171
step:489500 source:(1, 37) loss:71.0199
step:490000 source:(1, 66) loss:79.962
step:490500 source:(1, 36) loss:50.7398
step:491000 source:(1, 33) loss:67.2788
step:491500 source:(1, 55) loss:64.7151
step:492000 source:(1, 59) loss:79.9062
step:492500 source:(1, 28) loss:73.8917
step:493000 source:(1, 33) loss:68.9516
step:493500 source:(1, 32) loss:80.03
step:494000 source:(1, 66) loss:72.2555
step:494500 source:(1, 53) loss:68.3696
step:495000 source:(1, 66) loss:57.9757
step:495500 source:(1, 66) loss:82.7209
step:496000 source:(1, 66) loss:59.7101
step:496500 source:(1, 66) loss:70.9457
step:497000 source:(1, 66) loss:70.933
step:497500 source:(1, 26) loss:75.5266
step:498000 source:(1, 66) loss:71.8074
step:498500 source:(1, 66) loss:76.4832
step:499000 source:(1, 66) loss:86.75
step:499500 source:(1, 47) loss:73.9268
step:500000 source:(1, 46) loss:63.3733
step:500500 source:(1, 66) loss:70.9615
step:501000 source:(1, 66) loss:82.117
step:501500 source:(1, 63) loss:83.865
step:502000 source:(1, 31) loss:87.3375
step:502500 source:(1, 33) loss:57.2192
step:503000 source:(1, 53) loss:61.2076
step:503500 source:(1, 66) loss:71.6263
step:504000 source:(1, 31) loss:79.3991
step:504500 source:(1, 66) loss:79.8858
step:505000 source:(1, 31) loss:92.8255
step:505500 source:(1, 66) loss:86.3216
step:506000 source:(1, 66) loss:87.1798
step:506500 source:(1, 66) loss:78.8128
step:507000 source:(1, 66) loss:63.8391
step:507500 source:(1, 57) loss:95.6275
step:508000 source:(1, 24) loss:68.0932
step:508500 source:(1, 59) loss:90.9646
step:509000 source:(1, 39) loss:58.4602
step:509500 source:(1, 35) loss:100.31
step:510000 source:(1, 66) loss:72.7285
step:510500 source:(1, 43) loss:68.4764
step:511000 source:(1, 54) loss:73.3721
step:511500 source:(1, 41) loss:59.6092
step:512000 source:(1, 66) loss:84.9102
step:512500 source:(1, 66) loss:85.045
step:513000 source:(1, 66) loss:69.4289
step:513500 source:(1, 44) loss:79.0517
step:514000 source:(1, 66) loss:82.1075
step:514500 source:(1, 66) loss:70.2116
step:515000 source:(1, 66) loss:66.4353
step:515500 source:(1, 51) loss:62.8037
step:516000 source:(1, 57) loss:83.951
step:516500 source:(1, 66) loss:89.1927
step:517000 source:(1, 34) loss:84.015
step:517500 source:(1, 66) loss:80.4858
step:518000 source:(1, 66) loss:75.6399
step:518500 source:(1, 35) loss:74.7679
step:519000 source:(1, 66) loss:68.8766
step:519500 source:(1, 35) loss:71.8938
step:520000 source:(1, 66) loss:88.9472
step:520500 source:(1, 66) loss:64.5747
step:521000 source:(1, 36) loss:71.1
step:521500 source:(1, 66) loss:64.2359
step:522000 source:(1, 48) loss:71.3034
step:522500 source:(1, 51) loss:67.5932
step:523000 source:(1, 66) loss:71.7094
step:523500 source:(1, 66) loss:78.3541
step:524000 source:(1, 66) loss:79.997
step:524500 source:(1, 66) loss:77.8103
step:525000 source:(1, 55) loss:62.4444
step:525500 source:(1, 33) loss:79.1472
step:526000 source:(1, 66) loss:72.9455
step:526500 source:(1, 35) loss:80.3207
step:527000 source:(1, 25) loss:63.6537
step:527500 source:(1, 30) loss:86.7728
step:528000 source:(1, 66) loss:66.0024
step:528500 source:(1, 41) loss:61.4279
step:529000 source:(1, 59) loss:68.8074
step:529500 source:(1, 64) loss:57.7564
step:530000 source:(1, 43) loss:67.3872
step:530500 source:(1, 66) loss:80.8663
step:531000 source:(1, 66) loss:79.9947
step:531500 source:(1, 66) loss:60.5205
step:532000 source:(1, 39) loss:85.9248
step:532500 source:(1, 66) loss:78.3876
step:533000 source:(1, 52) loss:71.6061
step:533500 source:(1, 66) loss:71.7801
step:534000 source:(1, 66) loss:85.6473
step:534500 source:(1, 66) loss:75.5943
step:535000 source:(1, 66) loss:73.2467
step:535500 source:(1, 62) loss:57.0327
step:536000 source:(1, 66) loss:79.1762
step:536500 source:(1, 66) loss:60.0253
step:537000 source:(1, 38) loss:84.5286
step:537500 source:(1, 66) loss:81.3487
step:538000 source:(1, 30) loss:74.8537
step:538500 source:(1, 66) loss:56.692
step:539000 source:(1, 33) loss:76.4881
step:539500 source:(1, 66) loss:81.4099
step:540000 source:(1, 30) loss:84.2204
step:540500 source:(1, 51) loss:78.8278
step:541000 source:(1, 66) loss:65.2345
step:541500 source:(1, 25) loss:59.9011
step:542000 source:(1, 39) loss:72.5272
step:542500 source:(1, 59) loss:85.3481
step:543000 source:(1, 22) loss:79.1294
step:543500 source:(1, 53) loss:80.0461
step:544000 source:(1, 66) loss:61.1858
step:544500 source:(1, 48) loss:74.1741
step:545000 source:(1, 23) loss:76.2909
step:545500 source:(1, 52) loss:74.7152
step:546000 source:(1, 40) loss:68.2713
step:546500 source:(1, 66) loss:72.9513
step:547000 source:(1, 66) loss:88.9936
step:547500 source:(1, 66) loss:70.2148
step:548000 source:(1, 49) loss:73.337
step:548500 source:(1, 27) loss:76.9792
step:549000 source:(1, 24) loss:74.3828
step:549500 source:(1, 66) loss:103.962
step:550000 source:(1, 58) loss:76.0625
epoch:2 eval_bleu:55.81547021865845
2018-07-20 22:25:27.074177: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 22:25:27.074251: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 22:41:07.221359: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 22:41:07.221443: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-20 22:41:07.221479: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:550500 source:(1, 38) loss:74.1488
step:551000 source:(1, 29) loss:55.7798
step:551500 source:(1, 32) loss:85.1284
step:552000 source:(1, 66) loss:65.1965
step:552500 source:(1, 66) loss:94.8613
step:553000 source:(1, 65) loss:80.1448
step:553500 source:(1, 66) loss:80.0715
step:554000 source:(1, 66) loss:83.3741
step:554500 source:(1, 66) loss:86.618
step:555000 source:(1, 28) loss:54.0901
step:555500 source:(1, 61) loss:69.9124
step:556000 source:(1, 66) loss:78.5266
step:556500 source:(1, 66) loss:67.7311
step:557000 source:(1, 61) loss:72.5016
step:557500 source:(1, 22) loss:79.6885
step:558000 source:(1, 47) loss:65.9178
step:558500 source:(1, 66) loss:69.8808
step:559000 source:(1, 66) loss:89.3594
step:559500 source:(1, 41) loss:71.7157
step:560000 source:(1, 66) loss:69.0339
step:560500 source:(1, 41) loss:91.768
step:561000 source:(1, 66) loss:71.8659
step:561500 source:(1, 66) loss:75.5387
step:562000 source:(1, 66) loss:52.6504
step:562500 source:(1, 66) loss:60.6073
step:563000 source:(1, 38) loss:69.8288
step:563500 source:(1, 66) loss:84.8125
step:564000 source:(1, 66) loss:61.6309
step:564500 source:(1, 56) loss:83.9937
step:565000 source:(1, 43) loss:57.4847
step:565500 source:(1, 66) loss:69.9879
step:566000 source:(1, 41) loss:61.8166
step:566500 source:(1, 66) loss:88.01
step:567000 source:(1, 66) loss:65.6092
step:567500 source:(1, 66) loss:67.9542
step:568000 source:(1, 66) loss:75.2867
step:568500 source:(1, 66) loss:80.2778
step:569000 source:(1, 66) loss:70.3142
step:569500 source:(1, 66) loss:68.1712
step:570000 source:(1, 66) loss:83.823
step:570500 source:(1, 42) loss:91.8028
step:571000 source:(1, 66) loss:82.3662
step:571500 source:(1, 66) loss:78.345
step:572000 source:(1, 66) loss:60.7039
step:572500 source:(1, 24) loss:68.2968
step:573000 source:(1, 66) loss:68.3011
step:573500 source:(1, 42) loss:89.5811
step:574000 source:(1, 66) loss:90.3177
step:574500 source:(1, 66) loss:60.5222
step:575000 source:(1, 27) loss:72.2096
step:575500 source:(1, 66) loss:79.1256
step:576000 source:(1, 32) loss:69.9513
step:576500 source:(1, 49) loss:62.4498
step:577000 source:(1, 66) loss:63.1288
step:577500 source:(1, 56) loss:62.3149
step:578000 source:(1, 66) loss:62.9107
step:578500 source:(1, 66) loss:89.7339
step:579000 source:(1, 66) loss:73.9007
step:579500 source:(1, 66) loss:82.6771
step:580000 source:(1, 66) loss:57.0599
step:580500 source:(1, 46) loss:75.9895
step:581000 source:(1, 66) loss:77.632
step:581500 source:(1, 66) loss:77.2869
step:582000 source:(1, 50) loss:77.1777
step:582500 source:(1, 44) loss:75.706
step:583000 source:(1, 30) loss:86.5228
step:583500 source:(1, 66) loss:62.9275
step:584000 source:(1, 66) loss:77.6863
step:584500 source:(1, 66) loss:67.3619
step:585000 source:(1, 66) loss:69.2101
step:585500 source:(1, 23) loss:55.5253
step:586000 source:(1, 66) loss:77.0274
step:586500 source:(1, 34) loss:74.2162
step:587000 source:(1, 66) loss:72.3993
step:587500 source:(1, 66) loss:69.5115
step:588000 source:(1, 66) loss:63.5463
step:588500 source:(1, 46) loss:79.7591
step:589000 source:(1, 66) loss:81.1582
step:589500 source:(1, 37) loss:79.9453
step:590000 source:(1, 66) loss:73.8346
step:590500 source:(1, 36) loss:61.7472
step:591000 source:(1, 33) loss:59.8857
step:591500 source:(1, 55) loss:56.7898
step:592000 source:(1, 59) loss:94.4982
step:592500 source:(1, 28) loss:70.1387
step:593000 source:(1, 33) loss:70.7971
step:593500 source:(1, 32) loss:80.2021
step:594000 source:(1, 66) loss:63.3959
step:594500 source:(1, 53) loss:46.9429
step:595000 source:(1, 66) loss:68.4247
step:595500 source:(1, 66) loss:70.9563
step:596000 source:(1, 66) loss:71.401
step:596500 source:(1, 66) loss:71.2958
step:597000 source:(1, 66) loss:89.7717
step:597500 source:(1, 26) loss:65.1538
step:598000 source:(1, 66) loss:66.1627
step:598500 source:(1, 66) loss:86.0957
step:599000 source:(1, 66) loss:87.3623
step:599500 source:(1, 47) loss:57.2292
step:600000 source:(1, 46) loss:79.0002
step:600500 source:(1, 66) loss:64.4152
step:601000 source:(1, 66) loss:75.9397
step:601500 source:(1, 63) loss:78.3258
step:602000 source:(1, 31) loss:71.4053
step:602500 source:(1, 33) loss:56.742
step:603000 source:(1, 53) loss:55.5014
step:603500 source:(1, 66) loss:87.4398
step:604000 source:(1, 31) loss:71.8696
step:604500 source:(1, 66) loss:69.4112
step:605000 source:(1, 31) loss:85.36
step:605500 source:(1, 66) loss:85.7585
step:606000 source:(1, 66) loss:80.6187
step:606500 source:(1, 66) loss:86.6108
step:607000 source:(1, 66) loss:62.4286
step:607500 source:(1, 57) loss:86.1559
step:608000 source:(1, 24) loss:58.6329
step:608500 source:(1, 59) loss:64.2923
step:609000 source:(1, 39) loss:71.0437
step:609500 source:(1, 35) loss:82.1646
step:610000 source:(1, 66) loss:71.394
step:610500 source:(1, 43) loss:73.619
step:611000 source:(1, 54) loss:72.293
step:611500 source:(1, 41) loss:72.8057
step:612000 source:(1, 66) loss:73.2644
step:612500 source:(1, 66) loss:70.1775
step:613000 source:(1, 66) loss:74.8852
step:613500 source:(1, 44) loss:104.118
step:614000 source:(1, 66) loss:84.3494
step:614500 source:(1, 66) loss:61.8902
step:615000 source:(1, 66) loss:67.6613
step:615500 source:(1, 51) loss:70.5399
step:616000 source:(1, 57) loss:53.9583
step:616500 source:(1, 66) loss:76.4165
step:617000 source:(1, 34) loss:81.7032
step:617500 source:(1, 66) loss:74.0929
step:618000 source:(1, 66) loss:54.6048
step:618500 source:(1, 35) loss:89.6084
step:619000 source:(1, 66) loss:88.7744
step:619500 source:(1, 35) loss:53.2377
step:620000 source:(1, 66) loss:75.6018
step:620500 source:(1, 66) loss:64.7506
step:621000 source:(1, 36) loss:68.3711
step:621500 source:(1, 66) loss:66.0224
step:622000 source:(1, 48) loss:70.1501
step:622500 source:(1, 51) loss:73.6889
step:623000 source:(1, 66) loss:75.1382
step:623500 source:(1, 66) loss:63.6495
step:624000 source:(1, 66) loss:79.9818
step:624500 source:(1, 66) loss:82.3412
step:625000 source:(1, 55) loss:63.7985
step:625500 source:(1, 33) loss:74.636
step:626000 source:(1, 66) loss:59.3232
step:626500 source:(1, 35) loss:72.0165
step:627000 source:(1, 25) loss:60.2576
step:627500 source:(1, 30) loss:65.7691
step:628000 source:(1, 66) loss:56.4634
step:628500 source:(1, 41) loss:70.6408
step:629000 source:(1, 59) loss:83.8188
step:629500 source:(1, 64) loss:60.0865
step:630000 source:(1, 43) loss:73.9131
step:630500 source:(1, 66) loss:74.0491
step:631000 source:(1, 66) loss:83.1579
step:631500 source:(1, 66) loss:76.4765
step:632000 source:(1, 39) loss:93.265
step:632500 source:(1, 66) loss:75.4553
step:633000 source:(1, 52) loss:75.8878
step:633500 source:(1, 66) loss:83.4094
step:634000 source:(1, 66) loss:61.6621
step:634500 source:(1, 66) loss:66.7594
step:635000 source:(1, 66) loss:72.4401
step:635500 source:(1, 62) loss:50.7528
step:636000 source:(1, 66) loss:82.637
step:636500 source:(1, 66) loss:82.0084
step:637000 source:(1, 38) loss:73.3067
step:637500 source:(1, 66) loss:78.2153
step:638000 source:(1, 30) loss:91.2589
step:638500 source:(1, 66) loss:89.3346
step:639000 source:(1, 33) loss:68.6098
step:639500 source:(1, 66) loss:60.2947
step:640000 source:(1, 30) loss:72.9884
step:640500 source:(1, 51) loss:90.6861
step:641000 source:(1, 66) loss:65.1118
step:641500 source:(1, 25) loss:77.7286
step:642000 source:(1, 39) loss:83.6534
step:642500 source:(1, 59) loss:65.7813
step:643000 source:(1, 22) loss:73.7478
step:643500 source:(1, 53) loss:66.5983
step:644000 source:(1, 66) loss:57.7942
step:644500 source:(1, 48) loss:68.9693
step:645000 source:(1, 23) loss:81.6585
step:645500 source:(1, 52) loss:60.3638
step:646000 source:(1, 40) loss:69.4536
step:646500 source:(1, 66) loss:70.7457
step:647000 source:(1, 66) loss:68.2522
step:647500 source:(1, 66) loss:72.2934
step:648000 source:(1, 49) loss:79.3528
step:648500 source:(1, 27) loss:77.7493
step:649000 source:(1, 24) loss:67.1966
step:649500 source:(1, 66) loss:76.1203
step:650000 source:(1, 58) loss:65.8754
epoch:3 eval_bleu:53.62403392791748
2018-07-21 01:22:43.115459: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 01:22:43.115538: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 01:38:58.144081: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 01:38:58.144161: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 01:38:58.144196: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:650500 source:(1, 38) loss:75.6919
step:651000 source:(1, 29) loss:49.6873
step:651500 source:(1, 32) loss:72.7937
step:652000 source:(1, 66) loss:69.4557
step:652500 source:(1, 66) loss:71.7558
step:653000 source:(1, 65) loss:76.0864
step:653500 source:(1, 66) loss:58.9671
step:654000 source:(1, 66) loss:81.5889
step:654500 source:(1, 66) loss:101.847
step:655000 source:(1, 28) loss:60.2222
step:655500 source:(1, 61) loss:78.9345
step:656000 source:(1, 66) loss:78.1253
step:656500 source:(1, 66) loss:59.9184
step:657000 source:(1, 61) loss:67.1331
step:657500 source:(1, 22) loss:77.1908
step:658000 source:(1, 47) loss:69.9697
step:658500 source:(1, 66) loss:65.7909
step:659000 source:(1, 66) loss:91.6585
step:659500 source:(1, 41) loss:84.1418
step:660000 source:(1, 66) loss:68.6027
step:660500 source:(1, 41) loss:76.9331
step:661000 source:(1, 66) loss:79.1338
step:661500 source:(1, 66) loss:70.5428
step:662000 source:(1, 66) loss:77.0294
step:662500 source:(1, 66) loss:73.6044
step:663000 source:(1, 38) loss:89.2439
step:663500 source:(1, 66) loss:84.728
step:664000 source:(1, 66) loss:85.6547
step:664500 source:(1, 56) loss:68.8745
step:665000 source:(1, 43) loss:78.9943
step:665500 source:(1, 66) loss:80.446
step:666000 source:(1, 41) loss:57.9048
step:666500 source:(1, 66) loss:86.0244
step:667000 source:(1, 66) loss:83.1274
step:667500 source:(1, 66) loss:67.9414
step:668000 source:(1, 66) loss:69.8831
step:668500 source:(1, 66) loss:77.6796
step:669000 source:(1, 66) loss:67.0361
step:669500 source:(1, 66) loss:71.8833
step:670000 source:(1, 66) loss:83.8604
step:670500 source:(1, 42) loss:88.4904
step:671000 source:(1, 66) loss:82.262
step:671500 source:(1, 66) loss:58.5674
step:672000 source:(1, 66) loss:101.867
step:672500 source:(1, 24) loss:77.6086
step:673000 source:(1, 66) loss:68.4017
step:673500 source:(1, 42) loss:89.1152
step:674000 source:(1, 66) loss:90.4903
step:674500 source:(1, 66) loss:65.9198
step:675000 source:(1, 27) loss:71.3873
step:675500 source:(1, 66) loss:72.0481
step:676000 source:(1, 32) loss:73.9738
step:676500 source:(1, 49) loss:75.5309
step:677000 source:(1, 66) loss:64.3055
step:677500 source:(1, 56) loss:64.4027
step:678000 source:(1, 66) loss:73.6615
step:678500 source:(1, 66) loss:83.7239
step:679000 source:(1, 66) loss:63.9508
step:679500 source:(1, 66) loss:79.7051
step:680000 source:(1, 66) loss:59.9813
step:680500 source:(1, 46) loss:87.9278
step:681000 source:(1, 66) loss:64.5731
step:681500 source:(1, 66) loss:77.7001
step:682000 source:(1, 50) loss:73.2675
step:682500 source:(1, 44) loss:70.3356
step:683000 source:(1, 30) loss:89.4967
step:683500 source:(1, 66) loss:67.1214
step:684000 source:(1, 66) loss:67.9753
step:684500 source:(1, 66) loss:75.133
step:685000 source:(1, 66) loss:81.5793
step:685500 source:(1, 23) loss:56.2077
step:686000 source:(1, 66) loss:50.8154
step:686500 source:(1, 34) loss:72.135
step:687000 source:(1, 66) loss:69.5664
step:687500 source:(1, 66) loss:70.2271
step:688000 source:(1, 66) loss:83.2679
step:688500 source:(1, 46) loss:60.084
step:689000 source:(1, 66) loss:69.2331
step:689500 source:(1, 37) loss:63.1212
step:690000 source:(1, 66) loss:69.1074
step:690500 source:(1, 36) loss:75.9584
step:691000 source:(1, 33) loss:51.8707
step:691500 source:(1, 55) loss:47.3878
step:692000 source:(1, 59) loss:77.1482
step:692500 source:(1, 28) loss:74.6085
step:693000 source:(1, 33) loss:79.7918
step:693500 source:(1, 32) loss:68.0576
step:694000 source:(1, 66) loss:55.1005
step:694500 source:(1, 53) loss:63.0797
step:695000 source:(1, 66) loss:61.3801
step:695500 source:(1, 66) loss:84.7613
step:696000 source:(1, 66) loss:50.9545
step:696500 source:(1, 66) loss:68.7921
step:697000 source:(1, 66) loss:80.1108
step:697500 source:(1, 26) loss:74.8391
step:698000 source:(1, 66) loss:61.4269
step:698500 source:(1, 66) loss:67.0201
step:699000 source:(1, 66) loss:72.9275
step:699500 source:(1, 47) loss:68.3447
step:700000 source:(1, 46) loss:70.0799
step:700500 source:(1, 66) loss:69.3593
step:701000 source:(1, 66) loss:64.4429
step:701500 source:(1, 63) loss:79.2245
step:702000 source:(1, 31) loss:73.486
step:702500 source:(1, 33) loss:57.7819
step:703000 source:(1, 53) loss:75.9347
step:703500 source:(1, 66) loss:83.0838
step:704000 source:(1, 31) loss:90.033
step:704500 source:(1, 66) loss:65.588
step:705000 source:(1, 31) loss:57.8673
step:705500 source:(1, 66) loss:111.532
step:706000 source:(1, 66) loss:66.918
step:706500 source:(1, 66) loss:86.9045
step:707000 source:(1, 66) loss:75.6053
step:707500 source:(1, 57) loss:78.1036
step:708000 source:(1, 24) loss:67.1286
step:708500 source:(1, 59) loss:76.415
step:709000 source:(1, 39) loss:62.0615
step:709500 source:(1, 35) loss:96.3714
step:710000 source:(1, 66) loss:71.5448
step:710500 source:(1, 43) loss:56.3307
step:711000 source:(1, 54) loss:89.2326
step:711500 source:(1, 41) loss:68.463
step:712000 source:(1, 66) loss:71.7155
step:712500 source:(1, 66) loss:77.8958
step:713000 source:(1, 66) loss:81.3014
step:713500 source:(1, 44) loss:93.6693
step:714000 source:(1, 66) loss:92.4214
step:714500 source:(1, 66) loss:60.2992
step:715000 source:(1, 66) loss:59.9697
step:715500 source:(1, 51) loss:66.4056
step:716000 source:(1, 57) loss:67.3512
step:716500 source:(1, 66) loss:74.4503
step:717000 source:(1, 34) loss:70.1365
step:717500 source:(1, 66) loss:81.4864
step:718000 source:(1, 66) loss:58.1788
step:718500 source:(1, 35) loss:76.5016
step:719000 source:(1, 66) loss:87.8684
step:719500 source:(1, 35) loss:57.8752
step:720000 source:(1, 66) loss:70.4677
step:720500 source:(1, 66) loss:60.862
step:721000 source:(1, 36) loss:63.9379
step:721500 source:(1, 66) loss:64.0046
step:722000 source:(1, 48) loss:86.6664
step:722500 source:(1, 51) loss:46.4441
step:723000 source:(1, 66) loss:71.7811
step:723500 source:(1, 66) loss:81.1553
step:724000 source:(1, 66) loss:67.4063
step:724500 source:(1, 66) loss:76.4405
step:725000 source:(1, 55) loss:64.3287
step:725500 source:(1, 33) loss:69.0585
step:726000 source:(1, 66) loss:75.2287
step:726500 source:(1, 35) loss:72.79
step:727000 source:(1, 25) loss:68.8927
step:727500 source:(1, 30) loss:75.8777
step:728000 source:(1, 66) loss:82.866
step:728500 source:(1, 41) loss:68.2619
step:729000 source:(1, 59) loss:72.9247
step:729500 source:(1, 64) loss:55.1974
step:730000 source:(1, 43) loss:69.7563
step:730500 source:(1, 66) loss:69.6596
step:731000 source:(1, 66) loss:65.4027
step:731500 source:(1, 66) loss:53.3318
step:732000 source:(1, 39) loss:88.6986
step:732500 source:(1, 66) loss:56.6813
step:733000 source:(1, 52) loss:83.8221
step:733500 source:(1, 66) loss:62.3841
step:734000 source:(1, 66) loss:55.0586
step:734500 source:(1, 66) loss:85.0956
step:735000 source:(1, 66) loss:75.4115
step:735500 source:(1, 62) loss:65.9958
step:736000 source:(1, 66) loss:68.1546
step:736500 source:(1, 66) loss:71.6327
step:737000 source:(1, 38) loss:66.3668
step:737500 source:(1, 66) loss:59.1501
step:738000 source:(1, 30) loss:81.8177
step:738500 source:(1, 66) loss:77.4379
step:739000 source:(1, 33) loss:68.0745
step:739500 source:(1, 66) loss:71.6656
step:740000 source:(1, 30) loss:80.6979
step:740500 source:(1, 51) loss:61.2192
step:741000 source:(1, 66) loss:60.7999
step:741500 source:(1, 25) loss:59.9588
step:742000 source:(1, 39) loss:81.2465
step:742500 source:(1, 59) loss:75.2332
step:743000 source:(1, 22) loss:65.0691
step:743500 source:(1, 53) loss:89.1463
step:744000 source:(1, 66) loss:82.8
step:744500 source:(1, 48) loss:72.7684
step:745000 source:(1, 23) loss:76.4754
step:745500 source:(1, 52) loss:77.7632
step:746000 source:(1, 40) loss:63.6836
step:746500 source:(1, 66) loss:61.973
step:747000 source:(1, 66) loss:65.6703
step:747500 source:(1, 66) loss:74.4932
step:748000 source:(1, 49) loss:75.5341
step:748500 source:(1, 27) loss:82.4712
step:749000 source:(1, 24) loss:75.4676
step:749500 source:(1, 66) loss:88.03
step:750000 source:(1, 58) loss:61.6223
epoch:4 eval_bleu:55.10795712471008
2018-07-21 01:57:19.988959: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-07-21 01:57:20.169252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.81GiB
2018-07-21 01:57:20.169295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-07-21 04:59:32.871605: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 04:59:32.872109: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 04:59:32.872575: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 04:59:32.873631: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 05:15:35.599526: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 05:15:35.599603: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
train_file:/home/syp/zwr/txtgen/examples/long_text/yahoo_data/yahoo.train.txt
valid_file:/home/syp/zwr/txtgen/examples/long_text/yahoo_data/yahoo.valid.txt
logdir:./log_dir/bsize32.epoch70.lr_c2warm16000/
step:250500 source:(1, 38) loss:5.59516
step:251000 source:(1, 29) loss:3.97935
step:251500 source:(1, 32) loss:7.01507
step:252000 source:(1, 66) loss:7.06819
step:252500 source:(1, 66) loss:7.62729
step:253000 source:(1, 65) loss:6.05738
step:253500 source:(1, 66) loss:5.89917
step:254000 source:(1, 66) loss:6.44081
step:254500 source:(1, 66) loss:7.80626
step:255000 source:(1, 28) loss:5.72055
step:255500 source:(1, 61) loss:5.3696
step:256000 source:(1, 66) loss:6.19584
step:256500 source:(1, 66) loss:5.85218
step:257000 source:(1, 61) loss:6.0797
step:257500 source:(1, 22) loss:6.56074
step:258000 source:(1, 47) loss:5.97954
step:258500 source:(1, 66) loss:5.28414
step:259000 source:(1, 66) loss:6.29734
step:259500 source:(1, 41) loss:6.29187
step:260000 source:(1, 66) loss:7.13772
step:260500 source:(1, 41) loss:5.53552
step:261000 source:(1, 66) loss:5.02045
step:261500 source:(1, 66) loss:5.91681
step:262000 source:(1, 66) loss:4.86843
step:262500 source:(1, 66) loss:5.8954
step:263000 source:(1, 38) loss:5.65504
step:263500 source:(1, 66) loss:6.79517
step:264000 source:(1, 66) loss:5.27096
step:264500 source:(1, 56) loss:6.28237
step:265000 source:(1, 43) loss:5.80103
step:265500 source:(1, 66) loss:4.91706
step:266000 source:(1, 41) loss:4.87164
step:266500 source:(1, 66) loss:7.71819
step:267000 source:(1, 66) loss:6.21695
step:267500 source:(1, 66) loss:6.31913
step:268000 source:(1, 66) loss:5.95223
step:268500 source:(1, 66) loss:5.03009
step:269000 source:(1, 66) loss:5.69676
step:269500 source:(1, 66) loss:5.32083
step:270000 source:(1, 66) loss:6.29802
step:270500 source:(1, 42) loss:6.31733
step:271000 source:(1, 66) loss:7.23408
step:271500 source:(1, 66) loss:4.00986
step:272000 source:(1, 66) loss:6.50908
step:272500 source:(1, 24) loss:6.38126
step:273000 source:(1, 66) loss:5.79296
step:273500 source:(1, 42) loss:7.53418
step:274000 source:(1, 66) loss:6.43664
step:274500 source:(1, 66) loss:6.13129
step:275000 source:(1, 27) loss:6.68485
step:275500 source:(1, 66) loss:6.3714
step:276000 source:(1, 32) loss:6.4034
step:276500 source:(1, 49) loss:5.87972
step:277000 source:(1, 66) loss:5.32983
step:277500 source:(1, 56) loss:6.36963
step:278000 source:(1, 66) loss:5.67992
step:278500 source:(1, 66) loss:6.63608
step:279000 source:(1, 66) loss:5.70518
step:279500 source:(1, 66) loss:5.5651
step:280000 source:(1, 66) loss:6.59166
step:280500 source:(1, 46) loss:5.8629
step:281000 source:(1, 66) loss:7.3964
step:281500 source:(1, 66) loss:5.28568
step:282000 source:(1, 50) loss:6.59747
step:282500 source:(1, 44) loss:5.29351
step:283000 source:(1, 30) loss:7.7802
step:283500 source:(1, 66) loss:5.71382
step:284000 source:(1, 66) loss:7.02631
step:284500 source:(1, 66) loss:5.60869
step:285000 source:(1, 66) loss:8.23289
step:285500 source:(1, 23) loss:4.99474
step:286000 source:(1, 66) loss:6.05685
step:286500 source:(1, 34) loss:5.46769
step:287000 source:(1, 66) loss:6.03824
step:287500 source:(1, 66) loss:6.46109
step:288000 source:(1, 66) loss:7.13231
step:288500 source:(1, 46) loss:5.21429
step:289000 source:(1, 66) loss:5.8986
step:289500 source:(1, 37) loss:6.09344
step:290000 source:(1, 66) loss:5.34294
step:290500 source:(1, 36) loss:5.16855
step:291000 source:(1, 33) loss:5.24407
step:291500 source:(1, 55) loss:4.23814
step:292000 source:(1, 59) loss:6.99051
step:292500 source:(1, 28) loss:5.50646
step:293000 source:(1, 33) loss:5.89426
step:293500 source:(1, 32) loss:6.45141
step:294000 source:(1, 66) loss:4.98562
step:294500 source:(1, 53) loss:5.02831
step:295000 source:(1, 66) loss:5.01623
step:295500 source:(1, 66) loss:6.38261
step:296000 source:(1, 66) loss:6.7864
step:296500 source:(1, 66) loss:6.00797
step:297000 source:(1, 66) loss:7.08933
step:297500 source:(1, 26) loss:5.83746
step:298000 source:(1, 66) loss:4.5312
step:298500 source:(1, 66) loss:5.91775
step:299000 source:(1, 66) loss:6.44802
step:299500 source:(1, 47) loss:5.38795
step:300000 source:(1, 46) loss:5.5727
step:300500 source:(1, 66) loss:6.33505
step:301000 source:(1, 66) loss:5.39026
step:301500 source:(1, 63) loss:5.46324
step:302000 source:(1, 31) loss:6.87672
step:302500 source:(1, 33) loss:4.77306
step:303000 source:(1, 53) loss:6.35563
step:303500 source:(1, 66) loss:7.81214
step:304000 source:(1, 31) loss:6.05437
step:304500 source:(1, 66) loss:6.44452
step:305000 source:(1, 31) loss:6.18547
step:305500 source:(1, 66) loss:7.77271
step:306000 source:(1, 66) loss:6.68
step:306500 source:(1, 66) loss:5.88964
step:307000 source:(1, 66) loss:5.76064
step:307500 source:(1, 57) loss:6.40445
step:308000 source:(1, 24) loss:5.54764
step:308500 source:(1, 59) loss:6.49045
step:309000 source:(1, 39) loss:6.39085
step:309500 source:(1, 35) loss:7.43367
step:310000 source:(1, 66) loss:5.04303
step:310500 source:(1, 43) loss:5.57686
step:311000 source:(1, 54) loss:6.18737
step:311500 source:(1, 41) loss:5.71588
step:312000 source:(1, 66) loss:5.59374
step:312500 source:(1, 66) loss:6.57482
step:313000 source:(1, 66) loss:7.75048
step:313500 source:(1, 44) loss:7.09539
step:314000 source:(1, 66) loss:7.22762
step:314500 source:(1, 66) loss:5.46202
step:315000 source:(1, 66) loss:4.90411
step:315500 source:(1, 51) loss:6.26163
step:316000 source:(1, 57) loss:4.78169
step:316500 source:(1, 66) loss:6.6998
step:317000 source:(1, 34) loss:6.48149
step:317500 source:(1, 66) loss:6.30903
step:318000 source:(1, 66) loss:6.57374
step:318500 source:(1, 35) loss:6.20061
step:319000 source:(1, 66) loss:7.58091
step:319500 source:(1, 35) loss:5.21425
step:320000 source:(1, 66) loss:6.86208
step:320500 source:(1, 66) loss:5.35108
step:321000 source:(1, 36) loss:6.50871
step:321500 source:(1, 66) loss:6.35719
step:322000 source:(1, 48) loss:6.54607
step:322500 source:(1, 51) loss:4.24591
step:323000 source:(1, 66) loss:5.60948
step:323500 source:(1, 66) loss:5.91286
step:324000 source:(1, 66) loss:6.54093
step:324500 source:(1, 66) loss:7.00276
step:325000 source:(1, 55) loss:7.59075
step:325500 source:(1, 33) loss:5.76859
step:326000 source:(1, 66) loss:6.11232
step:326500 source:(1, 35) loss:6.60397
step:327000 source:(1, 25) loss:5.71901
step:327500 source:(1, 30) loss:6.41253
step:328000 source:(1, 66) loss:4.9943
step:328500 source:(1, 41) loss:5.33404
step:329000 source:(1, 59) loss:5.36314
step:329500 source:(1, 64) loss:4.63831
step:330000 source:(1, 43) loss:5.71789
step:330500 source:(1, 66) loss:5.55558
step:331000 source:(1, 66) loss:7.81259
step:331500 source:(1, 66) loss:6.07634
step:332000 source:(1, 39) loss:7.54633
step:332500 source:(1, 66) loss:5.68683
step:333000 source:(1, 52) loss:5.99587
step:333500 source:(1, 66) loss:6.40446
step:334000 source:(1, 66) loss:6.20989
step:334500 source:(1, 66) loss:6.49747
step:335000 source:(1, 66) loss:6.1088
step:335500 source:(1, 62) loss:5.17752
step:336000 source:(1, 66) loss:5.14895
step:336500 source:(1, 66) loss:6.1632
step:337000 source:(1, 38) loss:6.16727
step:337500 source:(1, 66) loss:6.30065
step:338000 source:(1, 30) loss:6.6559
step:338500 source:(1, 66) loss:6.35069
step:339000 source:(1, 33) loss:6.24411
step:339500 source:(1, 66) loss:5.95555
step:340000 source:(1, 30) loss:6.70183
step:340500 source:(1, 51) loss:7.18679
step:341000 source:(1, 66) loss:4.92082
step:341500 source:(1, 25) loss:5.88107
step:342000 source:(1, 39) loss:4.85879
step:342500 source:(1, 59) loss:6.30905
step:343000 source:(1, 22) loss:7.00124
step:343500 source:(1, 53) loss:6.8734
step:344000 source:(1, 66) loss:4.91996
step:344500 source:(1, 48) loss:5.9837
step:345000 source:(1, 23) loss:6.2737
step:345500 source:(1, 52) loss:4.91818
step:346000 source:(1, 40) loss:5.67151
step:346500 source:(1, 66) loss:7.21676
step:347000 source:(1, 66) loss:8.12236
step:347500 source:(1, 66) loss:5.82061
step:348000 source:(1, 49) loss:6.01744
step:348500 source:(1, 27) loss:6.33728
step:349000 source:(1, 24) loss:6.00037
step:349500 source:(1, 66) loss:6.38417
step:350000 source:(1, 58) loss:6.72711
epoch:0 eval_bleu:56.43312931060791
the 0 epoch, highest bleu 56.433129
2018-07-21 08:12:16.283662: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 08:12:16.284012: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 08:12:16.284065: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 08:12:16.285271: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 08:12:16.285276: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 08:27:44.824087: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 08:27:44.824148: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 08:27:44.824178: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:350500 source:(1, 38) loss:7.41539
step:351000 source:(1, 29) loss:3.69334
step:351500 source:(1, 32) loss:6.88536
step:352000 source:(1, 66) loss:5.09702
step:352500 source:(1, 66) loss:6.07908
step:353000 source:(1, 65) loss:6.59449
step:353500 source:(1, 66) loss:6.12915
step:354000 source:(1, 66) loss:7.5927
step:354500 source:(1, 66) loss:6.92388
step:355000 source:(1, 28) loss:4.37833
step:355500 source:(1, 61) loss:6.15611
step:356000 source:(1, 66) loss:6.79095
step:356500 source:(1, 66) loss:5.06508
step:357000 source:(1, 61) loss:6.32239
step:357500 source:(1, 22) loss:6.40949
step:358000 source:(1, 47) loss:5.55191
step:358500 source:(1, 66) loss:6.021
step:359000 source:(1, 66) loss:7.95702
step:359500 source:(1, 41) loss:6.17131
step:360000 source:(1, 66) loss:6.11731
step:360500 source:(1, 41) loss:5.10569
step:361000 source:(1, 66) loss:5.46134
step:361500 source:(1, 66) loss:5.61488
step:362000 source:(1, 66) loss:6.02112
step:362500 source:(1, 66) loss:5.80871
step:363000 source:(1, 38) loss:6.45689
step:363500 source:(1, 66) loss:6.09577
step:364000 source:(1, 66) loss:6.30327
step:364500 source:(1, 56) loss:5.54468
step:365000 source:(1, 43) loss:4.8805
step:365500 source:(1, 66) loss:5.35253
step:366000 source:(1, 41) loss:6.04811
step:366500 source:(1, 66) loss:6.86896
step:367000 source:(1, 66) loss:7.6654
step:367500 source:(1, 66) loss:6.23712
step:368000 source:(1, 66) loss:6.81332
step:368500 source:(1, 66) loss:5.95236
step:369000 source:(1, 66) loss:6.15951
step:369500 source:(1, 66) loss:6.23864
step:370000 source:(1, 66) loss:6.00193
step:370500 source:(1, 42) loss:6.03229
step:371000 source:(1, 66) loss:7.62694
step:371500 source:(1, 66) loss:4.45252
step:372000 source:(1, 66) loss:6.06405
step:372500 source:(1, 24) loss:6.2515
step:373000 source:(1, 66) loss:6.07532
step:373500 source:(1, 42) loss:7.54854
step:374000 source:(1, 66) loss:6.57782
step:374500 source:(1, 66) loss:5.65366
step:375000 source:(1, 27) loss:6.33652
step:375500 source:(1, 66) loss:5.78606
step:376000 source:(1, 32) loss:5.77581
step:376500 source:(1, 49) loss:5.42196
step:377000 source:(1, 66) loss:5.58133
step:377500 source:(1, 56) loss:6.90311
step:378000 source:(1, 66) loss:5.66653
step:378500 source:(1, 66) loss:6.91283
step:379000 source:(1, 66) loss:7.36411
step:379500 source:(1, 66) loss:5.58215
step:380000 source:(1, 66) loss:5.03382
step:380500 source:(1, 46) loss:7.01818
step:381000 source:(1, 66) loss:6.10005
step:381500 source:(1, 66) loss:4.89118
step:382000 source:(1, 50) loss:6.41809
step:382500 source:(1, 44) loss:5.1181
step:383000 source:(1, 30) loss:8.31412
step:383500 source:(1, 66) loss:7.1498
step:384000 source:(1, 66) loss:7.45787
step:384500 source:(1, 66) loss:6.0509
step:385000 source:(1, 66) loss:5.00357
step:385500 source:(1, 23) loss:4.7882
step:386000 source:(1, 66) loss:5.77916
step:386500 source:(1, 34) loss:5.3168
step:387000 source:(1, 66) loss:5.82405
step:387500 source:(1, 66) loss:4.69659
step:388000 source:(1, 66) loss:6.23875
step:388500 source:(1, 46) loss:6.65416
step:389000 source:(1, 66) loss:5.78001
step:389500 source:(1, 37) loss:6.10935
step:390000 source:(1, 66) loss:5.90901
step:390500 source:(1, 36) loss:5.68363
step:391000 source:(1, 33) loss:5.53852
step:391500 source:(1, 55) loss:5.37285
step:392000 source:(1, 59) loss:6.141
step:392500 source:(1, 28) loss:6.32528
step:393000 source:(1, 33) loss:5.44831
step:393500 source:(1, 32) loss:5.64569
step:394000 source:(1, 66) loss:4.33098
step:394500 source:(1, 53) loss:5.30795
step:395000 source:(1, 66) loss:6.09272
step:395500 source:(1, 66) loss:5.49237
step:396000 source:(1, 66) loss:5.66236
step:396500 source:(1, 66) loss:5.72948
step:397000 source:(1, 66) loss:6.18294
step:397500 source:(1, 26) loss:6.33056
step:398000 source:(1, 66) loss:5.84058
step:398500 source:(1, 66) loss:6.87306
step:399000 source:(1, 66) loss:6.75118
step:399500 source:(1, 47) loss:5.65109
step:400000 source:(1, 46) loss:5.64986
step:400500 source:(1, 66) loss:6.69648
step:401000 source:(1, 66) loss:5.44915
step:401500 source:(1, 63) loss:6.87408
step:402000 source:(1, 31) loss:6.6021
step:402500 source:(1, 33) loss:5.65029
step:403000 source:(1, 53) loss:6.75571
step:403500 source:(1, 66) loss:6.29784
step:404000 source:(1, 31) loss:7.13496
step:404500 source:(1, 66) loss:5.46721
step:405000 source:(1, 31) loss:6.01192
step:405500 source:(1, 66) loss:7.5476
step:406000 source:(1, 66) loss:7.16239
step:406500 source:(1, 66) loss:7.65455
step:407000 source:(1, 66) loss:6.44726
step:407500 source:(1, 57) loss:6.69133
step:408000 source:(1, 24) loss:5.11532
step:408500 source:(1, 59) loss:6.52392
step:409000 source:(1, 39) loss:4.73819
step:409500 source:(1, 35) loss:7.98618
step:410000 source:(1, 66) loss:6.22771
step:410500 source:(1, 43) loss:6.52275
step:411000 source:(1, 54) loss:6.98078
step:411500 source:(1, 41) loss:4.84233
step:412000 source:(1, 66) loss:6.56289
step:412500 source:(1, 66) loss:6.39279
step:413000 source:(1, 66) loss:7.78059
step:413500 source:(1, 44) loss:7.62118
step:414000 source:(1, 66) loss:7.30231
step:414500 source:(1, 66) loss:5.91093
step:415000 source:(1, 66) loss:6.94614
step:415500 source:(1, 51) loss:5.67553
step:416000 source:(1, 57) loss:5.63648
step:416500 source:(1, 66) loss:6.12624
step:417000 source:(1, 34) loss:5.80661
step:417500 source:(1, 66) loss:7.54753
step:418000 source:(1, 66) loss:5.99973
step:418500 source:(1, 35) loss:5.73967
step:419000 source:(1, 66) loss:7.33448
step:419500 source:(1, 35) loss:6.3295
step:420000 source:(1, 66) loss:7.38577
step:420500 source:(1, 66) loss:6.60391
step:421000 source:(1, 36) loss:5.85629
step:421500 source:(1, 66) loss:5.15774
step:422000 source:(1, 48) loss:6.97734
step:422500 source:(1, 51) loss:4.88889
step:423000 source:(1, 66) loss:5.39803
step:423500 source:(1, 66) loss:6.17053
step:424000 source:(1, 66) loss:7.12007
step:424500 source:(1, 66) loss:5.93181
step:425000 source:(1, 55) loss:5.1844
step:425500 source:(1, 33) loss:5.30125
step:426000 source:(1, 66) loss:5.62471
step:426500 source:(1, 35) loss:6.06749
step:427000 source:(1, 25) loss:5.65385
step:427500 source:(1, 30) loss:6.16769
step:428000 source:(1, 66) loss:4.84418
step:428500 source:(1, 41) loss:5.09299
step:429000 source:(1, 59) loss:6.7768
step:429500 source:(1, 64) loss:5.24691
step:430000 source:(1, 43) loss:4.76589
step:430500 source:(1, 66) loss:6.4117
step:431000 source:(1, 66) loss:7.04755
step:431500 source:(1, 66) loss:5.25733
step:432000 source:(1, 39) loss:7.46158
step:432500 source:(1, 66) loss:6.10038
step:433000 source:(1, 52) loss:6.28138
step:433500 source:(1, 66) loss:6.08531
step:434000 source:(1, 66) loss:5.93208
step:434500 source:(1, 66) loss:7.09784
step:435000 source:(1, 66) loss:6.45891
step:435500 source:(1, 62) loss:5.25737
step:436000 source:(1, 66) loss:5.9594
step:436500 source:(1, 66) loss:4.82583
step:437000 source:(1, 38) loss:6.2796
step:437500 source:(1, 66) loss:5.59888
step:438000 source:(1, 30) loss:7.32741
step:438500 source:(1, 66) loss:5.45074
step:439000 source:(1, 33) loss:6.33156
step:439500 source:(1, 66) loss:5.3096
step:440000 source:(1, 30) loss:6.81242
step:440500 source:(1, 51) loss:6.49058
step:441000 source:(1, 66) loss:4.29806
step:441500 source:(1, 25) loss:6.52641
step:442000 source:(1, 39) loss:6.0147
step:442500 source:(1, 59) loss:6.15777
step:443000 source:(1, 22) loss:5.12074
step:443500 source:(1, 53) loss:7.02526
step:444000 source:(1, 66) loss:5.29231
step:444500 source:(1, 48) loss:5.71984
step:445000 source:(1, 23) loss:6.62832
step:445500 source:(1, 52) loss:4.55974
step:446000 source:(1, 40) loss:5.45083
step:446500 source:(1, 66) loss:5.27307
step:447000 source:(1, 66) loss:7.92882
step:447500 source:(1, 66) loss:5.70724
step:448000 source:(1, 49) loss:6.57915
step:448500 source:(1, 27) loss:6.43945
step:449000 source:(1, 24) loss:5.95796
step:449500 source:(1, 66) loss:5.34015
step:450000 source:(1, 58) loss:5.79817
epoch:1 eval_bleu:56.50411248207092
the 1 epoch, highest bleu 56.504112
2018-07-21 11:21:20.521010: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 11:21:20.521098: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 11:21:20.522574: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 11:21:20.522637: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 11:36:42.731810: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 11:36:42.731864: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 11:36:42.731884: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:450500 source:(1, 38) loss:6.31053
step:451000 source:(1, 29) loss:4.53981
step:451500 source:(1, 32) loss:6.37295
step:452000 source:(1, 66) loss:5.04762
step:452500 source:(1, 66) loss:6.64744
step:453000 source:(1, 65) loss:6.48319
step:453500 source:(1, 66) loss:5.92233
step:454000 source:(1, 66) loss:5.80411
step:454500 source:(1, 66) loss:8.19019
step:455000 source:(1, 28) loss:5.18732
step:455500 source:(1, 61) loss:5.91446
step:456000 source:(1, 66) loss:5.45557
step:456500 source:(1, 66) loss:5.55078
step:457000 source:(1, 61) loss:5.5709
step:457500 source:(1, 22) loss:6.71787
step:458000 source:(1, 47) loss:6.02156
step:458500 source:(1, 66) loss:5.79863
step:459000 source:(1, 66) loss:9.78372
step:459500 source:(1, 41) loss:6.45448
step:460000 source:(1, 66) loss:6.99191
step:460500 source:(1, 41) loss:5.99841
step:461000 source:(1, 66) loss:4.8832
step:461500 source:(1, 66) loss:5.93411
step:462000 source:(1, 66) loss:4.38265
step:462500 source:(1, 66) loss:6.05415
step:463000 source:(1, 38) loss:5.51988
step:463500 source:(1, 66) loss:6.79101
step:464000 source:(1, 66) loss:5.3706
step:464500 source:(1, 56) loss:6.18546
step:465000 source:(1, 43) loss:6.28836
step:465500 source:(1, 66) loss:5.85244
step:466000 source:(1, 41) loss:5.74917
step:466500 source:(1, 66) loss:7.5318
step:467000 source:(1, 66) loss:5.90423
step:467500 source:(1, 66) loss:5.6172
step:468000 source:(1, 66) loss:6.65078
step:468500 source:(1, 66) loss:6.08792
step:469000 source:(1, 66) loss:6.97306
step:469500 source:(1, 66) loss:5.21756
step:470000 source:(1, 66) loss:6.70367
step:470500 source:(1, 42) loss:6.49673
step:471000 source:(1, 66) loss:7.61117
step:471500 source:(1, 66) loss:5.6856
step:472000 source:(1, 66) loss:5.83002
step:472500 source:(1, 24) loss:6.90511
step:473000 source:(1, 66) loss:6.27189
step:473500 source:(1, 42) loss:7.26519
step:474000 source:(1, 66) loss:6.78158
step:474500 source:(1, 66) loss:6.84322
step:475000 source:(1, 27) loss:6.9113
step:475500 source:(1, 66) loss:6.93301
step:476000 source:(1, 32) loss:6.03047
step:476500 source:(1, 49) loss:5.86928
step:477000 source:(1, 66) loss:6.23448
step:477500 source:(1, 56) loss:6.73056
step:478000 source:(1, 66) loss:4.9376
step:478500 source:(1, 66) loss:7.06083
step:479000 source:(1, 66) loss:8.26125
step:479500 source:(1, 66) loss:7.26707
step:480000 source:(1, 66) loss:6.19789
step:480500 source:(1, 46) loss:6.52099
step:481000 source:(1, 66) loss:6.24911
step:481500 source:(1, 66) loss:6.43718
step:482000 source:(1, 50) loss:5.75758
step:482500 source:(1, 44) loss:6.71954
step:483000 source:(1, 30) loss:8.29327
step:483500 source:(1, 66) loss:5.59139
step:484000 source:(1, 66) loss:5.88987
step:484500 source:(1, 66) loss:7.44967
step:485000 source:(1, 66) loss:7.40243
step:485500 source:(1, 23) loss:4.83543
step:486000 source:(1, 66) loss:5.71579
step:486500 source:(1, 34) loss:6.08513
step:487000 source:(1, 66) loss:5.95538
step:487500 source:(1, 66) loss:5.59605
step:488000 source:(1, 66) loss:7.25107
step:488500 source:(1, 46) loss:5.63922
step:489000 source:(1, 66) loss:5.68951
step:489500 source:(1, 37) loss:5.89991
step:490000 source:(1, 66) loss:5.54148
step:490500 source:(1, 36) loss:6.134
step:491000 source:(1, 33) loss:5.09171
step:491500 source:(1, 55) loss:4.33572
step:492000 source:(1, 59) loss:6.3376
step:492500 source:(1, 28) loss:6.01128
step:493000 source:(1, 33) loss:5.85268
step:493500 source:(1, 32) loss:6.39884
step:494000 source:(1, 66) loss:6.05773
step:494500 source:(1, 53) loss:5.06773
step:495000 source:(1, 66) loss:3.56543
step:495500 source:(1, 66) loss:7.02591
step:496000 source:(1, 66) loss:5.50406
step:496500 source:(1, 66) loss:5.45239
step:497000 source:(1, 66) loss:6.8112
step:497500 source:(1, 26) loss:5.63506
step:498000 source:(1, 66) loss:6.15852
step:498500 source:(1, 66) loss:5.67112
step:499000 source:(1, 66) loss:7.35402
step:499500 source:(1, 47) loss:5.9692
step:500000 source:(1, 46) loss:5.74307
step:500500 source:(1, 66) loss:6.43466
step:501000 source:(1, 66) loss:3.85016
step:501500 source:(1, 63) loss:6.84434
step:502000 source:(1, 31) loss:6.66587
step:502500 source:(1, 33) loss:4.74139
step:503000 source:(1, 53) loss:6.116
step:503500 source:(1, 66) loss:6.74413
step:504000 source:(1, 31) loss:6.02742
step:504500 source:(1, 66) loss:6.03823
step:505000 source:(1, 31) loss:4.32035
step:505500 source:(1, 66) loss:8.9663
step:506000 source:(1, 66) loss:6.15479
step:506500 source:(1, 66) loss:7.57754
step:507000 source:(1, 66) loss:5.62514
step:507500 source:(1, 57) loss:6.71177
step:508000 source:(1, 24) loss:5.57374
step:508500 source:(1, 59) loss:6.85193
step:509000 source:(1, 39) loss:5.31145
step:509500 source:(1, 35) loss:7.16994
step:510000 source:(1, 66) loss:4.92011
step:510500 source:(1, 43) loss:6.27909
step:511000 source:(1, 54) loss:6.46203
step:511500 source:(1, 41) loss:4.91566
step:512000 source:(1, 66) loss:6.45551
step:512500 source:(1, 66) loss:7.08433
step:513000 source:(1, 66) loss:7.24667
step:513500 source:(1, 44) loss:8.78625
step:514000 source:(1, 66) loss:6.55855
step:514500 source:(1, 66) loss:5.8452
step:515000 source:(1, 66) loss:5.15285
step:515500 source:(1, 51) loss:5.75495
step:516000 source:(1, 57) loss:6.15279
step:516500 source:(1, 66) loss:6.80339
step:517000 source:(1, 34) loss:7.6102
step:517500 source:(1, 66) loss:5.75709
step:518000 source:(1, 66) loss:6.3515
step:518500 source:(1, 35) loss:6.28448
step:519000 source:(1, 66) loss:6.47673
step:519500 source:(1, 35) loss:6.04745
step:520000 source:(1, 66) loss:5.97395
step:520500 source:(1, 66) loss:6.30703
step:521000 source:(1, 36) loss:6.59377
step:521500 source:(1, 66) loss:5.31058
step:522000 source:(1, 48) loss:5.85147
step:522500 source:(1, 51) loss:4.13712
step:523000 source:(1, 66) loss:5.86294
step:523500 source:(1, 66) loss:6.81651
step:524000 source:(1, 66) loss:7.29484
step:524500 source:(1, 66) loss:5.74081
step:525000 source:(1, 55) loss:5.42223
step:525500 source:(1, 33) loss:6.89856
step:526000 source:(1, 66) loss:6.4721
step:526500 source:(1, 35) loss:5.75076
step:527000 source:(1, 25) loss:5.36865
step:527500 source:(1, 30) loss:6.00983
step:528000 source:(1, 66) loss:4.91643
step:528500 source:(1, 41) loss:5.59444
step:529000 source:(1, 59) loss:5.6432
step:529500 source:(1, 64) loss:6.70104
step:530000 source:(1, 43) loss:4.85458
step:530500 source:(1, 66) loss:5.29455
step:531000 source:(1, 66) loss:6.88327
step:531500 source:(1, 66) loss:5.53628
step:532000 source:(1, 39) loss:7.17237
step:532500 source:(1, 66) loss:5.09365
step:533000 source:(1, 52) loss:5.84137
step:533500 source:(1, 66) loss:5.54311
step:534000 source:(1, 66) loss:5.37659
step:534500 source:(1, 66) loss:5.9695
step:535000 source:(1, 66) loss:5.89609
step:535500 source:(1, 62) loss:5.39024
step:536000 source:(1, 66) loss:4.99004
step:536500 source:(1, 66) loss:6.02925
step:537000 source:(1, 38) loss:6.92388
step:537500 source:(1, 66) loss:6.75212
step:538000 source:(1, 30) loss:6.51676
step:538500 source:(1, 66) loss:6.03532
step:539000 source:(1, 33) loss:5.30947
step:539500 source:(1, 66) loss:6.61231
step:540000 source:(1, 30) loss:7.07617
step:540500 source:(1, 51) loss:6.1502
step:541000 source:(1, 66) loss:4.81133
step:541500 source:(1, 25) loss:6.26826
step:542000 source:(1, 39) loss:6.17382
step:542500 source:(1, 59) loss:7.44471
step:543000 source:(1, 22) loss:6.89683
step:543500 source:(1, 53) loss:4.98083
step:544000 source:(1, 66) loss:5.39258
step:544500 source:(1, 48) loss:5.08479
step:545000 source:(1, 23) loss:6.45539
step:545500 source:(1, 52) loss:4.8541
step:546000 source:(1, 40) loss:6.13637
step:546500 source:(1, 66) loss:8.00413
step:547000 source:(1, 66) loss:7.08982
step:547500 source:(1, 66) loss:5.48407
step:548000 source:(1, 49) loss:6.1491
step:548500 source:(1, 27) loss:7.76656
step:549000 source:(1, 24) loss:6.49851
step:549500 source:(1, 66) loss:7.36551
step:550000 source:(1, 58) loss:6.62193
epoch:2 eval_bleu:54.89278435707092
2018-07-21 14:27:39.681632: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 14:27:39.681719: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 14:43:04.708818: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 14:43:04.710779: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 14:43:04.710817: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:550500 source:(1, 38) loss:5.56077
step:551000 source:(1, 29) loss:5.72661
step:551500 source:(1, 32) loss:5.85164
step:552000 source:(1, 66) loss:5.64417
step:552500 source:(1, 66) loss:5.80522
step:553000 source:(1, 65) loss:6.31816
step:553500 source:(1, 66) loss:5.8545
step:554000 source:(1, 66) loss:7.1877
step:554500 source:(1, 66) loss:7.70563
step:555000 source:(1, 28) loss:5.27552
step:555500 source:(1, 61) loss:5.64868
step:556000 source:(1, 66) loss:6.48576
step:556500 source:(1, 66) loss:5.85153
step:557000 source:(1, 61) loss:6.91322
step:557500 source:(1, 22) loss:6.61619
step:558000 source:(1, 47) loss:5.13633
step:558500 source:(1, 66) loss:5.75103
step:559000 source:(1, 66) loss:6.42789
step:559500 source:(1, 41) loss:5.68535
step:560000 source:(1, 66) loss:6.88143
step:560500 source:(1, 41) loss:6.59698
step:561000 source:(1, 66) loss:6.49516
step:561500 source:(1, 66) loss:5.91325
step:562000 source:(1, 66) loss:4.33489
step:562500 source:(1, 66) loss:5.48835
step:563000 source:(1, 38) loss:5.73598
step:563500 source:(1, 66) loss:6.90274
step:564000 source:(1, 66) loss:6.49456
step:564500 source:(1, 56) loss:5.94078
step:565000 source:(1, 43) loss:5.30466
step:565500 source:(1, 66) loss:5.73273
step:566000 source:(1, 41) loss:5.21444
step:566500 source:(1, 66) loss:5.78532
step:567000 source:(1, 66) loss:5.60579
step:567500 source:(1, 66) loss:5.21032
step:568000 source:(1, 66) loss:5.82765
step:568500 source:(1, 66) loss:7.3445
step:569000 source:(1, 66) loss:6.98961
step:569500 source:(1, 66) loss:5.90188
step:570000 source:(1, 66) loss:6.80095
step:570500 source:(1, 42) loss:6.69053
step:571000 source:(1, 66) loss:5.96574
step:571500 source:(1, 66) loss:5.44048
step:572000 source:(1, 66) loss:6.40439
step:572500 source:(1, 24) loss:6.38236
step:573000 source:(1, 66) loss:7.26175
step:573500 source:(1, 42) loss:6.87118
step:574000 source:(1, 66) loss:6.9622
step:574500 source:(1, 66) loss:6.80351
step:575000 source:(1, 27) loss:6.16355
step:575500 source:(1, 66) loss:5.80636
step:576000 source:(1, 32) loss:5.6988
step:576500 source:(1, 49) loss:4.67595
step:577000 source:(1, 66) loss:5.63114
step:577500 source:(1, 56) loss:6.03934
step:578000 source:(1, 66) loss:7.03225
step:578500 source:(1, 66) loss:4.72215
step:579000 source:(1, 66) loss:6.68796
step:579500 source:(1, 66) loss:5.63368
step:580000 source:(1, 66) loss:5.48514
step:580500 source:(1, 46) loss:6.98424
step:581000 source:(1, 66) loss:6.73207
step:581500 source:(1, 66) loss:5.49396
step:582000 source:(1, 50) loss:7.09478
step:582500 source:(1, 44) loss:6.27296
step:583000 source:(1, 30) loss:8.07606
step:583500 source:(1, 66) loss:5.71623
step:584000 source:(1, 66) loss:6.17905
step:584500 source:(1, 66) loss:5.26214
step:585000 source:(1, 66) loss:4.90812
step:585500 source:(1, 23) loss:5.37774
step:586000 source:(1, 66) loss:7.41407
step:586500 source:(1, 34) loss:6.21425
step:587000 source:(1, 66) loss:5.55184
step:587500 source:(1, 66) loss:6.149
step:588000 source:(1, 66) loss:5.9651
step:588500 source:(1, 46) loss:5.74285
step:589000 source:(1, 66) loss:5.36525
step:589500 source:(1, 37) loss:6.31529
step:590000 source:(1, 66) loss:6.11637
step:590500 source:(1, 36) loss:4.35386
step:591000 source:(1, 33) loss:5.58829
step:591500 source:(1, 55) loss:4.92449
step:592000 source:(1, 59) loss:6.37516
step:592500 source:(1, 28) loss:5.76968
step:593000 source:(1, 33) loss:5.8247
step:593500 source:(1, 32) loss:5.40947
step:594000 source:(1, 66) loss:5.26968
step:594500 source:(1, 53) loss:5.50266
step:595000 source:(1, 66) loss:4.7963
step:595500 source:(1, 66) loss:5.69936
step:596000 source:(1, 66) loss:4.41876
step:596500 source:(1, 66) loss:6.01797
step:597000 source:(1, 66) loss:7.08991
step:597500 source:(1, 26) loss:5.34926
step:598000 source:(1, 66) loss:5.17298
step:598500 source:(1, 66) loss:6.66701
step:599000 source:(1, 66) loss:7.55714
step:599500 source:(1, 47) loss:5.98949
step:600000 source:(1, 46) loss:5.77938
step:600500 source:(1, 66) loss:6.25079
step:601000 source:(1, 66) loss:5.46871
step:601500 source:(1, 63) loss:6.42453
step:602000 source:(1, 31) loss:8.28606
step:602500 source:(1, 33) loss:4.92642
step:603000 source:(1, 53) loss:7.00644
step:603500 source:(1, 66) loss:6.09126
step:604000 source:(1, 31) loss:7.19564
step:604500 source:(1, 66) loss:5.01054
step:605000 source:(1, 31) loss:6.72947
step:605500 source:(1, 66) loss:7.78958
step:606000 source:(1, 66) loss:6.31892
step:606500 source:(1, 66) loss:5.97508
step:607000 source:(1, 66) loss:5.4072
step:607500 source:(1, 57) loss:5.67171
step:608000 source:(1, 24) loss:5.51056
step:608500 source:(1, 59) loss:4.43951
step:609000 source:(1, 39) loss:4.83207
step:609500 source:(1, 35) loss:7.94891
step:610000 source:(1, 66) loss:5.73464
step:610500 source:(1, 43) loss:6.54314
step:611000 source:(1, 54) loss:6.10933
step:611500 source:(1, 41) loss:5.07542
step:612000 source:(1, 66) loss:6.58209
step:612500 source:(1, 66) loss:6.11502
step:613000 source:(1, 66) loss:7.26244
step:613500 source:(1, 44) loss:8.43192
step:614000 source:(1, 66) loss:6.30227
step:614500 source:(1, 66) loss:5.9169
step:615000 source:(1, 66) loss:4.74967
step:615500 source:(1, 51) loss:4.89371
step:616000 source:(1, 57) loss:5.22812
step:616500 source:(1, 66) loss:5.71258
step:617000 source:(1, 34) loss:6.62962
step:617500 source:(1, 66) loss:5.85262
step:618000 source:(1, 66) loss:5.1478
step:618500 source:(1, 35) loss:6.08318
step:619000 source:(1, 66) loss:6.92164
step:619500 source:(1, 35) loss:4.93452
step:620000 source:(1, 66) loss:6.67927
step:620500 source:(1, 66) loss:5.11319
step:621000 source:(1, 36) loss:5.42675
step:621500 source:(1, 66) loss:5.67987
step:622000 source:(1, 48) loss:6.17922
step:622500 source:(1, 51) loss:3.33018
step:623000 source:(1, 66) loss:6.01447
step:623500 source:(1, 66) loss:6.31561
step:624000 source:(1, 66) loss:6.80648
step:624500 source:(1, 66) loss:8.53557
step:625000 source:(1, 55) loss:5.45403
step:625500 source:(1, 33) loss:6.06438
step:626000 source:(1, 66) loss:5.7112
step:626500 source:(1, 35) loss:6.91327
step:627000 source:(1, 25) loss:6.23895
step:627500 source:(1, 30) loss:5.75185
step:628000 source:(1, 66) loss:4.74351
step:628500 source:(1, 41) loss:5.19663
step:629000 source:(1, 59) loss:6.20484
step:629500 source:(1, 64) loss:7.27253
step:630000 source:(1, 43) loss:6.10962
step:630500 source:(1, 66) loss:6.08761
step:631000 source:(1, 66) loss:7.21375
step:631500 source:(1, 66) loss:5.71101
step:632000 source:(1, 39) loss:7.72455
step:632500 source:(1, 66) loss:5.85097
step:633000 source:(1, 52) loss:5.69364
step:633500 source:(1, 66) loss:6.08191
step:634000 source:(1, 66) loss:5.51982
step:634500 source:(1, 66) loss:6.56869
step:635000 source:(1, 66) loss:7.03216
step:635500 source:(1, 62) loss:5.26705
step:636000 source:(1, 66) loss:5.87986
step:636500 source:(1, 66) loss:5.04149
step:637000 source:(1, 38) loss:5.90172
step:637500 source:(1, 66) loss:6.59268
step:638000 source:(1, 30) loss:6.77015
step:638500 source:(1, 66) loss:6.6414
step:639000 source:(1, 33) loss:5.88812
step:639500 source:(1, 66) loss:6.05772
step:640000 source:(1, 30) loss:6.30699
step:640500 source:(1, 51) loss:5.21646
step:641000 source:(1, 66) loss:5.73088
step:641500 source:(1, 25) loss:6.35601
step:642000 source:(1, 39) loss:6.55189
step:642500 source:(1, 59) loss:7.96479
step:643000 source:(1, 22) loss:7.05708
step:643500 source:(1, 53) loss:6.28544
step:644000 source:(1, 66) loss:6.03658
step:644500 source:(1, 48) loss:4.61215
step:645000 source:(1, 23) loss:5.6449
step:645500 source:(1, 52) loss:4.83546
step:646000 source:(1, 40) loss:6.15247
step:646500 source:(1, 66) loss:7.16952
step:647000 source:(1, 66) loss:7.39761
step:647500 source:(1, 66) loss:5.50087
step:648000 source:(1, 49) loss:5.95573
step:648500 source:(1, 27) loss:6.65071
step:649000 source:(1, 24) loss:6.28614
step:649500 source:(1, 66) loss:7.65733
step:650000 source:(1, 58) loss:5.843
epoch:3 eval_bleu:51.05096101760864
2018-07-21 17:42:43.521050: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 17:42:43.521111: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 17:59:43.495053: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 17:59:43.495122: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 17:59:43.495147: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:650500 source:(1, 38) loss:4.89114
step:651000 source:(1, 29) loss:3.66433
step:651500 source:(1, 32) loss:7.02203
step:652000 source:(1, 66) loss:5.7363
step:652500 source:(1, 66) loss:5.46472
step:653000 source:(1, 65) loss:5.94531
step:653500 source:(1, 66) loss:5.98282
step:654000 source:(1, 66) loss:5.51115
step:654500 source:(1, 66) loss:5.25134
step:655000 source:(1, 28) loss:5.67839
step:655500 source:(1, 61) loss:5.7551
step:656000 source:(1, 66) loss:5.93695
step:656500 source:(1, 66) loss:4.56064
step:657000 source:(1, 61) loss:7.0666
step:657500 source:(1, 22) loss:6.32755
step:658000 source:(1, 47) loss:5.65167
step:658500 source:(1, 66) loss:6.51144
step:659000 source:(1, 66) loss:6.62554
step:659500 source:(1, 41) loss:5.42779
step:660000 source:(1, 66) loss:6.26778
step:660500 source:(1, 41) loss:5.14074
step:661000 source:(1, 66) loss:4.88625
step:661500 source:(1, 66) loss:5.3879
step:662000 source:(1, 66) loss:5.24829
step:662500 source:(1, 66) loss:6.39276
step:663000 source:(1, 38) loss:5.42105
step:663500 source:(1, 66) loss:6.77264
step:664000 source:(1, 66) loss:5.60681
step:664500 source:(1, 56) loss:5.83413
step:665000 source:(1, 43) loss:6.52161
step:665500 source:(1, 66) loss:6.32552
step:666000 source:(1, 41) loss:5.51934
step:666500 source:(1, 66) loss:7.75922
step:667000 source:(1, 66) loss:5.79611
step:667500 source:(1, 66) loss:4.80202
step:668000 source:(1, 66) loss:6.63834
step:668500 source:(1, 66) loss:6.94286
step:669000 source:(1, 66) loss:7.11524
step:669500 source:(1, 66) loss:5.86416
step:670000 source:(1, 66) loss:7.39921
step:670500 source:(1, 42) loss:6.4503
step:671000 source:(1, 66) loss:6.57679
step:671500 source:(1, 66) loss:4.3817
step:672000 source:(1, 66) loss:4.66545
step:672500 source:(1, 24) loss:6.0766
step:673000 source:(1, 66) loss:6.50579
step:673500 source:(1, 42) loss:6.99682
step:674000 source:(1, 66) loss:6.12286
step:674500 source:(1, 66) loss:5.67042
step:675000 source:(1, 27) loss:5.85757
step:675500 source:(1, 66) loss:5.48407
step:676000 source:(1, 32) loss:6.38416
step:676500 source:(1, 49) loss:5.67218
step:677000 source:(1, 66) loss:4.40852
step:677500 source:(1, 56) loss:5.05021
step:678000 source:(1, 66) loss:5.54878
step:678500 source:(1, 66) loss:6.52024
step:679000 source:(1, 66) loss:7.37641
step:679500 source:(1, 66) loss:6.23292
step:680000 source:(1, 66) loss:6.67716
step:680500 source:(1, 46) loss:5.64242
step:681000 source:(1, 66) loss:6.28451
step:681500 source:(1, 66) loss:6.88484
step:682000 source:(1, 50) loss:6.59162
step:682500 source:(1, 44) loss:7.30271
step:683000 source:(1, 30) loss:6.98311
step:683500 source:(1, 66) loss:7.3863
step:684000 source:(1, 66) loss:5.39698
step:684500 source:(1, 66) loss:6.16652
step:685000 source:(1, 66) loss:6.64068
step:685500 source:(1, 23) loss:4.51993
step:686000 source:(1, 66) loss:5.91989
step:686500 source:(1, 34) loss:5.64646
step:687000 source:(1, 66) loss:5.6096
step:687500 source:(1, 66) loss:5.59219
step:688000 source:(1, 66) loss:6.30382
step:688500 source:(1, 46) loss:6.8932
step:689000 source:(1, 66) loss:6.3069
step:689500 source:(1, 37) loss:6.0503
step:690000 source:(1, 66) loss:7.87001
step:690500 source:(1, 36) loss:6.40253
step:691000 source:(1, 33) loss:5.35958
step:691500 source:(1, 55) loss:4.92773
step:692000 source:(1, 59) loss:7.04103
step:692500 source:(1, 28) loss:5.49788
step:693000 source:(1, 33) loss:6.11467
step:693500 source:(1, 32) loss:5.72385
step:694000 source:(1, 66) loss:5.5231
step:694500 source:(1, 53) loss:4.43581
step:695000 source:(1, 66) loss:5.18845
step:695500 source:(1, 66) loss:6.41025
step:696000 source:(1, 66) loss:5.0058
step:696500 source:(1, 66) loss:7.17705
step:697000 source:(1, 66) loss:6.68033
step:697500 source:(1, 26) loss:5.41452
step:698000 source:(1, 66) loss:5.71709
step:698500 source:(1, 66) loss:6.96854
step:699000 source:(1, 66) loss:6.39538
step:699500 source:(1, 47) loss:6.93137
step:700000 source:(1, 46) loss:5.5112
step:700500 source:(1, 66) loss:6.18209
step:701000 source:(1, 66) loss:4.85662
step:701500 source:(1, 63) loss:6.7376
step:702000 source:(1, 31) loss:7.37269
step:702500 source:(1, 33) loss:5.02973
step:703000 source:(1, 53) loss:6.27048
step:703500 source:(1, 66) loss:6.42981
step:704000 source:(1, 31) loss:7.67339
step:704500 source:(1, 66) loss:5.67341
step:705000 source:(1, 31) loss:5.73671
step:705500 source:(1, 66) loss:9.16903
step:706000 source:(1, 66) loss:5.5062
step:706500 source:(1, 66) loss:5.34291
step:707000 source:(1, 66) loss:4.52214
step:707500 source:(1, 57) loss:5.19807
step:708000 source:(1, 24) loss:5.50846
step:708500 source:(1, 59) loss:6.05975
step:709000 source:(1, 39) loss:5.37032
step:709500 source:(1, 35) loss:8.55002
step:710000 source:(1, 66) loss:5.32029
step:710500 source:(1, 43) loss:5.2654
step:711000 source:(1, 54) loss:6.16035
step:711500 source:(1, 41) loss:4.55277
step:712000 source:(1, 66) loss:5.16525
step:712500 source:(1, 66) loss:7.1321
step:713000 source:(1, 66) loss:6.86574
step:713500 source:(1, 44) loss:8.22394
step:714000 source:(1, 66) loss:5.79079
step:714500 source:(1, 66) loss:5.25855
step:715000 source:(1, 66) loss:5.27905
step:715500 source:(1, 51) loss:4.82058
step:716000 source:(1, 57) loss:6.90801
step:716500 source:(1, 66) loss:6.12498
step:717000 source:(1, 34) loss:7.98513
step:717500 source:(1, 66) loss:6.47636
step:718000 source:(1, 66) loss:6.43126
step:718500 source:(1, 35) loss:5.08613
step:719000 source:(1, 66) loss:7.28192
step:719500 source:(1, 35) loss:4.86339
step:720000 source:(1, 66) loss:7.4687
step:720500 source:(1, 66) loss:4.97971
step:721000 source:(1, 36) loss:5.55308
step:721500 source:(1, 66) loss:6.80646
step:722000 source:(1, 48) loss:6.65773
step:722500 source:(1, 51) loss:4.6131
step:723000 source:(1, 66) loss:5.89951
step:723500 source:(1, 66) loss:6.61
step:724000 source:(1, 66) loss:5.89147
step:724500 source:(1, 66) loss:5.89622
step:725000 source:(1, 55) loss:5.8533
step:725500 source:(1, 33) loss:7.42927
step:726000 source:(1, 66) loss:5.95244
step:726500 source:(1, 35) loss:7.19895
step:727000 source:(1, 25) loss:5.60269
step:727500 source:(1, 30) loss:5.34733
step:728000 source:(1, 66) loss:4.18873
step:728500 source:(1, 41) loss:6.19447
step:729000 source:(1, 59) loss:6.62207
step:729500 source:(1, 64) loss:4.43953
step:730000 source:(1, 43) loss:6.0221
step:730500 source:(1, 66) loss:6.32895
step:731000 source:(1, 66) loss:6.63448
step:731500 source:(1, 66) loss:4.40664
step:732000 source:(1, 39) loss:7.69117
step:732500 source:(1, 66) loss:5.59406
step:733000 source:(1, 52) loss:6.63312
step:733500 source:(1, 66) loss:7.48335
step:734000 source:(1, 66) loss:5.06567
step:734500 source:(1, 66) loss:5.7928
step:735000 source:(1, 66) loss:4.77705
step:735500 source:(1, 62) loss:4.97123
step:736000 source:(1, 66) loss:6.2689
step:736500 source:(1, 66) loss:5.54229
step:737000 source:(1, 38) loss:5.96324
step:737500 source:(1, 66) loss:6.13764
step:738000 source:(1, 30) loss:6.30347
step:738500 source:(1, 66) loss:7.7254
step:739000 source:(1, 33) loss:5.57684
step:739500 source:(1, 66) loss:6.00527
step:740000 source:(1, 30) loss:7.55349
step:740500 source:(1, 51) loss:6.18489
step:741000 source:(1, 66) loss:4.69746
step:741500 source:(1, 25) loss:4.65378
step:742000 source:(1, 39) loss:7.31284
step:742500 source:(1, 59) loss:7.18426
step:743000 source:(1, 22) loss:5.5134
step:743500 source:(1, 53) loss:7.57431
step:744000 source:(1, 66) loss:4.90765
step:744500 source:(1, 48) loss:5.96908
step:745000 source:(1, 23) loss:5.8628
step:745500 source:(1, 52) loss:6.40002
step:746000 source:(1, 40) loss:5.47702
step:746500 source:(1, 66) loss:7.6554
step:747000 source:(1, 66) loss:6.13677
step:747500 source:(1, 66) loss:5.92059
step:748000 source:(1, 49) loss:5.37892
step:748500 source:(1, 27) loss:6.1285
step:749000 source:(1, 24) loss:6.4181
step:749500 source:(1, 66) loss:6.21251
step:750000 source:(1, 58) loss:5.76824
epoch:4 eval_bleu:54.64844107627869
2018-07-21 21:20:59.199885: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 21:20:59.200020: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 21:20:59.201576: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 21:20:59.201628: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 21:37:57.952057: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 21:37:57.952146: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-21 21:37:57.952186: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:750500 source:(1, 38) loss:5.80182
step:751000 source:(1, 29) loss:3.04665
step:751500 source:(1, 32) loss:6.37157
step:752000 source:(1, 66) loss:5.89921
step:752500 source:(1, 66) loss:6.76312
step:753000 source:(1, 65) loss:6.17834
step:753500 source:(1, 66) loss:6.27253
step:754000 source:(1, 66) loss:6.49218
step:754500 source:(1, 66) loss:7.33041
step:755000 source:(1, 28) loss:5.10268
step:755500 source:(1, 61) loss:6.82997
step:756000 source:(1, 66) loss:7.05205
step:756500 source:(1, 66) loss:5.71424
step:757000 source:(1, 61) loss:5.47211
step:757500 source:(1, 22) loss:6.47839
step:758000 source:(1, 47) loss:5.47552
step:758500 source:(1, 66) loss:6.23092
step:759000 source:(1, 66) loss:7.58398
step:759500 source:(1, 41) loss:7.35996
step:760000 source:(1, 66) loss:6.282
step:760500 source:(1, 41) loss:5.52266
step:761000 source:(1, 66) loss:5.51932
step:761500 source:(1, 66) loss:5.67217
step:762000 source:(1, 66) loss:6.92063
step:762500 source:(1, 66) loss:5.30237
step:763000 source:(1, 38) loss:6.26388
step:763500 source:(1, 66) loss:6.72195
step:764000 source:(1, 66) loss:5.21122
step:764500 source:(1, 56) loss:6.65849
step:765000 source:(1, 43) loss:4.749
step:765500 source:(1, 66) loss:5.27351
step:766000 source:(1, 41) loss:4.59628
step:766500 source:(1, 66) loss:7.40128
step:767000 source:(1, 66) loss:5.12183
step:767500 source:(1, 66) loss:5.81803
step:768000 source:(1, 66) loss:6.54412
step:768500 source:(1, 66) loss:6.00787
step:769000 source:(1, 66) loss:6.65648
step:769500 source:(1, 66) loss:5.6612
step:770000 source:(1, 66) loss:6.94263
step:770500 source:(1, 42) loss:6.71823
step:771000 source:(1, 66) loss:7.72472
step:771500 source:(1, 66) loss:5.49454
step:772000 source:(1, 66) loss:6.29891
step:772500 source:(1, 24) loss:6.45957
step:773000 source:(1, 66) loss:7.1726
step:773500 source:(1, 42) loss:6.55114
step:774000 source:(1, 66) loss:6.82471
step:774500 source:(1, 66) loss:5.81086
step:775000 source:(1, 27) loss:6.42395
step:775500 source:(1, 66) loss:6.12034
step:776000 source:(1, 32) loss:4.97811
step:776500 source:(1, 49) loss:6.92915
step:777000 source:(1, 66) loss:5.18746
step:777500 source:(1, 56) loss:5.50253
step:778000 source:(1, 66) loss:5.56716
step:778500 source:(1, 66) loss:6.61497
step:779000 source:(1, 66) loss:7.75724
step:779500 source:(1, 66) loss:5.96669
step:780000 source:(1, 66) loss:5.04439
step:780500 source:(1, 46) loss:6.59783
step:781000 source:(1, 66) loss:6.54963
step:781500 source:(1, 66) loss:4.51226
step:782000 source:(1, 50) loss:7.12243
step:782500 source:(1, 44) loss:5.58939
step:783000 source:(1, 30) loss:7.35554
step:783500 source:(1, 66) loss:6.17384
step:784000 source:(1, 66) loss:5.9506
step:784500 source:(1, 66) loss:6.09894
step:785000 source:(1, 66) loss:6.4376
step:785500 source:(1, 23) loss:4.35704
step:786000 source:(1, 66) loss:6.81753
step:786500 source:(1, 34) loss:5.50705
step:787000 source:(1, 66) loss:6.55499
step:787500 source:(1, 66) loss:5.43716
step:788000 source:(1, 66) loss:6.21401
step:788500 source:(1, 46) loss:6.09237
step:789000 source:(1, 66) loss:6.3761
step:789500 source:(1, 37) loss:5.91766
step:790000 source:(1, 66) loss:5.44053
step:790500 source:(1, 36) loss:5.44955
step:791000 source:(1, 33) loss:5.08185
step:791500 source:(1, 55) loss:4.77022
step:792000 source:(1, 59) loss:6.73546
step:792500 source:(1, 28) loss:6.17476
step:793000 source:(1, 33) loss:5.76306
step:793500 source:(1, 32) loss:5.00369
step:794000 source:(1, 66) loss:4.16932
step:794500 source:(1, 53) loss:5.17667
step:795000 source:(1, 66) loss:5.31299
step:795500 source:(1, 66) loss:6.05079
step:796000 source:(1, 66) loss:5.13285
step:796500 source:(1, 66) loss:6.37926
step:797000 source:(1, 66) loss:6.71256
step:797500 source:(1, 26) loss:6.45452
step:798000 source:(1, 66) loss:5.45296
step:798500 source:(1, 66) loss:5.7848
step:799000 source:(1, 66) loss:6.50485
step:799500 source:(1, 47) loss:5.4773
step:800000 source:(1, 46) loss:5.18001
step:800500 source:(1, 66) loss:5.55111
step:801000 source:(1, 66) loss:5.22457
step:801500 source:(1, 63) loss:6.40114
step:802000 source:(1, 31) loss:6.49265
step:802500 source:(1, 33) loss:5.19688
step:803000 source:(1, 53) loss:5.59747
step:803500 source:(1, 66) loss:6.00635
step:804000 source:(1, 31) loss:7.40929
step:804500 source:(1, 66) loss:5.40455
step:805000 source:(1, 31) loss:6.60221
step:805500 source:(1, 66) loss:7.82761
step:806000 source:(1, 66) loss:7.36115
step:806500 source:(1, 66) loss:7.39618
step:807000 source:(1, 66) loss:5.08307
step:807500 source:(1, 57) loss:5.16357
step:808000 source:(1, 24) loss:5.62824
step:808500 source:(1, 59) loss:6.12723
step:809000 source:(1, 39) loss:5.01216
step:809500 source:(1, 35) loss:8.28313
step:810000 source:(1, 66) loss:5.42258
step:810500 source:(1, 43) loss:6.24348
step:811000 source:(1, 54) loss:5.62386
step:811500 source:(1, 41) loss:5.32413
step:812000 source:(1, 66) loss:6.2679
step:812500 source:(1, 66) loss:6.15945
step:813000 source:(1, 66) loss:6.56641
step:813500 source:(1, 44) loss:8.15981
step:814000 source:(1, 66) loss:6.50721
step:814500 source:(1, 66) loss:4.63113
step:815000 source:(1, 66) loss:6.06524
step:815500 source:(1, 51) loss:5.90236
step:816000 source:(1, 57) loss:4.27797
step:816500 source:(1, 66) loss:5.62443
step:817000 source:(1, 34) loss:7.1051
step:817500 source:(1, 66) loss:6.59407
step:818000 source:(1, 66) loss:5.36618
step:818500 source:(1, 35) loss:6.42215
step:819000 source:(1, 66) loss:7.72125
step:819500 source:(1, 35) loss:6.02843
step:820000 source:(1, 66) loss:7.63581
step:820500 source:(1, 66) loss:5.28938
step:821000 source:(1, 36) loss:6.32302
step:821500 source:(1, 66) loss:5.83702
step:822000 source:(1, 48) loss:6.72542
step:822500 source:(1, 51) loss:6.74578
step:823000 source:(1, 66) loss:6.12218
step:823500 source:(1, 66) loss:5.1654
step:824000 source:(1, 66) loss:8.42204
step:824500 source:(1, 66) loss:5.6459
step:825000 source:(1, 55) loss:7.39333
step:825500 source:(1, 33) loss:5.61989
step:826000 source:(1, 66) loss:6.46036
step:826500 source:(1, 35) loss:7.4204
step:827000 source:(1, 25) loss:5.02829
step:827500 source:(1, 30) loss:6.64785
step:828000 source:(1, 66) loss:5.8501
step:828500 source:(1, 41) loss:5.33581
step:829000 source:(1, 59) loss:5.81589
step:829500 source:(1, 64) loss:6.00274
step:830000 source:(1, 43) loss:6.13618
step:830500 source:(1, 66) loss:5.20413
step:831000 source:(1, 66) loss:7.32483
step:831500 source:(1, 66) loss:5.28346
step:832000 source:(1, 39) loss:6.58232
step:832500 source:(1, 66) loss:5.4205
step:833000 source:(1, 52) loss:6.3438
step:833500 source:(1, 66) loss:6.72587
step:834000 source:(1, 66) loss:6.42102
step:834500 source:(1, 66) loss:6.52464
step:835000 source:(1, 66) loss:6.4054
step:835500 source:(1, 62) loss:5.03381
step:836000 source:(1, 66) loss:5.14694
step:836500 source:(1, 66) loss:6.14744
step:837000 source:(1, 38) loss:5.70458
step:837500 source:(1, 66) loss:5.94308
step:838000 source:(1, 30) loss:6.67764
step:838500 source:(1, 66) loss:5.28735
step:839000 source:(1, 33) loss:5.19858
step:839500 source:(1, 66) loss:5.65816
step:840000 source:(1, 30) loss:5.9721
step:840500 source:(1, 51) loss:5.65275
step:841000 source:(1, 66) loss:5.64492
step:841500 source:(1, 25) loss:5.4622
step:842000 source:(1, 39) loss:6.21435
step:842500 source:(1, 59) loss:6.98333
step:843000 source:(1, 22) loss:6.97725
step:843500 source:(1, 53) loss:6.07452
step:844000 source:(1, 66) loss:5.71686
step:844500 source:(1, 48) loss:4.81311
step:845000 source:(1, 23) loss:6.19198
step:845500 source:(1, 52) loss:4.41119
step:846000 source:(1, 40) loss:5.45993
step:846500 source:(1, 66) loss:7.15346
step:847000 source:(1, 66) loss:6.42813
step:847500 source:(1, 66) loss:4.32262
step:848000 source:(1, 49) loss:5.83239
step:848500 source:(1, 27) loss:6.22202
step:849000 source:(1, 24) loss:6.32651
step:849500 source:(1, 66) loss:6.49336
step:850000 source:(1, 58) loss:6.05565
epoch:5 eval_bleu:52.57151126861572
2018-07-22 00:59:09.198498: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 00:59:09.198566: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 01:16:08.801379: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 01:16:08.801452: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 01:16:08.801484: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:850500 source:(1, 38) loss:6.3004
step:851000 source:(1, 29) loss:5.03349
step:851500 source:(1, 32) loss:5.64472
step:852000 source:(1, 66) loss:4.62112
step:852500 source:(1, 66) loss:6.32854
step:853000 source:(1, 65) loss:5.75465
step:853500 source:(1, 66) loss:5.85419
step:854000 source:(1, 66) loss:6.85445
step:854500 source:(1, 66) loss:8.11308
step:855000 source:(1, 28) loss:5.32164
step:855500 source:(1, 61) loss:5.92901
step:856000 source:(1, 66) loss:5.62961
step:856500 source:(1, 66) loss:5.74851
step:857000 source:(1, 61) loss:6.49189
step:857500 source:(1, 22) loss:6.31415
step:858000 source:(1, 47) loss:6.11664
step:858500 source:(1, 66) loss:5.3189
step:859000 source:(1, 66) loss:6.90609
step:859500 source:(1, 41) loss:5.88849
step:860000 source:(1, 66) loss:5.46133
step:860500 source:(1, 41) loss:5.71189
step:861000 source:(1, 66) loss:5.3842
step:861500 source:(1, 66) loss:5.08183
step:862000 source:(1, 66) loss:5.27478
step:862500 source:(1, 66) loss:5.00689
step:863000 source:(1, 38) loss:6.25234
step:863500 source:(1, 66) loss:5.69638
step:864000 source:(1, 66) loss:4.76795
step:864500 source:(1, 56) loss:5.24197
step:865000 source:(1, 43) loss:6.1014
step:865500 source:(1, 66) loss:4.74667
step:866000 source:(1, 41) loss:4.25274
step:866500 source:(1, 66) loss:5.76164
step:867000 source:(1, 66) loss:6.56993
step:867500 source:(1, 66) loss:6.15162
step:868000 source:(1, 66) loss:6.0075
step:868500 source:(1, 66) loss:6.10926
step:869000 source:(1, 66) loss:7.73181
step:869500 source:(1, 66) loss:6.8376
step:870000 source:(1, 66) loss:6.58985
step:870500 source:(1, 42) loss:7.16078
step:871000 source:(1, 66) loss:7.15335
step:871500 source:(1, 66) loss:5.02468
step:872000 source:(1, 66) loss:6.85065
step:872500 source:(1, 24) loss:6.31896
step:873000 source:(1, 66) loss:6.21237
step:873500 source:(1, 42) loss:6.6937
step:874000 source:(1, 66) loss:6.06852
step:874500 source:(1, 66) loss:5.23086
step:875000 source:(1, 27) loss:6.55934
step:875500 source:(1, 66) loss:6.20947
step:876000 source:(1, 32) loss:5.85779
step:876500 source:(1, 49) loss:6.87057
step:877000 source:(1, 66) loss:5.8042
step:877500 source:(1, 56) loss:6.00343
step:878000 source:(1, 66) loss:6.34702
step:878500 source:(1, 66) loss:6.95112
step:879000 source:(1, 66) loss:5.56841
step:879500 source:(1, 66) loss:5.21204
step:880000 source:(1, 66) loss:6.08673
step:880500 source:(1, 46) loss:6.5365
step:881000 source:(1, 66) loss:5.75337
step:881500 source:(1, 66) loss:5.56149
step:882000 source:(1, 50) loss:5.63147
step:882500 source:(1, 44) loss:6.04268
step:883000 source:(1, 30) loss:7.80106
step:883500 source:(1, 66) loss:6.33841
step:884000 source:(1, 66) loss:6.37396
step:884500 source:(1, 66) loss:5.51142
step:885000 source:(1, 66) loss:6.02102
step:885500 source:(1, 23) loss:4.60851
step:886000 source:(1, 66) loss:5.97939
step:886500 source:(1, 34) loss:6.18747
step:887000 source:(1, 66) loss:5.4723
step:887500 source:(1, 66) loss:5.25296
step:888000 source:(1, 66) loss:6.34823
step:888500 source:(1, 46) loss:6.00627
step:889000 source:(1, 66) loss:5.35409
step:889500 source:(1, 37) loss:6.2453
step:890000 source:(1, 66) loss:5.37601
step:890500 source:(1, 36) loss:4.03184
step:891000 source:(1, 33) loss:5.14823
step:891500 source:(1, 55) loss:4.04122
step:892000 source:(1, 59) loss:6.10588
step:892500 source:(1, 28) loss:6.16431
step:893000 source:(1, 33) loss:5.9146
step:893500 source:(1, 32) loss:7.29583
step:894000 source:(1, 66) loss:6.13806
step:894500 source:(1, 53) loss:5.65711
step:895000 source:(1, 66) loss:4.93651
step:895500 source:(1, 66) loss:6.32379
step:896000 source:(1, 66) loss:5.42787
step:896500 source:(1, 66) loss:7.31774
step:897000 source:(1, 66) loss:7.2799
step:897500 source:(1, 26) loss:5.0354
step:898000 source:(1, 66) loss:5.70378
step:898500 source:(1, 66) loss:6.15876
step:899000 source:(1, 66) loss:6.44635
step:899500 source:(1, 47) loss:6.99258
step:900000 source:(1, 46) loss:7.209
step:900500 source:(1, 66) loss:5.75031
step:901000 source:(1, 66) loss:5.94021
step:901500 source:(1, 63) loss:5.90546
step:902000 source:(1, 31) loss:6.85834
step:902500 source:(1, 33) loss:4.90899
step:903000 source:(1, 53) loss:5.74126
step:903500 source:(1, 66) loss:6.69781
step:904000 source:(1, 31) loss:7.33247
step:904500 source:(1, 66) loss:5.83285
step:905000 source:(1, 31) loss:6.02378
step:905500 source:(1, 66) loss:7.53378
step:906000 source:(1, 66) loss:6.3584
step:906500 source:(1, 66) loss:6.55538
step:907000 source:(1, 66) loss:5.87036
step:907500 source:(1, 57) loss:5.366
step:908000 source:(1, 24) loss:5.59356
step:908500 source:(1, 59) loss:6.466
step:909000 source:(1, 39) loss:6.4208
step:909500 source:(1, 35) loss:8.44981
step:910000 source:(1, 66) loss:5.13871
step:910500 source:(1, 43) loss:4.94103
step:911000 source:(1, 54) loss:6.18273
step:911500 source:(1, 41) loss:5.43668
step:912000 source:(1, 66) loss:5.51391
step:912500 source:(1, 66) loss:6.75669
step:913000 source:(1, 66) loss:5.88021
step:913500 source:(1, 44) loss:6.50228
step:914000 source:(1, 66) loss:6.47707
step:914500 source:(1, 66) loss:6.04823
step:915000 source:(1, 66) loss:6.06484
step:915500 source:(1, 51) loss:5.49098
step:916000 source:(1, 57) loss:5.34676
step:916500 source:(1, 66) loss:7.69431
step:917000 source:(1, 34) loss:6.80251
step:917500 source:(1, 66) loss:5.90267
step:918000 source:(1, 66) loss:6.87359
step:918500 source:(1, 35) loss:6.0136
step:919000 source:(1, 66) loss:6.76329
step:919500 source:(1, 35) loss:5.73901
step:920000 source:(1, 66) loss:6.61298
step:920500 source:(1, 66) loss:6.286
step:921000 source:(1, 36) loss:6.04893
step:921500 source:(1, 66) loss:5.861
step:922000 source:(1, 48) loss:6.44732
step:922500 source:(1, 51) loss:3.86002
step:923000 source:(1, 66) loss:6.64595
step:923500 source:(1, 66) loss:7.06248
step:924000 source:(1, 66) loss:8.23836
step:924500 source:(1, 66) loss:6.75895
step:925000 source:(1, 55) loss:6.34039
step:925500 source:(1, 33) loss:5.86303
step:926000 source:(1, 66) loss:5.10279
step:926500 source:(1, 35) loss:6.08388
step:927000 source:(1, 25) loss:5.92829
step:927500 source:(1, 30) loss:6.10614
step:928000 source:(1, 66) loss:5.28844
step:928500 source:(1, 41) loss:6.96493
step:929000 source:(1, 59) loss:4.89153
step:929500 source:(1, 64) loss:5.73962
step:930000 source:(1, 43) loss:5.8029
step:930500 source:(1, 66) loss:5.71432
step:931000 source:(1, 66) loss:6.67757
step:931500 source:(1, 66) loss:5.21221
step:932000 source:(1, 39) loss:7.66641
step:932500 source:(1, 66) loss:6.01382
step:933000 source:(1, 52) loss:6.06022
step:933500 source:(1, 66) loss:5.93837
step:934000 source:(1, 66) loss:7.25927
step:934500 source:(1, 66) loss:6.32904
step:935000 source:(1, 66) loss:5.44518
step:935500 source:(1, 62) loss:4.61628
step:936000 source:(1, 66) loss:5.67362
step:936500 source:(1, 66) loss:5.75483
step:937000 source:(1, 38) loss:6.1086
step:937500 source:(1, 66) loss:5.833
step:938000 source:(1, 30) loss:6.48063
step:938500 source:(1, 66) loss:6.15543
step:939000 source:(1, 33) loss:5.89948
step:939500 source:(1, 66) loss:6.10658
step:940000 source:(1, 30) loss:6.87177
step:940500 source:(1, 51) loss:5.20891
step:941000 source:(1, 66) loss:5.38981
step:941500 source:(1, 25) loss:5.30382
step:942000 source:(1, 39) loss:5.03232
step:942500 source:(1, 59) loss:8.99357
step:943000 source:(1, 22) loss:6.78029
step:943500 source:(1, 53) loss:6.67161
step:944000 source:(1, 66) loss:6.17619
step:944500 source:(1, 48) loss:5.0173
step:945000 source:(1, 23) loss:6.85103
step:945500 source:(1, 52) loss:5.23564
step:946000 source:(1, 40) loss:5.64951
step:946500 source:(1, 66) loss:7.37141
step:947000 source:(1, 66) loss:5.23244
step:947500 source:(1, 66) loss:6.2921
step:948000 source:(1, 49) loss:5.47329
step:948500 source:(1, 27) loss:6.18572
step:949000 source:(1, 24) loss:5.95722
step:949500 source:(1, 66) loss:7.57633
step:950000 source:(1, 58) loss:5.14036
epoch:6 eval_bleu:51.97728872299194
2018-07-22 04:36:57.060890: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 04:36:57.060992: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 04:53:55.099813: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 04:53:55.099891: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 04:53:55.099931: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:950500 source:(1, 38) loss:5.94611
step:951000 source:(1, 29) loss:4.25199
step:951500 source:(1, 32) loss:5.12322
step:952000 source:(1, 66) loss:5.73892
step:952500 source:(1, 66) loss:5.63802
step:953000 source:(1, 65) loss:5.46416
step:953500 source:(1, 66) loss:6.19456
step:954000 source:(1, 66) loss:7.25159
step:954500 source:(1, 66) loss:7.5047
step:955000 source:(1, 28) loss:5.33589
step:955500 source:(1, 61) loss:6.62747
step:956000 source:(1, 66) loss:6.52566
step:956500 source:(1, 66) loss:6.66119
step:957000 source:(1, 61) loss:6.49264
step:957500 source:(1, 22) loss:6.18932
step:958000 source:(1, 47) loss:5.72144
step:958500 source:(1, 66) loss:5.90621
step:959000 source:(1, 66) loss:7.59005
step:959500 source:(1, 41) loss:6.86118
step:960000 source:(1, 66) loss:6.45371
step:960500 source:(1, 41) loss:6.83799
step:961000 source:(1, 66) loss:5.15755
step:961500 source:(1, 66) loss:5.46631
step:962000 source:(1, 66) loss:4.62555
step:962500 source:(1, 66) loss:5.6278
step:963000 source:(1, 38) loss:5.76937
step:963500 source:(1, 66) loss:6.92742
step:964000 source:(1, 66) loss:6.2467
step:964500 source:(1, 56) loss:7.06622
step:965000 source:(1, 43) loss:5.65978
step:965500 source:(1, 66) loss:5.0911
step:966000 source:(1, 41) loss:3.41185
step:966500 source:(1, 66) loss:7.00115
step:967000 source:(1, 66) loss:5.56842
step:967500 source:(1, 66) loss:5.51872
step:968000 source:(1, 66) loss:5.93699
step:968500 source:(1, 66) loss:6.22678
step:969000 source:(1, 66) loss:6.28519
step:969500 source:(1, 66) loss:6.16649
step:970000 source:(1, 66) loss:5.90109
step:970500 source:(1, 42) loss:6.99907
step:971000 source:(1, 66) loss:7.94799
step:971500 source:(1, 66) loss:4.93503
step:972000 source:(1, 66) loss:6.67704
step:972500 source:(1, 24) loss:5.87473
step:973000 source:(1, 66) loss:5.84654
step:973500 source:(1, 42) loss:7.45799
step:974000 source:(1, 66) loss:5.01323
step:974500 source:(1, 66) loss:6.58295
step:975000 source:(1, 27) loss:6.1311
step:975500 source:(1, 66) loss:5.58027
step:976000 source:(1, 32) loss:6.29154
step:976500 source:(1, 49) loss:5.81654
step:977000 source:(1, 66) loss:5.42555
step:977500 source:(1, 56) loss:5.60663
step:978000 source:(1, 66) loss:4.65275
step:978500 source:(1, 66) loss:5.13899
step:979000 source:(1, 66) loss:6.43724
step:979500 source:(1, 66) loss:6.52106
step:980000 source:(1, 66) loss:5.38464
step:980500 source:(1, 46) loss:6.79465
step:981000 source:(1, 66) loss:5.89233
step:981500 source:(1, 66) loss:7.10339
step:982000 source:(1, 50) loss:6.94417
step:982500 source:(1, 44) loss:5.35186
step:983000 source:(1, 30) loss:7.63972
step:983500 source:(1, 66) loss:6.46338
step:984000 source:(1, 66) loss:6.36589
step:984500 source:(1, 66) loss:7.00745
step:985000 source:(1, 66) loss:6.92957
step:985500 source:(1, 23) loss:4.47899
step:986000 source:(1, 66) loss:5.16203
step:986500 source:(1, 34) loss:5.3735
step:987000 source:(1, 66) loss:5.9687
step:987500 source:(1, 66) loss:5.48159
step:988000 source:(1, 66) loss:5.55217
step:988500 source:(1, 46) loss:5.80511
step:989000 source:(1, 66) loss:5.23868
step:989500 source:(1, 37) loss:5.88649
step:990000 source:(1, 66) loss:5.22925
step:990500 source:(1, 36) loss:4.64378
step:991000 source:(1, 33) loss:5.67932
step:991500 source:(1, 55) loss:4.37608
step:992000 source:(1, 59) loss:6.92233
step:992500 source:(1, 28) loss:6.1356
step:993000 source:(1, 33) loss:5.81249
step:993500 source:(1, 32) loss:8.00841
step:994000 source:(1, 66) loss:5.76806
step:994500 source:(1, 53) loss:5.83818
step:995000 source:(1, 66) loss:5.11857
step:995500 source:(1, 66) loss:5.44844
step:996000 source:(1, 66) loss:5.50399
step:996500 source:(1, 66) loss:5.29822
step:997000 source:(1, 66) loss:5.02333
step:997500 source:(1, 26) loss:6.18461
step:998000 source:(1, 66) loss:5.61363
step:998500 source:(1, 66) loss:7.11046
step:999000 source:(1, 66) loss:6.04354
step:999500 source:(1, 47) loss:5.92882
step:1000000 source:(1, 46) loss:5.75734
step:1000500 source:(1, 66) loss:6.31023
step:1001000 source:(1, 66) loss:4.90524
step:1001500 source:(1, 63) loss:6.59129
step:1002000 source:(1, 31) loss:6.836
step:1002500 source:(1, 33) loss:5.2674
step:1003000 source:(1, 53) loss:6.0186
step:1003500 source:(1, 66) loss:6.37239
step:1004000 source:(1, 31) loss:8.04885
step:1004500 source:(1, 66) loss:5.45916
step:1005000 source:(1, 31) loss:5.93684
step:1005500 source:(1, 66) loss:7.14385
step:1006000 source:(1, 66) loss:6.27011
step:1006500 source:(1, 66) loss:6.08148
step:1007000 source:(1, 66) loss:4.95627
step:1007500 source:(1, 57) loss:5.56755
step:1008000 source:(1, 24) loss:5.53836
step:1008500 source:(1, 59) loss:6.64532
step:1009000 source:(1, 39) loss:4.99988
step:1009500 source:(1, 35) loss:8.10193
step:1010000 source:(1, 66) loss:4.7184
step:1010500 source:(1, 43) loss:6.06589
step:1011000 source:(1, 54) loss:6.58738
step:1011500 source:(1, 41) loss:5.36281
step:1012000 source:(1, 66) loss:5.91746
step:1012500 source:(1, 66) loss:5.80075
step:1013000 source:(1, 66) loss:8.20109
step:1013500 source:(1, 44) loss:7.72282
step:1014000 source:(1, 66) loss:7.64296
step:1014500 source:(1, 66) loss:4.79614
step:1015000 source:(1, 66) loss:5.98876
step:1015500 source:(1, 51) loss:6.3124
step:1016000 source:(1, 57) loss:5.46577
step:1016500 source:(1, 66) loss:7.44474
step:1017000 source:(1, 34) loss:6.09546
step:1017500 source:(1, 66) loss:6.57792
step:1018000 source:(1, 66) loss:4.50786
step:1018500 source:(1, 35) loss:5.80549
step:1019000 source:(1, 66) loss:7.22238
step:1019500 source:(1, 35) loss:5.73262
step:1020000 source:(1, 66) loss:7.2164
step:1020500 source:(1, 66) loss:6.4743
step:1021000 source:(1, 36) loss:5.39672
step:1021500 source:(1, 66) loss:5.61832
step:1022000 source:(1, 48) loss:5.45768
step:1022500 source:(1, 51) loss:3.14611
step:1023000 source:(1, 66) loss:6.71321
step:1023500 source:(1, 66) loss:5.39958
step:1024000 source:(1, 66) loss:7.32297
step:1024500 source:(1, 66) loss:5.08379
step:1025000 source:(1, 55) loss:5.50211
step:1025500 source:(1, 33) loss:6.03527
step:1026000 source:(1, 66) loss:5.27606
step:1026500 source:(1, 35) loss:6.75849
step:1027000 source:(1, 25) loss:5.5675
step:1027500 source:(1, 30) loss:6.0549
step:1028000 source:(1, 66) loss:5.31471
step:1028500 source:(1, 41) loss:5.83733
step:1029000 source:(1, 59) loss:5.71109
step:1029500 source:(1, 64) loss:5.87497
step:1030000 source:(1, 43) loss:6.29653
step:1030500 source:(1, 66) loss:5.457
step:1031000 source:(1, 66) loss:6.47621
step:1031500 source:(1, 66) loss:6.01527
step:1032000 source:(1, 39) loss:7.69664
step:1032500 source:(1, 66) loss:5.96596
step:1033000 source:(1, 52) loss:6.1181
step:1033500 source:(1, 66) loss:6.20018
step:1034000 source:(1, 66) loss:5.69128
step:1034500 source:(1, 66) loss:6.50887
step:1035000 source:(1, 66) loss:5.48741
step:1035500 source:(1, 62) loss:5.20999
step:1036000 source:(1, 66) loss:5.26901
step:1036500 source:(1, 66) loss:5.61462
step:1037000 source:(1, 38) loss:6.02119
step:1037500 source:(1, 66) loss:5.86612
step:1038000 source:(1, 30) loss:6.6816
step:1038500 source:(1, 66) loss:7.36317
step:1039000 source:(1, 33) loss:5.48923
step:1039500 source:(1, 66) loss:6.05118
step:1040000 source:(1, 30) loss:5.84189
step:1040500 source:(1, 51) loss:6.66018
step:1041000 source:(1, 66) loss:4.80396
step:1041500 source:(1, 25) loss:6.22145
step:1042000 source:(1, 39) loss:6.68848
step:1042500 source:(1, 59) loss:5.7837
step:1043000 source:(1, 22) loss:6.87654
step:1043500 source:(1, 53) loss:7.45437
step:1044000 source:(1, 66) loss:5.32286
step:1044500 source:(1, 48) loss:5.43028
step:1045000 source:(1, 23) loss:6.46702
step:1045500 source:(1, 52) loss:5.43016
step:1046000 source:(1, 40) loss:5.60458
step:1046500 source:(1, 66) loss:5.92823
step:1047000 source:(1, 66) loss:6.61962
step:1047500 source:(1, 66) loss:7.06035
step:1048000 source:(1, 49) loss:6.46163
step:1048500 source:(1, 27) loss:6.93073
step:1049000 source:(1, 24) loss:6.5837
step:1049500 source:(1, 66) loss:6.91914
step:1050000 source:(1, 58) loss:5.89345
epoch:7 eval_bleu:51.43967866897583
2018-07-22 08:14:15.846960: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 08:14:15.847639: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 08:14:15.847680: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 08:31:13.956366: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 08:31:13.956442: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 08:31:13.956473: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:1050500 source:(1, 38) loss:6.3219
step:1051000 source:(1, 29) loss:4.2142
step:1051500 source:(1, 32) loss:6.26444
step:1052000 source:(1, 66) loss:5.16183
step:1052500 source:(1, 66) loss:6.74775
step:1053000 source:(1, 65) loss:5.92379
step:1053500 source:(1, 66) loss:6.30383
step:1054000 source:(1, 66) loss:7.27551
step:1054500 source:(1, 66) loss:6.88876
step:1055000 source:(1, 28) loss:4.68388
step:1055500 source:(1, 61) loss:6.77743
step:1056000 source:(1, 66) loss:6.77403
step:1056500 source:(1, 66) loss:6.08086
step:1057000 source:(1, 61) loss:5.76506
step:1057500 source:(1, 22) loss:6.56457
step:1058000 source:(1, 47) loss:4.78955
step:1058500 source:(1, 66) loss:6.16615
step:1059000 source:(1, 66) loss:7.0593
step:1059500 source:(1, 41) loss:6.1936
step:1060000 source:(1, 66) loss:5.51108
step:1060500 source:(1, 41) loss:5.87399
step:1061000 source:(1, 66) loss:5.28915
step:1061500 source:(1, 66) loss:4.90205
step:1062000 source:(1, 66) loss:5.64374
step:1062500 source:(1, 66) loss:6.22203
step:1063000 source:(1, 38) loss:5.10856
step:1063500 source:(1, 66) loss:6.19845
step:1064000 source:(1, 66) loss:5.95651
step:1064500 source:(1, 56) loss:6.20488
step:1065000 source:(1, 43) loss:5.59406
step:1065500 source:(1, 66) loss:5.21575
step:1066000 source:(1, 41) loss:4.71629
step:1066500 source:(1, 66) loss:7.59918
step:1067000 source:(1, 66) loss:6.65596
step:1067500 source:(1, 66) loss:4.71893
step:1068000 source:(1, 66) loss:6.12181
step:1068500 source:(1, 66) loss:6.52158
step:1069000 source:(1, 66) loss:7.02143
step:1069500 source:(1, 66) loss:5.77404
step:1070000 source:(1, 66) loss:6.38998
step:1070500 source:(1, 42) loss:6.85551
step:1071000 source:(1, 66) loss:6.39688
step:1071500 source:(1, 66) loss:5.60261
step:1072000 source:(1, 66) loss:6.57776
step:1072500 source:(1, 24) loss:6.33633
step:1073000 source:(1, 66) loss:5.73214
step:1073500 source:(1, 42) loss:6.34834
step:1074000 source:(1, 66) loss:7.68968
step:1074500 source:(1, 66) loss:5.94872
step:1075000 source:(1, 27) loss:6.63367
step:1075500 source:(1, 66) loss:6.54897
step:1076000 source:(1, 32) loss:5.6306
step:1076500 source:(1, 49) loss:6.47231
step:1077000 source:(1, 66) loss:4.26088
step:1077500 source:(1, 56) loss:6.50178
step:1078000 source:(1, 66) loss:5.34681
step:1078500 source:(1, 66) loss:5.7166
step:1079000 source:(1, 66) loss:6.97941
step:1079500 source:(1, 66) loss:5.40976
step:1080000 source:(1, 66) loss:5.52594
step:1080500 source:(1, 46) loss:7.20762
step:1081000 source:(1, 66) loss:5.86996
step:1081500 source:(1, 66) loss:6.48969
step:1082000 source:(1, 50) loss:5.73919
step:1082500 source:(1, 44) loss:5.93451
step:1083000 source:(1, 30) loss:7.30996
step:1083500 source:(1, 66) loss:6.20618
step:1084000 source:(1, 66) loss:6.30771
step:1084500 source:(1, 66) loss:5.49891
step:1085000 source:(1, 66) loss:7.18087
step:1085500 source:(1, 23) loss:4.69838
step:1086000 source:(1, 66) loss:4.91726
step:1086500 source:(1, 34) loss:6.01241
step:1087000 source:(1, 66) loss:5.91222
step:1087500 source:(1, 66) loss:5.7936
step:1088000 source:(1, 66) loss:7.24663
step:1088500 source:(1, 46) loss:7.03974
step:1089000 source:(1, 66) loss:4.95901
step:1089500 source:(1, 37) loss:6.50479
step:1090000 source:(1, 66) loss:5.44036
step:1090500 source:(1, 36) loss:5.44565
step:1091000 source:(1, 33) loss:6.12134
step:1091500 source:(1, 55) loss:3.87308
step:1092000 source:(1, 59) loss:5.74699
step:1092500 source:(1, 28) loss:5.68002
step:1093000 source:(1, 33) loss:6.12333
step:1093500 source:(1, 32) loss:4.72145
step:1094000 source:(1, 66) loss:5.84089
step:1094500 source:(1, 53) loss:5.41369
step:1095000 source:(1, 66) loss:5.54774
step:1095500 source:(1, 66) loss:6.70002
step:1096000 source:(1, 66) loss:4.96264
step:1096500 source:(1, 66) loss:6.78417
step:1097000 source:(1, 66) loss:5.93989
step:1097500 source:(1, 26) loss:6.74991
step:1098000 source:(1, 66) loss:5.99702
step:1098500 source:(1, 66) loss:6.98506
step:1099000 source:(1, 66) loss:5.64231
step:1099500 source:(1, 47) loss:5.9353
step:1100000 source:(1, 46) loss:5.8189
step:1100500 source:(1, 66) loss:6.38115
step:1101000 source:(1, 66) loss:5.16248
step:1101500 source:(1, 63) loss:7.53339
step:1102000 source:(1, 31) loss:7.26211
step:1102500 source:(1, 33) loss:5.89704
step:1103000 source:(1, 53) loss:5.64658
step:1103500 source:(1, 66) loss:7.35122
step:1104000 source:(1, 31) loss:6.65856
step:1104500 source:(1, 66) loss:5.59702
step:1105000 source:(1, 31) loss:6.37921
step:1105500 source:(1, 66) loss:6.40668
step:1106000 source:(1, 66) loss:5.76906
step:1106500 source:(1, 66) loss:7.54005
step:1107000 source:(1, 66) loss:4.7
step:1107500 source:(1, 57) loss:5.64967
step:1108000 source:(1, 24) loss:5.39081
step:1108500 source:(1, 59) loss:6.30362
step:1109000 source:(1, 39) loss:5.03461
step:1109500 source:(1, 35) loss:7.55991
step:1110000 source:(1, 66) loss:4.6027
step:1110500 source:(1, 43) loss:5.37262
step:1111000 source:(1, 54) loss:5.79041
step:1111500 source:(1, 41) loss:5.47416
step:1112000 source:(1, 66) loss:5.19096
step:1112500 source:(1, 66) loss:5.79706
step:1113000 source:(1, 66) loss:7.2438
step:1113500 source:(1, 44) loss:6.41778
step:1114000 source:(1, 66) loss:7.18614
step:1114500 source:(1, 66) loss:5.37029
step:1115000 source:(1, 66) loss:5.03739
step:1115500 source:(1, 51) loss:5.41042
step:1116000 source:(1, 57) loss:4.66887
step:1116500 source:(1, 66) loss:5.79582
step:1117000 source:(1, 34) loss:5.9454
step:1117500 source:(1, 66) loss:5.87607
step:1118000 source:(1, 66) loss:4.29881
step:1118500 source:(1, 35) loss:6.50746
step:1119000 source:(1, 66) loss:7.48103
step:1119500 source:(1, 35) loss:6.1315
step:1120000 source:(1, 66) loss:5.88651
step:1120500 source:(1, 66) loss:5.28017
step:1121000 source:(1, 36) loss:5.30835
step:1121500 source:(1, 66) loss:5.98356
step:1122000 source:(1, 48) loss:5.7328
step:1122500 source:(1, 51) loss:5.70279
step:1123000 source:(1, 66) loss:6.93027
step:1123500 source:(1, 66) loss:6.86627
step:1124000 source:(1, 66) loss:5.63053
step:1124500 source:(1, 66) loss:6.66479
step:1125000 source:(1, 55) loss:5.00116
step:1125500 source:(1, 33) loss:5.58517
step:1126000 source:(1, 66) loss:4.91871
step:1126500 source:(1, 35) loss:6.79065
step:1127000 source:(1, 25) loss:5.4057
step:1127500 source:(1, 30) loss:5.50814
step:1128000 source:(1, 66) loss:5.34876
step:1128500 source:(1, 41) loss:5.39705
step:1129000 source:(1, 59) loss:6.46605
step:1129500 source:(1, 64) loss:6.21281
step:1130000 source:(1, 43) loss:6.21453
step:1130500 source:(1, 66) loss:6.25739
step:1131000 source:(1, 66) loss:5.29284
step:1131500 source:(1, 66) loss:6.18556
step:1132000 source:(1, 39) loss:7.69222
step:1132500 source:(1, 66) loss:6.15009
step:1133000 source:(1, 52) loss:5.99291
step:1133500 source:(1, 66) loss:7.77662
step:1134000 source:(1, 66) loss:5.61916
step:1134500 source:(1, 66) loss:7.67905
step:1135000 source:(1, 66) loss:6.30741
step:1135500 source:(1, 62) loss:4.88922
step:1136000 source:(1, 66) loss:5.09737
step:1136500 source:(1, 66) loss:4.86406
step:1137000 source:(1, 38) loss:5.31339
step:1137500 source:(1, 66) loss:7.14769
step:1138000 source:(1, 30) loss:6.34858
step:1138500 source:(1, 66) loss:5.76386
step:1139000 source:(1, 33) loss:5.11724
step:1139500 source:(1, 66) loss:5.634
step:1140000 source:(1, 30) loss:5.99036
step:1140500 source:(1, 51) loss:6.1845
step:1141000 source:(1, 66) loss:6.28943
step:1141500 source:(1, 25) loss:5.27553
step:1142000 source:(1, 39) loss:4.96597
step:1142500 source:(1, 59) loss:8.11937
step:1143000 source:(1, 22) loss:7.05121
step:1143500 source:(1, 53) loss:7.55928
step:1144000 source:(1, 66) loss:5.56817
step:1144500 source:(1, 48) loss:4.71584
step:1145000 source:(1, 23) loss:6.61412
step:1145500 source:(1, 52) loss:4.58671
step:1146000 source:(1, 40) loss:5.29114
step:1146500 source:(1, 66) loss:6.06585
step:1147000 source:(1, 66) loss:6.74521
step:1147500 source:(1, 66) loss:7.54423
step:1148000 source:(1, 49) loss:5.97424
step:1148500 source:(1, 27) loss:6.20467
step:1149000 source:(1, 24) loss:5.71471
step:1149500 source:(1, 66) loss:7.44369
step:1150000 source:(1, 58) loss:6.70412
epoch:8 eval_bleu:53.928184509277344
2018-07-22 11:51:49.961753: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 11:51:49.963004: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 11:51:49.963057: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 12:08:50.212442: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 12:08:50.212514: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 12:08:50.212538: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:1150500 source:(1, 38) loss:5.44529
step:1151000 source:(1, 29) loss:3.3619
step:1151500 source:(1, 32) loss:5.34905
step:1152000 source:(1, 66) loss:5.43643
step:1152500 source:(1, 66) loss:6.9782
step:1153000 source:(1, 65) loss:6.13744
step:1153500 source:(1, 66) loss:5.69006
step:1154000 source:(1, 66) loss:6.25408
step:1154500 source:(1, 66) loss:6.01928
step:1155000 source:(1, 28) loss:5.13864
step:1155500 source:(1, 61) loss:5.99327
step:1156000 source:(1, 66) loss:6.1414
step:1156500 source:(1, 66) loss:4.98279
step:1157000 source:(1, 61) loss:6.19798
step:1157500 source:(1, 22) loss:6.31851
step:1158000 source:(1, 47) loss:5.34923
step:1158500 source:(1, 66) loss:6.05375
step:1159000 source:(1, 66) loss:7.32164
step:1159500 source:(1, 41) loss:5.96391
step:1160000 source:(1, 66) loss:5.97281
step:1160500 source:(1, 41) loss:5.31466
step:1161000 source:(1, 66) loss:5.3945
step:1161500 source:(1, 66) loss:4.88369
step:1162000 source:(1, 66) loss:5.44143
step:1162500 source:(1, 66) loss:6.08148
step:1163000 source:(1, 38) loss:5.97344
step:1163500 source:(1, 66) loss:6.47985
step:1164000 source:(1, 66) loss:6.00099
step:1164500 source:(1, 56) loss:5.94279
step:1165000 source:(1, 43) loss:6.89004
step:1165500 source:(1, 66) loss:4.84642
step:1166000 source:(1, 41) loss:5.00934
step:1166500 source:(1, 66) loss:6.73301
step:1167000 source:(1, 66) loss:6.93352
step:1167500 source:(1, 66) loss:5.61783
step:1168000 source:(1, 66) loss:6.0039
step:1168500 source:(1, 66) loss:7.2397
step:1169000 source:(1, 66) loss:6.12858
step:1169500 source:(1, 66) loss:7.13228
step:1170000 source:(1, 66) loss:6.46792
step:1170500 source:(1, 42) loss:7.28823
step:1171000 source:(1, 66) loss:6.25417
step:1171500 source:(1, 66) loss:4.91367
step:1172000 source:(1, 66) loss:7.14072
step:1172500 source:(1, 24) loss:7.04538
step:1173000 source:(1, 66) loss:8.00104
step:1173500 source:(1, 42) loss:6.80193
step:1174000 source:(1, 66) loss:6.59736
step:1174500 source:(1, 66) loss:5.70007
step:1175000 source:(1, 27) loss:5.3716
step:1175500 source:(1, 66) loss:6.26661
step:1176000 source:(1, 32) loss:5.13681
step:1176500 source:(1, 49) loss:6.90377
step:1177000 source:(1, 66) loss:4.60156
step:1177500 source:(1, 56) loss:5.38608
step:1178000 source:(1, 66) loss:4.83938
step:1178500 source:(1, 66) loss:5.85885
step:1179000 source:(1, 66) loss:7.87642
step:1179500 source:(1, 66) loss:6.65439
step:1180000 source:(1, 66) loss:6.24809
step:1180500 source:(1, 46) loss:7.29061
step:1181000 source:(1, 66) loss:7.08483
step:1181500 source:(1, 66) loss:7.58364
step:1182000 source:(1, 50) loss:7.12246
step:1182500 source:(1, 44) loss:5.35998
step:1183000 source:(1, 30) loss:8.46078
step:1183500 source:(1, 66) loss:6.39712
step:1184000 source:(1, 66) loss:6.55744
step:1184500 source:(1, 66) loss:5.68785
step:1185000 source:(1, 66) loss:6.65473
step:1185500 source:(1, 23) loss:4.74092
step:1186000 source:(1, 66) loss:7.28733
step:1186500 source:(1, 34) loss:5.12518
step:1187000 source:(1, 66) loss:5.4775
step:1187500 source:(1, 66) loss:5.58628
step:1188000 source:(1, 66) loss:6.60639
step:1188500 source:(1, 46) loss:6.06737
step:1189000 source:(1, 66) loss:5.73795
step:1189500 source:(1, 37) loss:6.12867
step:1190000 source:(1, 66) loss:5.04893
step:1190500 source:(1, 36) loss:5.75813
step:1191000 source:(1, 33) loss:5.71207
step:1191500 source:(1, 55) loss:4.56325
step:1192000 source:(1, 59) loss:5.64314
step:1192500 source:(1, 28) loss:6.35971
step:1193000 source:(1, 33) loss:5.37936
step:1193500 source:(1, 32) loss:6.1115
step:1194000 source:(1, 66) loss:5.16633
step:1194500 source:(1, 53) loss:5.89966
step:1195000 source:(1, 66) loss:4.20743
step:1195500 source:(1, 66) loss:6.01158
step:1196000 source:(1, 66) loss:5.40503
step:1196500 source:(1, 66) loss:5.54156
step:1197000 source:(1, 66) loss:7.11101
step:1197500 source:(1, 26) loss:6.04896
step:1198000 source:(1, 66) loss:5.43779
step:1198500 source:(1, 66) loss:8.40107
step:1199000 source:(1, 66) loss:6.28256
step:1199500 source:(1, 47) loss:4.64954
step:1200000 source:(1, 46) loss:4.99432
step:1200500 source:(1, 66) loss:7.07926
step:1201000 source:(1, 66) loss:7.24867
step:1201500 source:(1, 63) loss:7.31578
step:1202000 source:(1, 31) loss:7.03599
step:1202500 source:(1, 33) loss:5.81103
step:1203000 source:(1, 53) loss:4.84711
step:1203500 source:(1, 66) loss:5.66655
step:1204000 source:(1, 31) loss:6.66835
step:1204500 source:(1, 66) loss:5.8338
step:1205000 source:(1, 31) loss:6.32261
step:1205500 source:(1, 66) loss:6.64371
step:1206000 source:(1, 66) loss:7.24055
step:1206500 source:(1, 66) loss:6.39714
step:1207000 source:(1, 66) loss:7.90426
step:1207500 source:(1, 57) loss:5.47818
step:1208000 source:(1, 24) loss:4.84649
step:1208500 source:(1, 59) loss:6.63264
step:1209000 source:(1, 39) loss:7.15558
step:1209500 source:(1, 35) loss:7.69493
step:1210000 source:(1, 66) loss:5.82679
step:1210500 source:(1, 43) loss:5.60301
step:1211000 source:(1, 54) loss:6.56508
step:1211500 source:(1, 41) loss:4.92926
step:1212000 source:(1, 66) loss:5.17562
step:1212500 source:(1, 66) loss:6.39724
step:1213000 source:(1, 66) loss:7.41724
step:1213500 source:(1, 44) loss:7.90518
step:1214000 source:(1, 66) loss:5.43122
step:1214500 source:(1, 66) loss:5.63435
step:1215000 source:(1, 66) loss:5.18767
step:1215500 source:(1, 51) loss:6.28628
step:1216000 source:(1, 57) loss:6.13675
step:1216500 source:(1, 66) loss:5.45792
step:1217000 source:(1, 34) loss:6.31541
step:1217500 source:(1, 66) loss:6.00217
step:1218000 source:(1, 66) loss:4.76598
step:1218500 source:(1, 35) loss:6.06136
step:1219000 source:(1, 66) loss:7.37602
step:1219500 source:(1, 35) loss:5.70898
step:1220000 source:(1, 66) loss:7.78312
step:1220500 source:(1, 66) loss:4.74999
step:1221000 source:(1, 36) loss:5.79146
step:1221500 source:(1, 66) loss:5.01953
step:1222000 source:(1, 48) loss:6.99999
step:1222500 source:(1, 51) loss:4.45182
step:1223000 source:(1, 66) loss:5.45054
step:1223500 source:(1, 66) loss:5.28332
step:1224000 source:(1, 66) loss:7.26173
step:1224500 source:(1, 66) loss:5.9323
step:1225000 source:(1, 55) loss:5.16198
step:1225500 source:(1, 33) loss:5.7862
step:1226000 source:(1, 66) loss:5.8998
step:1226500 source:(1, 35) loss:7.19326
step:1227000 source:(1, 25) loss:5.48942
step:1227500 source:(1, 30) loss:6.75473
step:1228000 source:(1, 66) loss:4.87791
step:1228500 source:(1, 41) loss:5.47585
step:1229000 source:(1, 59) loss:5.41611
step:1229500 source:(1, 64) loss:4.72323
step:1230000 source:(1, 43) loss:5.87951
step:1230500 source:(1, 66) loss:5.33959
step:1231000 source:(1, 66) loss:5.68959
step:1231500 source:(1, 66) loss:5.33174
step:1232000 source:(1, 39) loss:6.4022
step:1232500 source:(1, 66) loss:6.24907
step:1233000 source:(1, 52) loss:5.54271
step:1233500 source:(1, 66) loss:7.12121
step:1234000 source:(1, 66) loss:4.68872
step:1234500 source:(1, 66) loss:7.16584
step:1235000 source:(1, 66) loss:5.30712
step:1235500 source:(1, 62) loss:4.85126
step:1236000 source:(1, 66) loss:7.39396
step:1236500 source:(1, 66) loss:5.94492
step:1237000 source:(1, 38) loss:6.78764
step:1237500 source:(1, 66) loss:5.77398
step:1238000 source:(1, 30) loss:7.13602
step:1238500 source:(1, 66) loss:5.55142
step:1239000 source:(1, 33) loss:4.88827
step:1239500 source:(1, 66) loss:5.19144
step:1240000 source:(1, 30) loss:6.13938
step:1240500 source:(1, 51) loss:5.59107
step:1241000 source:(1, 66) loss:4.49617
step:1241500 source:(1, 25) loss:5.15489
step:1242000 source:(1, 39) loss:6.68816
step:1242500 source:(1, 59) loss:6.25459
step:1243000 source:(1, 22) loss:6.17568
step:1243500 source:(1, 53) loss:6.98966
step:1244000 source:(1, 66) loss:6.67375
step:1244500 source:(1, 48) loss:4.83326
step:1245000 source:(1, 23) loss:6.23161
step:1245500 source:(1, 52) loss:6.2555
step:1246000 source:(1, 40) loss:5.80499
step:1246500 source:(1, 66) loss:6.55571
step:1247000 source:(1, 66) loss:5.21312
step:1247500 source:(1, 66) loss:6.12106
step:1248000 source:(1, 49) loss:6.15858
step:1248500 source:(1, 27) loss:6.05658
step:1249000 source:(1, 24) loss:5.76781
step:1249500 source:(1, 66) loss:4.92048
step:1250000 source:(1, 58) loss:5.96823
epoch:9 eval_bleu:50.44596195220947
2018-07-22 15:32:15.269556: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 15:32:15.269647: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 15:32:15.269936: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 15:32:15.269973: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 15:50:08.434689: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 15:50:08.434795: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 15:50:08.434844: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:1250500 source:(1, 38) loss:5.85817
step:1251000 source:(1, 29) loss:3.8872
step:1251500 source:(1, 32) loss:6.01369
step:1252000 source:(1, 66) loss:5.38208
step:1252500 source:(1, 66) loss:6.69809
step:1253000 source:(1, 65) loss:6.34339
step:1253500 source:(1, 66) loss:6.21318
step:1254000 source:(1, 66) loss:7.37231
step:1254500 source:(1, 66) loss:7.76976
step:1255000 source:(1, 28) loss:4.90592
step:1255500 source:(1, 61) loss:5.62057
step:1256000 source:(1, 66) loss:5.72104
step:1256500 source:(1, 66) loss:5.43941
step:1257000 source:(1, 61) loss:5.97075
step:1257500 source:(1, 22) loss:6.71748
step:1258000 source:(1, 47) loss:5.40821
step:1258500 source:(1, 66) loss:5.32634
step:1259000 source:(1, 66) loss:8.69171
step:1259500 source:(1, 41) loss:5.16498
step:1260000 source:(1, 66) loss:5.14852
step:1260500 source:(1, 41) loss:7.87691
step:1261000 source:(1, 66) loss:5.32541
step:1261500 source:(1, 66) loss:5.84368
step:1262000 source:(1, 66) loss:3.98938
step:1262500 source:(1, 66) loss:5.63413
step:1263000 source:(1, 38) loss:6.45357
step:1263500 source:(1, 66) loss:6.63812
step:1264000 source:(1, 66) loss:5.31786
step:1264500 source:(1, 56) loss:6.32166
step:1265000 source:(1, 43) loss:5.10031
step:1265500 source:(1, 66) loss:6.07876
step:1266000 source:(1, 41) loss:5.46797
step:1266500 source:(1, 66) loss:6.80426
step:1267000 source:(1, 66) loss:5.67454
step:1267500 source:(1, 66) loss:4.87065
step:1268000 source:(1, 66) loss:5.98949
step:1268500 source:(1, 66) loss:6.31475
step:1269000 source:(1, 66) loss:6.37232
step:1269500 source:(1, 66) loss:6.02718
step:1270000 source:(1, 66) loss:6.79668
step:1270500 source:(1, 42) loss:6.28548
step:1271000 source:(1, 66) loss:6.4484
step:1271500 source:(1, 66) loss:3.88019
step:1272000 source:(1, 66) loss:6.23889
step:1272500 source:(1, 24) loss:5.51359
step:1273000 source:(1, 66) loss:6.01049
step:1273500 source:(1, 42) loss:6.83348
step:1274000 source:(1, 66) loss:6.45125
step:1274500 source:(1, 66) loss:6.05245
step:1275000 source:(1, 27) loss:5.55782
step:1275500 source:(1, 66) loss:6.55426
step:1276000 source:(1, 32) loss:5.75809
step:1276500 source:(1, 49) loss:6.02865
step:1277000 source:(1, 66) loss:5.39163
step:1277500 source:(1, 56) loss:5.14052
step:1278000 source:(1, 66) loss:5.91134
step:1278500 source:(1, 66) loss:6.99607
step:1279000 source:(1, 66) loss:6.8946
step:1279500 source:(1, 66) loss:6.46912
step:1280000 source:(1, 66) loss:5.70243
step:1280500 source:(1, 46) loss:6.96912
step:1281000 source:(1, 66) loss:6.78933
step:1281500 source:(1, 66) loss:6.07696
step:1282000 source:(1, 50) loss:6.82506
step:1282500 source:(1, 44) loss:5.40215
step:1283000 source:(1, 30) loss:7.66186
step:1283500 source:(1, 66) loss:5.89573
step:1284000 source:(1, 66) loss:5.81298
step:1284500 source:(1, 66) loss:6.56331
step:1285000 source:(1, 66) loss:6.68952
step:1285500 source:(1, 23) loss:4.50859
step:1286000 source:(1, 66) loss:6.49232
step:1286500 source:(1, 34) loss:5.95754
step:1287000 source:(1, 66) loss:5.35029
step:1287500 source:(1, 66) loss:4.54651
step:1288000 source:(1, 66) loss:6.60001
step:1288500 source:(1, 46) loss:5.60072
step:1289000 source:(1, 66) loss:4.89221
step:1289500 source:(1, 37) loss:6.41639
step:1290000 source:(1, 66) loss:6.41949
step:1290500 source:(1, 36) loss:4.23705
step:1291000 source:(1, 33) loss:5.39603
step:1291500 source:(1, 55) loss:5.04711
step:1292000 source:(1, 59) loss:7.02937
step:1292500 source:(1, 28) loss:5.61884
step:1293000 source:(1, 33) loss:5.63172
step:1293500 source:(1, 32) loss:6.2113
step:1294000 source:(1, 66) loss:5.37372
step:1294500 source:(1, 53) loss:5.30619
step:1295000 source:(1, 66) loss:4.73362
step:1295500 source:(1, 66) loss:6.70031
step:1296000 source:(1, 66) loss:4.5492
step:1296500 source:(1, 66) loss:6.86565
step:1297000 source:(1, 66) loss:5.84197
step:1297500 source:(1, 26) loss:4.70203
step:1298000 source:(1, 66) loss:4.93371
step:1298500 source:(1, 66) loss:6.73155
step:1299000 source:(1, 66) loss:7.7229
step:1299500 source:(1, 47) loss:5.87825
step:1300000 source:(1, 46) loss:6.74869
step:1300500 source:(1, 66) loss:6.06869
step:1301000 source:(1, 66) loss:5.72285
step:1301500 source:(1, 63) loss:7.62259
step:1302000 source:(1, 31) loss:8.24766
step:1302500 source:(1, 33) loss:5.23616
step:1303000 source:(1, 53) loss:6.615
step:1303500 source:(1, 66) loss:5.73231
step:1304000 source:(1, 31) loss:7.39086
step:1304500 source:(1, 66) loss:5.86342
step:1305000 source:(1, 31) loss:5.75912
step:1305500 source:(1, 66) loss:7.09108
step:1306000 source:(1, 66) loss:6.21977
step:1306500 source:(1, 66) loss:6.52165
step:1307000 source:(1, 66) loss:5.44215
step:1307500 source:(1, 57) loss:5.36572
step:1308000 source:(1, 24) loss:5.43184
step:1308500 source:(1, 59) loss:6.62634
step:1309000 source:(1, 39) loss:5.36409
step:1309500 source:(1, 35) loss:7.15012
step:1310000 source:(1, 66) loss:5.37198
step:1310500 source:(1, 43) loss:5.77951
step:1311000 source:(1, 54) loss:6.6696
step:1311500 source:(1, 41) loss:5.38599
step:1312000 source:(1, 66) loss:5.37402
step:1312500 source:(1, 66) loss:6.92393
step:1313000 source:(1, 66) loss:7.25595
step:1313500 source:(1, 44) loss:7.73433
step:1314000 source:(1, 66) loss:5.90152
step:1314500 source:(1, 66) loss:4.59417
step:1315000 source:(1, 66) loss:6.01214
step:1315500 source:(1, 51) loss:6.48373
step:1316000 source:(1, 57) loss:6.1261
step:1316500 source:(1, 66) loss:5.79466
step:1317000 source:(1, 34) loss:6.66984
step:1317500 source:(1, 66) loss:6.31087
step:1318000 source:(1, 66) loss:4.61148
step:1318500 source:(1, 35) loss:7.22839
step:1319000 source:(1, 66) loss:7.74026
step:1319500 source:(1, 35) loss:5.89717
step:1320000 source:(1, 66) loss:7.05664
step:1320500 source:(1, 66) loss:7.75649
step:1321000 source:(1, 36) loss:6.07297
step:1321500 source:(1, 66) loss:5.76332
step:1322000 source:(1, 48) loss:6.37354
step:1322500 source:(1, 51) loss:4.62387
step:1323000 source:(1, 66) loss:5.92008
step:1323500 source:(1, 66) loss:7.745
step:1324000 source:(1, 66) loss:7.20922
step:1324500 source:(1, 66) loss:6.61589
step:1325000 source:(1, 55) loss:5.77568
step:1325500 source:(1, 33) loss:5.82244
step:1326000 source:(1, 66) loss:5.83874
step:1326500 source:(1, 35) loss:6.72301
step:1327000 source:(1, 25) loss:4.84057
step:1327500 source:(1, 30) loss:6.4605
step:1328000 source:(1, 66) loss:5.04692
step:1328500 source:(1, 41) loss:5.02881
step:1329000 source:(1, 59) loss:5.75971
step:1329500 source:(1, 64) loss:5.52482
step:1330000 source:(1, 43) loss:4.64931
step:1330500 source:(1, 66) loss:6.01409
step:1331000 source:(1, 66) loss:4.57384
step:1331500 source:(1, 66) loss:5.00212
step:1332000 source:(1, 39) loss:7.07904
step:1332500 source:(1, 66) loss:5.73304
step:1333000 source:(1, 52) loss:6.1091
step:1333500 source:(1, 66) loss:5.42089
step:1334000 source:(1, 66) loss:6.85454
step:1334500 source:(1, 66) loss:6.60899
step:1335000 source:(1, 66) loss:5.58774
step:1335500 source:(1, 62) loss:4.59544
step:1336000 source:(1, 66) loss:5.5966
step:1336500 source:(1, 66) loss:5.81583
step:1337000 source:(1, 38) loss:6.1552
step:1337500 source:(1, 66) loss:7.23219
step:1338000 source:(1, 30) loss:6.57795
step:1338500 source:(1, 66) loss:6.38175
step:1339000 source:(1, 33) loss:5.74805
step:1339500 source:(1, 66) loss:5.39034
step:1340000 source:(1, 30) loss:7.04731
step:1340500 source:(1, 51) loss:6.58961
step:1341000 source:(1, 66) loss:6.1867
step:1341500 source:(1, 25) loss:6.14625
step:1342000 source:(1, 39) loss:6.99314
step:1342500 source:(1, 59) loss:8.3698
step:1343000 source:(1, 22) loss:6.0787
step:1343500 source:(1, 53) loss:7.20983
step:1344000 source:(1, 66) loss:5.88317
step:1344500 source:(1, 48) loss:5.75444
step:1345000 source:(1, 23) loss:6.60661
step:1345500 source:(1, 52) loss:4.64596
step:1346000 source:(1, 40) loss:5.78854
step:1346500 source:(1, 66) loss:6.06751
step:1347000 source:(1, 66) loss:5.9978
step:1347500 source:(1, 66) loss:4.98829
step:1348000 source:(1, 49) loss:6.55269
step:1348500 source:(1, 27) loss:6.50008
step:1349000 source:(1, 24) loss:6.04266
step:1349500 source:(1, 66) loss:4.95942
step:1350000 source:(1, 58) loss:6.24106
epoch:10 eval_bleu:51.67311429977417
2018-07-22 18:53:45.729895: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 18:53:45.730649: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 18:53:45.730693: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 19:10:12.327821: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 19:10:12.327904: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 19:10:12.327934: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:1350500 source:(1, 38) loss:5.6148
step:1351000 source:(1, 29) loss:4.67459
step:1351500 source:(1, 32) loss:5.54063
step:1352000 source:(1, 66) loss:5.29615
step:1352500 source:(1, 66) loss:6.7108
step:1353000 source:(1, 65) loss:6.64737
step:1353500 source:(1, 66) loss:7.01651
step:1354000 source:(1, 66) loss:7.28371
step:1354500 source:(1, 66) loss:6.52753
step:1355000 source:(1, 28) loss:5.44813
step:1355500 source:(1, 61) loss:6.2955
step:1356000 source:(1, 66) loss:5.44556
step:1356500 source:(1, 66) loss:5.657
step:1357000 source:(1, 61) loss:6.45655
step:1357500 source:(1, 22) loss:6.83249
step:1358000 source:(1, 47) loss:5.10207
step:1358500 source:(1, 66) loss:4.99426
step:1359000 source:(1, 66) loss:7.2123
step:1359500 source:(1, 41) loss:6.6731
step:1360000 source:(1, 66) loss:5.19233
step:1360500 source:(1, 41) loss:5.17026
step:1361000 source:(1, 66) loss:5.41709
step:1361500 source:(1, 66) loss:5.53034
step:1362000 source:(1, 66) loss:4.06973
step:1362500 source:(1, 66) loss:5.65986
step:1363000 source:(1, 38) loss:6.25015
step:1363500 source:(1, 66) loss:6.13987
step:1364000 source:(1, 66) loss:6.68958
step:1364500 source:(1, 56) loss:5.68639
step:1365000 source:(1, 43) loss:5.86509
step:1365500 source:(1, 66) loss:4.76656
step:1366000 source:(1, 41) loss:4.08898
step:1366500 source:(1, 66) loss:6.86281
step:1367000 source:(1, 66) loss:8.40849
step:1367500 source:(1, 66) loss:4.96294
step:1368000 source:(1, 66) loss:5.79473
step:1368500 source:(1, 66) loss:6.26886
step:1369000 source:(1, 66) loss:6.27689
step:1369500 source:(1, 66) loss:6.40672
step:1370000 source:(1, 66) loss:5.8669
step:1370500 source:(1, 42) loss:6.56413
step:1371000 source:(1, 66) loss:7.12774
step:1371500 source:(1, 66) loss:6.03025
step:1372000 source:(1, 66) loss:5.76727
step:1372500 source:(1, 24) loss:6.86221
step:1373000 source:(1, 66) loss:5.1304
step:1373500 source:(1, 42) loss:7.49476
step:1374000 source:(1, 66) loss:7.08377
step:1374500 source:(1, 66) loss:5.45256
step:1375000 source:(1, 27) loss:5.60819
step:1375500 source:(1, 66) loss:7.99391
step:1376000 source:(1, 32) loss:6.40408
step:1376500 source:(1, 49) loss:6.03893
step:1377000 source:(1, 66) loss:4.42653
step:1377500 source:(1, 56) loss:5.00123
step:1378000 source:(1, 66) loss:5.63155
step:1378500 source:(1, 66) loss:6.5028
step:1379000 source:(1, 66) loss:6.41674
step:1379500 source:(1, 66) loss:6.44147
step:1380000 source:(1, 66) loss:7.06191
step:1380500 source:(1, 46) loss:6.94371
step:1381000 source:(1, 66) loss:6.7212
step:1381500 source:(1, 66) loss:6.07769
step:1382000 source:(1, 50) loss:5.48986
step:1382500 source:(1, 44) loss:6.02205
step:1383000 source:(1, 30) loss:7.5637
step:1383500 source:(1, 66) loss:5.54088
step:1384000 source:(1, 66) loss:5.47533
step:1384500 source:(1, 66) loss:6.05099
step:1385000 source:(1, 66) loss:7.81692
step:1385500 source:(1, 23) loss:4.72796
step:1386000 source:(1, 66) loss:5.38664
step:1386500 source:(1, 34) loss:5.61784
step:1387000 source:(1, 66) loss:6.08856
step:1387500 source:(1, 66) loss:5.20191
step:1388000 source:(1, 66) loss:7.87445
step:1388500 source:(1, 46) loss:6.93807
step:1389000 source:(1, 66) loss:5.5687
step:1389500 source:(1, 37) loss:5.84895
step:1390000 source:(1, 66) loss:6.16903
step:1390500 source:(1, 36) loss:4.33927
step:1391000 source:(1, 33) loss:4.6774
step:1391500 source:(1, 55) loss:5.04582
step:1392000 source:(1, 59) loss:7.50006
step:1392500 source:(1, 28) loss:5.58934
step:1393000 source:(1, 33) loss:5.46664
step:1393500 source:(1, 32) loss:5.9443
step:1394000 source:(1, 66) loss:4.11787
step:1394500 source:(1, 53) loss:5.02648
step:1395000 source:(1, 66) loss:5.20336
step:1395500 source:(1, 66) loss:6.03825
step:1396000 source:(1, 66) loss:5.14934
step:1396500 source:(1, 66) loss:6.45007
step:1397000 source:(1, 66) loss:5.99418
step:1397500 source:(1, 26) loss:5.02574
step:1398000 source:(1, 66) loss:4.59419
step:1398500 source:(1, 66) loss:6.85237
step:1399000 source:(1, 66) loss:6.74541
step:1399500 source:(1, 47) loss:5.83246
step:1400000 source:(1, 46) loss:5.94479
step:1400500 source:(1, 66) loss:6.07795
step:1401000 source:(1, 66) loss:6.57334
step:1401500 source:(1, 63) loss:6.42523
step:1402000 source:(1, 31) loss:6.10517
step:1402500 source:(1, 33) loss:4.78498
step:1403000 source:(1, 53) loss:5.31151
step:1403500 source:(1, 66) loss:5.37005
step:1404000 source:(1, 31) loss:6.19364
step:1404500 source:(1, 66) loss:5.37841
step:1405000 source:(1, 31) loss:5.6463
step:1405500 source:(1, 66) loss:9.29689
step:1406000 source:(1, 66) loss:6.4949
step:1406500 source:(1, 66) loss:7.53823
step:1407000 source:(1, 66) loss:5.27509
step:1407500 source:(1, 57) loss:5.8115
step:1408000 source:(1, 24) loss:5.67507
step:1408500 source:(1, 59) loss:5.39377
step:1409000 source:(1, 39) loss:5.45518
step:1409500 source:(1, 35) loss:7.15049
step:1410000 source:(1, 66) loss:5.57219
step:1410500 source:(1, 43) loss:5.77182
step:1411000 source:(1, 54) loss:4.99241
step:1411500 source:(1, 41) loss:5.04108
step:1412000 source:(1, 66) loss:6.86399
step:1412500 source:(1, 66) loss:6.58613
step:1413000 source:(1, 66) loss:7.45309
step:1413500 source:(1, 44) loss:7.2015
step:1414000 source:(1, 66) loss:5.59083
step:1414500 source:(1, 66) loss:4.89965
step:1415000 source:(1, 66) loss:5.56967
step:1415500 source:(1, 51) loss:6.06551
step:1416000 source:(1, 57) loss:5.40708
step:1416500 source:(1, 66) loss:7.07553
step:1417000 source:(1, 34) loss:6.42101
step:1417500 source:(1, 66) loss:6.95716
step:1418000 source:(1, 66) loss:5.29709
step:1418500 source:(1, 35) loss:6.16616
step:1419000 source:(1, 66) loss:6.87931
step:1419500 source:(1, 35) loss:5.76202
step:1420000 source:(1, 66) loss:6.35215
step:1420500 source:(1, 66) loss:5.27598
step:1421000 source:(1, 36) loss:5.79616
step:1421500 source:(1, 66) loss:4.98139
step:1422000 source:(1, 48) loss:5.09511
step:1422500 source:(1, 51) loss:4.65071
step:1423000 source:(1, 66) loss:5.38939
step:1423500 source:(1, 66) loss:7.33916
step:1424000 source:(1, 66) loss:6.4462
step:1424500 source:(1, 66) loss:6.72709
step:1425000 source:(1, 55) loss:5.86987
step:1425500 source:(1, 33) loss:6.82749
step:1426000 source:(1, 66) loss:6.14774
step:1426500 source:(1, 35) loss:6.34837
step:1427000 source:(1, 25) loss:5.60111
step:1427500 source:(1, 30) loss:5.56522
step:1428000 source:(1, 66) loss:5.40062
step:1428500 source:(1, 41) loss:5.9138
step:1429000 source:(1, 59) loss:6.24353
step:1429500 source:(1, 64) loss:5.36902
step:1430000 source:(1, 43) loss:5.43025
step:1430500 source:(1, 66) loss:5.55619
step:1431000 source:(1, 66) loss:6.453
step:1431500 source:(1, 66) loss:5.21144
step:1432000 source:(1, 39) loss:6.70616
step:1432500 source:(1, 66) loss:6.13271
step:1433000 source:(1, 52) loss:6.13181
step:1433500 source:(1, 66) loss:8.29824
step:1434000 source:(1, 66) loss:5.07029
step:1434500 source:(1, 66) loss:6.02943
step:1435000 source:(1, 66) loss:6.05522
step:1435500 source:(1, 62) loss:5.2543
step:1436000 source:(1, 66) loss:5.5232
step:1436500 source:(1, 66) loss:5.33883
step:1437000 source:(1, 38) loss:6.01358
step:1437500 source:(1, 66) loss:5.62939
step:1438000 source:(1, 30) loss:6.77649
step:1438500 source:(1, 66) loss:4.54267
step:1439000 source:(1, 33) loss:5.40499
step:1439500 source:(1, 66) loss:5.37
step:1440000 source:(1, 30) loss:5.87157
step:1440500 source:(1, 51) loss:6.81575
step:1441000 source:(1, 66) loss:5.50508
step:1441500 source:(1, 25) loss:6.31067
step:1442000 source:(1, 39) loss:6.07608
step:1442500 source:(1, 59) loss:7.53405
step:1443000 source:(1, 22) loss:6.92224
step:1443500 source:(1, 53) loss:5.94934
step:1444000 source:(1, 66) loss:5.38696
step:1444500 source:(1, 48) loss:4.8932
step:1445000 source:(1, 23) loss:6.39556
step:1445500 source:(1, 52) loss:4.72345
step:1446000 source:(1, 40) loss:5.25564
step:1446500 source:(1, 66) loss:7.32688
step:1447000 source:(1, 66) loss:5.79262
step:1447500 source:(1, 66) loss:6.25564
step:1448000 source:(1, 49) loss:5.46689
step:1448500 source:(1, 27) loss:6.56311
step:1449000 source:(1, 24) loss:6.26714
step:1449500 source:(1, 66) loss:6.86291
step:1450000 source:(1, 58) loss:5.96842
epoch:11 eval_bleu:52.48115062713623
2018-07-22 22:28:25.851572: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 22:28:25.852798: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 22:28:25.852854: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 22:43:59.953777: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 22:43:59.953849: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-22 22:43:59.953881: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:1450500 source:(1, 38) loss:5.56907
step:1451000 source:(1, 29) loss:3.8319
step:1451500 source:(1, 32) loss:5.731
step:1452000 source:(1, 66) loss:4.72436
step:1452500 source:(1, 66) loss:7.03854
step:1453000 source:(1, 65) loss:6.2186
step:1453500 source:(1, 66) loss:6.60505
step:1454000 source:(1, 66) loss:6.36802
step:1454500 source:(1, 66) loss:6.75402
step:1455000 source:(1, 28) loss:4.45181
step:1455500 source:(1, 61) loss:6.53083
step:1456000 source:(1, 66) loss:6.14901
step:1456500 source:(1, 66) loss:6.50209
step:1457000 source:(1, 61) loss:6.00778
step:1457500 source:(1, 22) loss:6.60111
step:1458000 source:(1, 47) loss:5.90099
step:1458500 source:(1, 66) loss:5.74471
step:1459000 source:(1, 66) loss:5.58606
step:1459500 source:(1, 41) loss:6.22636
step:1460000 source:(1, 66) loss:6.57574
step:1460500 source:(1, 41) loss:7.55551
step:1461000 source:(1, 66) loss:4.84826
step:1461500 source:(1, 66) loss:6.67019
step:1462000 source:(1, 66) loss:4.66372
step:1462500 source:(1, 66) loss:4.88682
step:1463000 source:(1, 38) loss:5.76631
step:1463500 source:(1, 66) loss:7.77134
step:1464000 source:(1, 66) loss:6.78222
step:1464500 source:(1, 56) loss:6.31798
step:1465000 source:(1, 43) loss:7.18273
step:1465500 source:(1, 66) loss:5.57712
step:1466000 source:(1, 41) loss:4.34041
step:1466500 source:(1, 66) loss:7.13477
step:1467000 source:(1, 66) loss:6.83869
step:1467500 source:(1, 66) loss:4.76928
step:1468000 source:(1, 66) loss:5.41639
step:1468500 source:(1, 66) loss:6.72823
step:1469000 source:(1, 66) loss:6.57163
step:1469500 source:(1, 66) loss:5.93572
step:1470000 source:(1, 66) loss:6.24177
step:1470500 source:(1, 42) loss:6.47277
step:1471000 source:(1, 66) loss:6.86841
step:1471500 source:(1, 66) loss:5.64916
step:1472000 source:(1, 66) loss:5.72059
step:1472500 source:(1, 24) loss:6.52226
step:1473000 source:(1, 66) loss:5.65092
step:1473500 source:(1, 42) loss:5.82196
step:1474000 source:(1, 66) loss:6.74561
step:1474500 source:(1, 66) loss:5.07771
step:1475000 source:(1, 27) loss:5.08242
step:1475500 source:(1, 66) loss:6.17271
step:1476000 source:(1, 32) loss:5.899
step:1476500 source:(1, 49) loss:5.75554
step:1477000 source:(1, 66) loss:5.44605
step:1477500 source:(1, 56) loss:5.92327
step:1478000 source:(1, 66) loss:5.91667
step:1478500 source:(1, 66) loss:5.75371
step:1479000 source:(1, 66) loss:7.00727
step:1479500 source:(1, 66) loss:7.10039
step:1480000 source:(1, 66) loss:5.15425
step:1480500 source:(1, 46) loss:6.63266
step:1481000 source:(1, 66) loss:6.57254
step:1481500 source:(1, 66) loss:5.7558
step:1482000 source:(1, 50) loss:7.47365
step:1482500 source:(1, 44) loss:5.803
step:1483000 source:(1, 30) loss:8.75651
step:1483500 source:(1, 66) loss:7.22077
step:1484000 source:(1, 66) loss:6.34997
step:1484500 source:(1, 66) loss:6.14869
step:1485000 source:(1, 66) loss:6.46945
step:1485500 source:(1, 23) loss:4.40278
step:1486000 source:(1, 66) loss:5.64053
step:1486500 source:(1, 34) loss:5.45632
step:1487000 source:(1, 66) loss:6.23545
step:1487500 source:(1, 66) loss:5.14432
step:1488000 source:(1, 66) loss:6.67402
step:1488500 source:(1, 46) loss:5.94964
step:1489000 source:(1, 66) loss:5.72389
step:1489500 source:(1, 37) loss:6.64214
step:1490000 source:(1, 66) loss:5.72055
step:1490500 source:(1, 36) loss:5.00225
step:1491000 source:(1, 33) loss:4.33746
step:1491500 source:(1, 55) loss:5.89853
step:1492000 source:(1, 59) loss:6.01951
step:1492500 source:(1, 28) loss:6.03282
step:1493000 source:(1, 33) loss:5.49821
step:1493500 source:(1, 32) loss:5.12336
step:1494000 source:(1, 66) loss:3.34034
step:1494500 source:(1, 53) loss:5.01917
step:1495000 source:(1, 66) loss:4.60334
step:1495500 source:(1, 66) loss:5.85496
step:1496000 source:(1, 66) loss:5.33836
step:1496500 source:(1, 66) loss:6.14258
step:1497000 source:(1, 66) loss:5.2358
step:1497500 source:(1, 26) loss:6.16274
step:1498000 source:(1, 66) loss:4.93184
step:1498500 source:(1, 66) loss:6.59017
step:1499000 source:(1, 66) loss:6.45436
step:1499500 source:(1, 47) loss:5.85772
step:1500000 source:(1, 46) loss:5.37977
step:1500500 source:(1, 66) loss:5.49984
step:1501000 source:(1, 66) loss:5.82605
step:1501500 source:(1, 63) loss:5.79865
step:1502000 source:(1, 31) loss:6.9875
step:1502500 source:(1, 33) loss:4.77024
step:1503000 source:(1, 53) loss:5.56957
step:1503500 source:(1, 66) loss:6.79781
step:1504000 source:(1, 31) loss:7.77214
step:1504500 source:(1, 66) loss:5.01994
step:1505000 source:(1, 31) loss:5.15476
step:1505500 source:(1, 66) loss:7.44955
step:1506000 source:(1, 66) loss:6.73819
step:1506500 source:(1, 66) loss:5.3466
step:1507000 source:(1, 66) loss:6.48797
step:1507500 source:(1, 57) loss:5.4685
step:1508000 source:(1, 24) loss:5.41042
step:1508500 source:(1, 59) loss:5.99332
step:1509000 source:(1, 39) loss:7.01279
step:1509500 source:(1, 35) loss:7.38045
step:1510000 source:(1, 66) loss:4.37925
step:1510500 source:(1, 43) loss:5.45191
step:1511000 source:(1, 54) loss:5.27742
step:1511500 source:(1, 41) loss:5.86034
step:1512000 source:(1, 66) loss:6.10154
step:1512500 source:(1, 66) loss:7.16652
step:1513000 source:(1, 66) loss:7.4464
step:1513500 source:(1, 44) loss:7.03986
step:1514000 source:(1, 66) loss:6.00344
step:1514500 source:(1, 66) loss:5.43971
step:1515000 source:(1, 66) loss:5.73034
step:1515500 source:(1, 51) loss:5.48119
step:1516000 source:(1, 57) loss:4.38958
step:1516500 source:(1, 66) loss:5.75517
step:1517000 source:(1, 34) loss:6.84733
step:1517500 source:(1, 66) loss:5.72334
step:1518000 source:(1, 66) loss:5.13062
step:1518500 source:(1, 35) loss:5.81336
step:1519000 source:(1, 66) loss:6.99877
step:1519500 source:(1, 35) loss:6.04525
step:1520000 source:(1, 66) loss:6.74371
step:1520500 source:(1, 66) loss:4.64376
step:1521000 source:(1, 36) loss:6.2826
step:1521500 source:(1, 66) loss:4.90718
step:1522000 source:(1, 48) loss:6.00001
step:1522500 source:(1, 51) loss:5.75984
step:1523000 source:(1, 66) loss:4.69062
step:1523500 source:(1, 66) loss:6.28024
step:1524000 source:(1, 66) loss:5.85062
step:1524500 source:(1, 66) loss:6.10977
step:1525000 source:(1, 55) loss:7.38494
step:1525500 source:(1, 33) loss:6.79664
step:1526000 source:(1, 66) loss:5.39051
step:1526500 source:(1, 35) loss:6.62987
step:1527000 source:(1, 25) loss:5.20501
step:1527500 source:(1, 30) loss:6.44885
step:1528000 source:(1, 66) loss:5.47807
step:1528500 source:(1, 41) loss:6.82813
step:1529000 source:(1, 59) loss:5.65965
step:1529500 source:(1, 64) loss:6.25217
step:1530000 source:(1, 43) loss:6.33374
step:1530500 source:(1, 66) loss:5.73526
step:1531000 source:(1, 66) loss:7.24417
step:1531500 source:(1, 66) loss:5.29427
step:1532000 source:(1, 39) loss:7.6995
step:1532500 source:(1, 66) loss:5.53588
step:1533000 source:(1, 52) loss:6.40091
step:1533500 source:(1, 66) loss:7.84584
step:1534000 source:(1, 66) loss:6.43713
step:1534500 source:(1, 66) loss:6.30919
step:1535000 source:(1, 66) loss:5.56373
step:1535500 source:(1, 62) loss:4.82668
step:1536000 source:(1, 66) loss:6.08511
step:1536500 source:(1, 66) loss:6.4766
step:1537000 source:(1, 38) loss:6.97767
step:1537500 source:(1, 66) loss:5.21034
step:1538000 source:(1, 30) loss:6.40539
step:1538500 source:(1, 66) loss:6.48156
step:1539000 source:(1, 33) loss:5.13172
step:1539500 source:(1, 66) loss:5.56367
step:1540000 source:(1, 30) loss:7.82588
step:1540500 source:(1, 51) loss:5.87586
step:1541000 source:(1, 66) loss:4.91456
step:1541500 source:(1, 25) loss:6.07227
step:1542000 source:(1, 39) loss:6.24685
step:1542500 source:(1, 59) loss:7.28515
step:1543000 source:(1, 22) loss:5.61405
step:1543500 source:(1, 53) loss:6.98038
step:1544000 source:(1, 66) loss:5.08734
step:1544500 source:(1, 48) loss:5.62019
step:1545000 source:(1, 23) loss:6.91978
step:1545500 source:(1, 52) loss:4.30254
step:1546000 source:(1, 40) loss:6.29356
step:1546500 source:(1, 66) loss:6.92566
step:1547000 source:(1, 66) loss:6.02101
step:1547500 source:(1, 66) loss:5.10396
step:1548000 source:(1, 49) loss:5.8571
step:1548500 source:(1, 27) loss:6.83125
step:1549000 source:(1, 24) loss:5.98876
step:1549500 source:(1, 66) loss:5.95275
step:1550000 source:(1, 58) loss:5.53749
epoch:12 eval_bleu:55.43292164802551
2018-07-23 01:33:52.942976: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 01:33:52.943059: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 01:33:52.943765: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 01:33:52.943827: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 01:49:11.235698: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 01:49:11.235772: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 01:49:11.235809: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:1550500 source:(1, 38) loss:5.50482
step:1551000 source:(1, 29) loss:5.66155
step:1551500 source:(1, 32) loss:6.71754
step:1552000 source:(1, 66) loss:6.25468
step:1552500 source:(1, 66) loss:7.35859
step:1553000 source:(1, 65) loss:5.6475
step:1553500 source:(1, 66) loss:6.39209
step:1554000 source:(1, 66) loss:9.20614
step:1554500 source:(1, 66) loss:7.85137
step:1555000 source:(1, 28) loss:5.11709
step:1555500 source:(1, 61) loss:7.31422
step:1556000 source:(1, 66) loss:5.35387
step:1556500 source:(1, 66) loss:6.14227
step:1557000 source:(1, 61) loss:5.93355
step:1557500 source:(1, 22) loss:6.24637
step:1558000 source:(1, 47) loss:5.34064
step:1558500 source:(1, 66) loss:5.9288
step:1559000 source:(1, 66) loss:7.45186
step:1559500 source:(1, 41) loss:5.43298
step:1560000 source:(1, 66) loss:5.68243
step:1560500 source:(1, 41) loss:5.12946
step:1561000 source:(1, 66) loss:5.68727
step:1561500 source:(1, 66) loss:5.65233
step:1562000 source:(1, 66) loss:5.33228
step:1562500 source:(1, 66) loss:5.94202
step:1563000 source:(1, 38) loss:6.38755
step:1563500 source:(1, 66) loss:6.45592
step:1564000 source:(1, 66) loss:6.50691
step:1564500 source:(1, 56) loss:5.93032
step:1565000 source:(1, 43) loss:6.82405
step:1565500 source:(1, 66) loss:5.18146
step:1566000 source:(1, 41) loss:4.50623
step:1566500 source:(1, 66) loss:7.34477
step:1567000 source:(1, 66) loss:5.69807
step:1567500 source:(1, 66) loss:5.93143
step:1568000 source:(1, 66) loss:6.36035
step:1568500 source:(1, 66) loss:6.671
step:1569000 source:(1, 66) loss:7.03894
step:1569500 source:(1, 66) loss:5.87225
step:1570000 source:(1, 66) loss:5.75195
step:1570500 source:(1, 42) loss:7.39422
step:1571000 source:(1, 66) loss:6.25243
step:1571500 source:(1, 66) loss:5.34733
step:1572000 source:(1, 66) loss:5.89565
step:1572500 source:(1, 24) loss:6.57002
step:1573000 source:(1, 66) loss:5.29188
step:1573500 source:(1, 42) loss:6.32539
step:1574000 source:(1, 66) loss:6.96274
step:1574500 source:(1, 66) loss:6.86458
step:1575000 source:(1, 27) loss:6.68442
step:1575500 source:(1, 66) loss:5.45389
step:1576000 source:(1, 32) loss:4.80052
step:1576500 source:(1, 49) loss:5.1725
step:1577000 source:(1, 66) loss:4.71844
step:1577500 source:(1, 56) loss:6.55985
step:1578000 source:(1, 66) loss:5.65478
step:1578500 source:(1, 66) loss:6.18447
step:1579000 source:(1, 66) loss:5.87919
step:1579500 source:(1, 66) loss:6.23362
step:1580000 source:(1, 66) loss:5.1086
step:1580500 source:(1, 46) loss:7.23016
step:1581000 source:(1, 66) loss:5.85294
step:1581500 source:(1, 66) loss:6.4964
step:1582000 source:(1, 50) loss:5.6733
step:1582500 source:(1, 44) loss:5.93829
step:1583000 source:(1, 30) loss:8.27036
step:1583500 source:(1, 66) loss:6.62901
step:1584000 source:(1, 66) loss:5.48479
step:1584500 source:(1, 66) loss:5.11578
step:1585000 source:(1, 66) loss:5.64477
step:1585500 source:(1, 23) loss:4.26585
step:1586000 source:(1, 66) loss:5.31284
step:1586500 source:(1, 34) loss:6.18548
step:1587000 source:(1, 66) loss:5.19179
step:1587500 source:(1, 66) loss:5.35441
step:1588000 source:(1, 66) loss:4.8632
step:1588500 source:(1, 46) loss:5.70721
step:1589000 source:(1, 66) loss:5.32656
step:1589500 source:(1, 37) loss:5.66184
step:1590000 source:(1, 66) loss:4.80674
step:1590500 source:(1, 36) loss:4.17736
step:1591000 source:(1, 33) loss:5.40765
step:1591500 source:(1, 55) loss:4.74794
step:1592000 source:(1, 59) loss:5.83733
step:1592500 source:(1, 28) loss:6.19761
step:1593000 source:(1, 33) loss:7.21874
step:1593500 source:(1, 32) loss:5.94228
step:1594000 source:(1, 66) loss:5.4805
step:1594500 source:(1, 53) loss:4.91499
step:1595000 source:(1, 66) loss:4.99343
step:1595500 source:(1, 66) loss:6.27455
step:1596000 source:(1, 66) loss:6.23288
step:1596500 source:(1, 66) loss:5.29184
step:1597000 source:(1, 66) loss:7.49562
step:1597500 source:(1, 26) loss:5.51042
step:1598000 source:(1, 66) loss:5.76135
step:1598500 source:(1, 66) loss:6.26258
step:1599000 source:(1, 66) loss:6.46811
step:1599500 source:(1, 47) loss:5.69593
step:1600000 source:(1, 46) loss:5.52643
step:1600500 source:(1, 66) loss:5.9108
step:1601000 source:(1, 66) loss:6.3042
step:1601500 source:(1, 63) loss:5.82855
step:1602000 source:(1, 31) loss:7.25538
step:1602500 source:(1, 33) loss:5.19079
step:1603000 source:(1, 53) loss:6.63299
step:1603500 source:(1, 66) loss:6.77352
step:1604000 source:(1, 31) loss:7.45671
step:1604500 source:(1, 66) loss:5.78936
step:1605000 source:(1, 31) loss:5.16088
step:1605500 source:(1, 66) loss:8.36909
step:1606000 source:(1, 66) loss:6.933
step:1606500 source:(1, 66) loss:7.19309
step:1607000 source:(1, 66) loss:5.70688
step:1607500 source:(1, 57) loss:5.31975
step:1608000 source:(1, 24) loss:5.19538
step:1608500 source:(1, 59) loss:7.28844
step:1609000 source:(1, 39) loss:5.68934
step:1609500 source:(1, 35) loss:8.23355
step:1610000 source:(1, 66) loss:5.69314
step:1610500 source:(1, 43) loss:5.27681
step:1611000 source:(1, 54) loss:6.38912
step:1611500 source:(1, 41) loss:5.84409
step:1612000 source:(1, 66) loss:6.02363
step:1612500 source:(1, 66) loss:6.40381
step:1613000 source:(1, 66) loss:6.39229
step:1613500 source:(1, 44) loss:7.06766
step:1614000 source:(1, 66) loss:7.88245
step:1614500 source:(1, 66) loss:4.49054
step:1615000 source:(1, 66) loss:6.04239
step:1615500 source:(1, 51) loss:4.94158
step:1616000 source:(1, 57) loss:5.83916
step:1616500 source:(1, 66) loss:6.20738
step:1617000 source:(1, 34) loss:6.83061
step:1617500 source:(1, 66) loss:6.10896
step:1618000 source:(1, 66) loss:6.01317
step:1618500 source:(1, 35) loss:6.64665
step:1619000 source:(1, 66) loss:7.201
step:1619500 source:(1, 35) loss:6.13787
step:1620000 source:(1, 66) loss:6.15287
step:1620500 source:(1, 66) loss:5.56382
step:1621000 source:(1, 36) loss:4.93638
step:1621500 source:(1, 66) loss:5.01949
step:1622000 source:(1, 48) loss:7.12976
step:1622500 source:(1, 51) loss:5.94954
step:1623000 source:(1, 66) loss:4.69896
step:1623500 source:(1, 66) loss:5.92363
step:1624000 source:(1, 66) loss:6.28487
step:1624500 source:(1, 66) loss:6.55697
step:1625000 source:(1, 55) loss:6.29456
step:1625500 source:(1, 33) loss:5.93519
step:1626000 source:(1, 66) loss:5.7119
step:1626500 source:(1, 35) loss:6.22002
step:1627000 source:(1, 25) loss:5.5306
step:1627500 source:(1, 30) loss:8.29654
step:1628000 source:(1, 66) loss:6.17451
step:1628500 source:(1, 41) loss:4.86157
step:1629000 source:(1, 59) loss:5.608
step:1629500 source:(1, 64) loss:6.4467
step:1630000 source:(1, 43) loss:5.03249
step:1630500 source:(1, 66) loss:4.46514
step:1631000 source:(1, 66) loss:6.25888
step:1631500 source:(1, 66) loss:4.69598
step:1632000 source:(1, 39) loss:6.47299
step:1632500 source:(1, 66) loss:5.62357
step:1633000 source:(1, 52) loss:6.66241
step:1633500 source:(1, 66) loss:8.06451
step:1634000 source:(1, 66) loss:6.61497
step:1634500 source:(1, 66) loss:6.22222
step:1635000 source:(1, 66) loss:5.70327
step:1635500 source:(1, 62) loss:5.00042
step:1636000 source:(1, 66) loss:4.73223
step:1636500 source:(1, 66) loss:5.56313
step:1637000 source:(1, 38) loss:6.57523
step:1637500 source:(1, 66) loss:5.38865
step:1638000 source:(1, 30) loss:7.16894
step:1638500 source:(1, 66) loss:5.72564
step:1639000 source:(1, 33) loss:5.3912
step:1639500 source:(1, 66) loss:6.71178
step:1640000 source:(1, 30) loss:5.8365
step:1640500 source:(1, 51) loss:6.66832
step:1641000 source:(1, 66) loss:4.38256
step:1641500 source:(1, 25) loss:4.73864
step:1642000 source:(1, 39) loss:6.20589
step:1642500 source:(1, 59) loss:6.99014
step:1643000 source:(1, 22) loss:6.82498
step:1643500 source:(1, 53) loss:7.61917
step:1644000 source:(1, 66) loss:5.65997
step:1644500 source:(1, 48) loss:4.89031
step:1645000 source:(1, 23) loss:5.9299
step:1645500 source:(1, 52) loss:4.7207
step:1646000 source:(1, 40) loss:5.94297
step:1646500 source:(1, 66) loss:6.31998
step:1647000 source:(1, 66) loss:5.9738
step:1647500 source:(1, 66) loss:6.37697
step:1648000 source:(1, 49) loss:5.36823
step:1648500 source:(1, 27) loss:7.83173
step:1649000 source:(1, 24) loss:6.15581
step:1649500 source:(1, 66) loss:7.84718
step:1650000 source:(1, 58) loss:4.98925
epoch:13 eval_bleu:55.89080452919006
2018-07-23 04:36:24.542727: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 04:36:24.543864: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 04:36:24.543919: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 04:51:34.760206: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 04:51:34.760283: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 04:51:34.760304: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:1650500 source:(1, 38) loss:5.2848
step:1651000 source:(1, 29) loss:4.13871
step:1651500 source:(1, 32) loss:5.63701
step:1652000 source:(1, 66) loss:6.13773
step:1652500 source:(1, 66) loss:6.92038
step:1653000 source:(1, 65) loss:5.6911
step:1653500 source:(1, 66) loss:5.98659
step:1654000 source:(1, 66) loss:7.81495
step:1654500 source:(1, 66) loss:8.00341
step:1655000 source:(1, 28) loss:3.73044
step:1655500 source:(1, 61) loss:5.97886
step:1656000 source:(1, 66) loss:5.78502
step:1656500 source:(1, 66) loss:5.95361
step:1657000 source:(1, 61) loss:6.18545
step:1657500 source:(1, 22) loss:6.08715
step:1658000 source:(1, 47) loss:5.8867
step:1658500 source:(1, 66) loss:5.30709
step:1659000 source:(1, 66) loss:8.2097
step:1659500 source:(1, 41) loss:5.8004
step:1660000 source:(1, 66) loss:4.97582
step:1660500 source:(1, 41) loss:6.62083
step:1661000 source:(1, 66) loss:5.03843
step:1661500 source:(1, 66) loss:5.38289
step:1662000 source:(1, 66) loss:4.19015
step:1662500 source:(1, 66) loss:6.16777
step:1663000 source:(1, 38) loss:5.85718
step:1663500 source:(1, 66) loss:7.64767
step:1664000 source:(1, 66) loss:7.14212
step:1664500 source:(1, 56) loss:5.93109
step:1665000 source:(1, 43) loss:6.58871
step:1665500 source:(1, 66) loss:5.17067
step:1666000 source:(1, 41) loss:4.50673
step:1666500 source:(1, 66) loss:7.17109
step:1667000 source:(1, 66) loss:6.01364
step:1667500 source:(1, 66) loss:5.71243
step:1668000 source:(1, 66) loss:5.85445
step:1668500 source:(1, 66) loss:6.11674
step:1669000 source:(1, 66) loss:6.48499
step:1669500 source:(1, 66) loss:5.75439
step:1670000 source:(1, 66) loss:5.76751
step:1670500 source:(1, 42) loss:6.55717
step:1671000 source:(1, 66) loss:6.58462
step:1671500 source:(1, 66) loss:7.18043
step:1672000 source:(1, 66) loss:6.88183
step:1672500 source:(1, 24) loss:7.24075
step:1673000 source:(1, 66) loss:6.36066
step:1673500 source:(1, 42) loss:6.77284
step:1674000 source:(1, 66) loss:6.63849
step:1674500 source:(1, 66) loss:6.02342
step:1675000 source:(1, 27) loss:6.65047
step:1675500 source:(1, 66) loss:6.58536
step:1676000 source:(1, 32) loss:5.43738
step:1676500 source:(1, 49) loss:6.38516
step:1677000 source:(1, 66) loss:4.43551
step:1677500 source:(1, 56) loss:5.92232
step:1678000 source:(1, 66) loss:6.12932
step:1678500 source:(1, 66) loss:5.85264
step:1679000 source:(1, 66) loss:6.617
step:1679500 source:(1, 66) loss:5.62914
step:1680000 source:(1, 66) loss:5.83696
step:1680500 source:(1, 46) loss:5.94372
step:1681000 source:(1, 66) loss:7.55093
step:1681500 source:(1, 66) loss:4.86053
step:1682000 source:(1, 50) loss:5.60177
step:1682500 source:(1, 44) loss:6.54927
step:1683000 source:(1, 30) loss:8.63313
step:1683500 source:(1, 66) loss:4.35362
step:1684000 source:(1, 66) loss:6.60244
step:1684500 source:(1, 66) loss:5.78471
step:1685000 source:(1, 66) loss:6.72888
step:1685500 source:(1, 23) loss:4.70052
step:1686000 source:(1, 66) loss:6.36669
step:1686500 source:(1, 34) loss:5.87395
step:1687000 source:(1, 66) loss:5.62241
step:1687500 source:(1, 66) loss:5.33001
step:1688000 source:(1, 66) loss:5.0331
step:1688500 source:(1, 46) loss:7.21066
step:1689000 source:(1, 66) loss:5.365
step:1689500 source:(1, 37) loss:6.04389
step:1690000 source:(1, 66) loss:5.83894
step:1690500 source:(1, 36) loss:5.27803
step:1691000 source:(1, 33) loss:5.45115
step:1691500 source:(1, 55) loss:5.6347
step:1692000 source:(1, 59) loss:5.75535
step:1692500 source:(1, 28) loss:5.88135
step:1693000 source:(1, 33) loss:5.7746
step:1693500 source:(1, 32) loss:6.13085
step:1694000 source:(1, 66) loss:5.42058
step:1694500 source:(1, 53) loss:5.78833
step:1695000 source:(1, 66) loss:5.91546
step:1695500 source:(1, 66) loss:6.23309
step:1696000 source:(1, 66) loss:5.21058
step:1696500 source:(1, 66) loss:6.64364
step:1697000 source:(1, 66) loss:6.89749
step:1697500 source:(1, 26) loss:5.31881
step:1698000 source:(1, 66) loss:5.505
step:1698500 source:(1, 66) loss:6.43148
step:1699000 source:(1, 66) loss:6.46678
step:1699500 source:(1, 47) loss:4.6516
step:1700000 source:(1, 46) loss:6.76641
step:1700500 source:(1, 66) loss:5.5871
step:1701000 source:(1, 66) loss:5.46809
step:1701500 source:(1, 63) loss:7.19961
step:1702000 source:(1, 31) loss:7.41693
step:1702500 source:(1, 33) loss:4.46014
step:1703000 source:(1, 53) loss:6.40525
step:1703500 source:(1, 66) loss:5.72347
step:1704000 source:(1, 31) loss:7.44744
step:1704500 source:(1, 66) loss:6.36574
step:1705000 source:(1, 31) loss:6.02204
step:1705500 source:(1, 66) loss:7.92655
step:1706000 source:(1, 66) loss:6.77993
step:1706500 source:(1, 66) loss:6.39708
step:1707000 source:(1, 66) loss:5.77044
step:1707500 source:(1, 57) loss:5.56647
step:1708000 source:(1, 24) loss:5.26058
step:1708500 source:(1, 59) loss:5.2762
step:1709000 source:(1, 39) loss:4.91407
step:1709500 source:(1, 35) loss:9.11378
step:1710000 source:(1, 66) loss:6.64268
step:1710500 source:(1, 43) loss:6.20643
step:1711000 source:(1, 54) loss:6.07295
step:1711500 source:(1, 41) loss:5.84304
step:1712000 source:(1, 66) loss:5.91358
step:1712500 source:(1, 66) loss:6.80738
step:1713000 source:(1, 66) loss:7.31758
step:1713500 source:(1, 44) loss:6.95011
step:1714000 source:(1, 66) loss:5.72805
step:1714500 source:(1, 66) loss:4.66721
step:1715000 source:(1, 66) loss:6.48918
step:1715500 source:(1, 51) loss:6.55247
step:1716000 source:(1, 57) loss:5.21011
step:1716500 source:(1, 66) loss:4.94611
step:1717000 source:(1, 34) loss:6.14942
step:1717500 source:(1, 66) loss:6.52442
step:1718000 source:(1, 66) loss:5.12226
step:1718500 source:(1, 35) loss:5.83904
step:1719000 source:(1, 66) loss:7.00995
step:1719500 source:(1, 35) loss:6.07801
step:1720000 source:(1, 66) loss:6.71261
step:1720500 source:(1, 66) loss:5.45683
step:1721000 source:(1, 36) loss:5.4448
step:1721500 source:(1, 66) loss:6.70045
step:1722000 source:(1, 48) loss:6.31779
step:1722500 source:(1, 51) loss:4.63633
step:1723000 source:(1, 66) loss:5.51058
step:1723500 source:(1, 66) loss:7.15052
step:1724000 source:(1, 66) loss:7.45198
step:1724500 source:(1, 66) loss:6.38566
step:1725000 source:(1, 55) loss:6.10028
step:1725500 source:(1, 33) loss:5.53995
step:1726000 source:(1, 66) loss:5.56995
step:1726500 source:(1, 35) loss:6.68031
step:1727000 source:(1, 25) loss:6.03962
step:1727500 source:(1, 30) loss:6.18073
step:1728000 source:(1, 66) loss:4.21979
step:1728500 source:(1, 41) loss:4.64992
step:1729000 source:(1, 59) loss:5.68404
step:1729500 source:(1, 64) loss:5.2139
step:1730000 source:(1, 43) loss:5.25542
step:1730500 source:(1, 66) loss:5.39769
step:1731000 source:(1, 66) loss:6.41208
step:1731500 source:(1, 66) loss:4.826
step:1732000 source:(1, 39) loss:7.1774
step:1732500 source:(1, 66) loss:5.09047
step:1733000 source:(1, 52) loss:5.98143
step:1733500 source:(1, 66) loss:6.99306
step:1734000 source:(1, 66) loss:5.02507
step:1734500 source:(1, 66) loss:6.8162
step:1735000 source:(1, 66) loss:6.35244
step:1735500 source:(1, 62) loss:5.43188
step:1736000 source:(1, 66) loss:7.41507
step:1736500 source:(1, 66) loss:5.53083
step:1737000 source:(1, 38) loss:5.24288
step:1737500 source:(1, 66) loss:5.5809
step:1738000 source:(1, 30) loss:6.18042
step:1738500 source:(1, 66) loss:6.29732
step:1739000 source:(1, 33) loss:5.31113
step:1739500 source:(1, 66) loss:7.61193
step:1740000 source:(1, 30) loss:5.77773
step:1740500 source:(1, 51) loss:6.39995
step:1741000 source:(1, 66) loss:5.59845
step:1741500 source:(1, 25) loss:6.6226
step:1742000 source:(1, 39) loss:6.30213
step:1742500 source:(1, 59) loss:6.71574
step:1743000 source:(1, 22) loss:5.5977
step:1743500 source:(1, 53) loss:5.93535
step:1744000 source:(1, 66) loss:5.78864
step:1744500 source:(1, 48) loss:5.76413
step:1745000 source:(1, 23) loss:6.22466
step:1745500 source:(1, 52) loss:4.53987
step:1746000 source:(1, 40) loss:5.54435
step:1746500 source:(1, 66) loss:7.23349
step:1747000 source:(1, 66) loss:6.505
step:1747500 source:(1, 66) loss:5.39666
step:1748000 source:(1, 49) loss:5.97397
step:1748500 source:(1, 27) loss:6.78529
step:1749000 source:(1, 24) loss:5.9614
step:1749500 source:(1, 66) loss:6.6963
step:1750000 source:(1, 58) loss:6.03293
epoch:14 eval_bleu:54.76679801940918
2018-07-23 07:36:19.714622: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 07:36:19.714703: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 07:51:30.619742: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 07:51:30.619798: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 07:51:30.619822: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:1750500 source:(1, 38) loss:5.49152
step:1751000 source:(1, 29) loss:3.12397
step:1751500 source:(1, 32) loss:5.7059
step:1752000 source:(1, 66) loss:5.70705
step:1752500 source:(1, 66) loss:6.17411
step:1753000 source:(1, 65) loss:5.60078
step:1753500 source:(1, 66) loss:5.56454
step:1754000 source:(1, 66) loss:5.07969
step:1754500 source:(1, 66) loss:8.08657
step:1755000 source:(1, 28) loss:4.66494
step:1755500 source:(1, 61) loss:6.1057
step:1756000 source:(1, 66) loss:5.61572
step:1756500 source:(1, 66) loss:5.87446
step:1757000 source:(1, 61) loss:5.89401
step:1757500 source:(1, 22) loss:6.43275
step:1758000 source:(1, 47) loss:5.3303
step:1758500 source:(1, 66) loss:5.58496
step:1759000 source:(1, 66) loss:6.92674
step:1759500 source:(1, 41) loss:7.45623
step:1760000 source:(1, 66) loss:5.83958
step:1760500 source:(1, 41) loss:6.2561
step:1761000 source:(1, 66) loss:5.38128
step:1761500 source:(1, 66) loss:5.56369
step:1762000 source:(1, 66) loss:4.79812
step:1762500 source:(1, 66) loss:6.46502
step:1763000 source:(1, 38) loss:5.66062
step:1763500 source:(1, 66) loss:6.19686
step:1764000 source:(1, 66) loss:6.93177
step:1764500 source:(1, 56) loss:6.66115
step:1765000 source:(1, 43) loss:5.90328
step:1765500 source:(1, 66) loss:5.8911
step:1766000 source:(1, 41) loss:4.67341
step:1766500 source:(1, 66) loss:6.7528
step:1767000 source:(1, 66) loss:5.95609
step:1767500 source:(1, 66) loss:5.38342
step:1768000 source:(1, 66) loss:6.55086
step:1768500 source:(1, 66) loss:6.00658
step:1769000 source:(1, 66) loss:6.95724
step:1769500 source:(1, 66) loss:5.12213
step:1770000 source:(1, 66) loss:5.62923
step:1770500 source:(1, 42) loss:7.18483
step:1771000 source:(1, 66) loss:6.56595
step:1771500 source:(1, 66) loss:4.70799
step:1772000 source:(1, 66) loss:4.74149
step:1772500 source:(1, 24) loss:7.02869
step:1773000 source:(1, 66) loss:5.63799
step:1773500 source:(1, 42) loss:7.70101
step:1774000 source:(1, 66) loss:6.11283
step:1774500 source:(1, 66) loss:6.88428
step:1775000 source:(1, 27) loss:6.38371
step:1775500 source:(1, 66) loss:6.80377
step:1776000 source:(1, 32) loss:4.98205
step:1776500 source:(1, 49) loss:6.22803
step:1777000 source:(1, 66) loss:4.94582
step:1777500 source:(1, 56) loss:5.82422
step:1778000 source:(1, 66) loss:4.82032
step:1778500 source:(1, 66) loss:6.57523
step:1779000 source:(1, 66) loss:5.59399
step:1779500 source:(1, 66) loss:7.47531
step:1780000 source:(1, 66) loss:6.06357
step:1780500 source:(1, 46) loss:7.54961
step:1781000 source:(1, 66) loss:5.2651
step:1781500 source:(1, 66) loss:6.15309
step:1782000 source:(1, 50) loss:5.79787
step:1782500 source:(1, 44) loss:5.99293
step:1783000 source:(1, 30) loss:8.11653
step:1783500 source:(1, 66) loss:6.05397
step:1784000 source:(1, 66) loss:5.84934
step:1784500 source:(1, 66) loss:5.55726
step:1785000 source:(1, 66) loss:6.1658
step:1785500 source:(1, 23) loss:4.91634
step:1786000 source:(1, 66) loss:4.62639
step:1786500 source:(1, 34) loss:5.29288
step:1787000 source:(1, 66) loss:5.85639
step:1787500 source:(1, 66) loss:5.48539
step:1788000 source:(1, 66) loss:5.24153
step:1788500 source:(1, 46) loss:4.73017
step:1789000 source:(1, 66) loss:6.7977
step:1789500 source:(1, 37) loss:6.0368
step:1790000 source:(1, 66) loss:5.99167
step:1790500 source:(1, 36) loss:4.09695
step:1791000 source:(1, 33) loss:5.15013
step:1791500 source:(1, 55) loss:5.5207
step:1792000 source:(1, 59) loss:6.01056
step:1792500 source:(1, 28) loss:5.32428
step:1793000 source:(1, 33) loss:5.62143
step:1793500 source:(1, 32) loss:5.41309
step:1794000 source:(1, 66) loss:4.8343
step:1794500 source:(1, 53) loss:4.82639
step:1795000 source:(1, 66) loss:4.9995
step:1795500 source:(1, 66) loss:5.29475
step:1796000 source:(1, 66) loss:4.47718
step:1796500 source:(1, 66) loss:6.17099
step:1797000 source:(1, 66) loss:6.40211
step:1797500 source:(1, 26) loss:5.05417
step:1798000 source:(1, 66) loss:5.52129
step:1798500 source:(1, 66) loss:6.10357
step:1799000 source:(1, 66) loss:7.77106
step:1799500 source:(1, 47) loss:5.09625
step:1800000 source:(1, 46) loss:5.61684
step:1800500 source:(1, 66) loss:6.51769
step:1801000 source:(1, 66) loss:5.56245
step:1801500 source:(1, 63) loss:6.92951
step:1802000 source:(1, 31) loss:7.11473
step:1802500 source:(1, 33) loss:5.34278
step:1803000 source:(1, 53) loss:5.59734
step:1803500 source:(1, 66) loss:6.30911
step:1804000 source:(1, 31) loss:7.93629
step:1804500 source:(1, 66) loss:4.79143
step:1805000 source:(1, 31) loss:5.51812
step:1805500 source:(1, 66) loss:7.20936
step:1806000 source:(1, 66) loss:6.33273
step:1806500 source:(1, 66) loss:6.10372
step:1807000 source:(1, 66) loss:5.86165
step:1807500 source:(1, 57) loss:5.95814
step:1808000 source:(1, 24) loss:5.12708
step:1808500 source:(1, 59) loss:7.41013
step:1809000 source:(1, 39) loss:5.9475
step:1809500 source:(1, 35) loss:7.85658
step:1810000 source:(1, 66) loss:4.93237
step:1810500 source:(1, 43) loss:4.89382
step:1811000 source:(1, 54) loss:6.32461
step:1811500 source:(1, 41) loss:5.35697
step:1812000 source:(1, 66) loss:5.79635
step:1812500 source:(1, 66) loss:6.40885
step:1813000 source:(1, 66) loss:6.38424
step:1813500 source:(1, 44) loss:6.79443
step:1814000 source:(1, 66) loss:6.16965
step:1814500 source:(1, 66) loss:4.47091
step:1815000 source:(1, 66) loss:4.52614
step:1815500 source:(1, 51) loss:6.47045
step:1816000 source:(1, 57) loss:5.76882
step:1816500 source:(1, 66) loss:6.66296
step:1817000 source:(1, 34) loss:7.13352
step:1817500 source:(1, 66) loss:5.65396
step:1818000 source:(1, 66) loss:4.80502
step:1818500 source:(1, 35) loss:5.23139
step:1819000 source:(1, 66) loss:5.2194
step:1819500 source:(1, 35) loss:6.15207
step:1820000 source:(1, 66) loss:7.87381
step:1820500 source:(1, 66) loss:4.57879
step:1821000 source:(1, 36) loss:5.49144
step:1821500 source:(1, 66) loss:5.61173
step:1822000 source:(1, 48) loss:6.60744
step:1822500 source:(1, 51) loss:5.51265
step:1823000 source:(1, 66) loss:5.71509
step:1823500 source:(1, 66) loss:5.4846
step:1824000 source:(1, 66) loss:6.03549
step:1824500 source:(1, 66) loss:5.40421
step:1825000 source:(1, 55) loss:6.49341
step:1825500 source:(1, 33) loss:7.18319
step:1826000 source:(1, 66) loss:5.02955
step:1826500 source:(1, 35) loss:6.67431
step:1827000 source:(1, 25) loss:5.22982
step:1827500 source:(1, 30) loss:6.88431
step:1828000 source:(1, 66) loss:4.39538
step:1828500 source:(1, 41) loss:6.16441
step:1829000 source:(1, 59) loss:6.40475
step:1829500 source:(1, 64) loss:5.55054
step:1830000 source:(1, 43) loss:6.05231
step:1830500 source:(1, 66) loss:5.9211
step:1831000 source:(1, 66) loss:6.76319
step:1831500 source:(1, 66) loss:5.26148
step:1832000 source:(1, 39) loss:8.07032
step:1832500 source:(1, 66) loss:5.94721
step:1833000 source:(1, 52) loss:4.95761
step:1833500 source:(1, 66) loss:7.40167
step:1834000 source:(1, 66) loss:6.51143
step:1834500 source:(1, 66) loss:6.73673
step:1835000 source:(1, 66) loss:5.54331
step:1835500 source:(1, 62) loss:5.33408
step:1836000 source:(1, 66) loss:6.30576
step:1836500 source:(1, 66) loss:6.5044
step:1837000 source:(1, 38) loss:6.21114
step:1837500 source:(1, 66) loss:5.61479
step:1838000 source:(1, 30) loss:6.71498
step:1838500 source:(1, 66) loss:5.13013
step:1839000 source:(1, 33) loss:5.46912
step:1839500 source:(1, 66) loss:6.63173
step:1840000 source:(1, 30) loss:6.84508
step:1840500 source:(1, 51) loss:6.54713
step:1841000 source:(1, 66) loss:5.70636
step:1841500 source:(1, 25) loss:4.87261
step:1842000 source:(1, 39) loss:5.71208
step:1842500 source:(1, 59) loss:6.39911
step:1843000 source:(1, 22) loss:4.82834
step:1843500 source:(1, 53) loss:5.29227
step:1844000 source:(1, 66) loss:6.0951
step:1844500 source:(1, 48) loss:5.1389
step:1845000 source:(1, 23) loss:6.45655
step:1845500 source:(1, 52) loss:6.33957
step:1846000 source:(1, 40) loss:4.06849
step:1846500 source:(1, 66) loss:7.64919
step:1847000 source:(1, 66) loss:5.57434
step:1847500 source:(1, 66) loss:5.7959
step:1848000 source:(1, 49) loss:5.86495
step:1848500 source:(1, 27) loss:6.69305
step:1849000 source:(1, 24) loss:6.31288
step:1849500 source:(1, 66) loss:6.61254
step:1850000 source:(1, 58) loss:6.59058
epoch:15 eval_bleu:55.60705065727234
2018-07-23 10:36:30.315400: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 10:36:30.315438: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 10:36:30.315494: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 10:36:30.316520: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 10:36:30.316550: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 10:51:37.731291: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 10:51:37.731348: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 10:51:37.731368: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:1850500 source:(1, 38) loss:5.0314
step:1851000 source:(1, 29) loss:4.68542
step:1851500 source:(1, 32) loss:5.70415
step:1852000 source:(1, 66) loss:4.92968
step:1852500 source:(1, 66) loss:5.29431
step:1853000 source:(1, 65) loss:5.62603
step:1853500 source:(1, 66) loss:5.70629
step:1854000 source:(1, 66) loss:6.83827
step:1854500 source:(1, 66) loss:5.18661
step:1855000 source:(1, 28) loss:5.22473
step:1855500 source:(1, 61) loss:6.06016
step:1856000 source:(1, 66) loss:6.17233
step:1856500 source:(1, 66) loss:6.15506
step:1857000 source:(1, 61) loss:6.33006
step:1857500 source:(1, 22) loss:6.63618
step:1858000 source:(1, 47) loss:5.08971
step:1858500 source:(1, 66) loss:6.22006
step:1859000 source:(1, 66) loss:7.4518
step:1859500 source:(1, 41) loss:7.01754
step:1860000 source:(1, 66) loss:5.75182
step:1860500 source:(1, 41) loss:7.24388
step:1861000 source:(1, 66) loss:6.22266
step:1861500 source:(1, 66) loss:4.34622
step:1862000 source:(1, 66) loss:4.39581
step:1862500 source:(1, 66) loss:5.92234
step:1863000 source:(1, 38) loss:5.61131
step:1863500 source:(1, 66) loss:6.67218
step:1864000 source:(1, 66) loss:6.5998
step:1864500 source:(1, 56) loss:5.50081
step:1865000 source:(1, 43) loss:5.82012
step:1865500 source:(1, 66) loss:5.32154
step:1866000 source:(1, 41) loss:4.90678
step:1866500 source:(1, 66) loss:7.54574
step:1867000 source:(1, 66) loss:7.18754
step:1867500 source:(1, 66) loss:6.15212
step:1868000 source:(1, 66) loss:5.96183
step:1868500 source:(1, 66) loss:5.97821
step:1869000 source:(1, 66) loss:6.712
step:1869500 source:(1, 66) loss:6.00026
step:1870000 source:(1, 66) loss:7.09302
step:1870500 source:(1, 42) loss:6.46602
step:1871000 source:(1, 66) loss:6.90333
step:1871500 source:(1, 66) loss:5.41729
step:1872000 source:(1, 66) loss:5.95762
step:1872500 source:(1, 24) loss:6.28012
step:1873000 source:(1, 66) loss:6.41575
step:1873500 source:(1, 42) loss:7.87087
step:1874000 source:(1, 66) loss:5.57703
step:1874500 source:(1, 66) loss:5.98967
step:1875000 source:(1, 27) loss:5.76017
step:1875500 source:(1, 66) loss:7.2917
step:1876000 source:(1, 32) loss:5.51882
step:1876500 source:(1, 49) loss:6.01843
step:1877000 source:(1, 66) loss:5.85334
step:1877500 source:(1, 56) loss:6.26358
step:1878000 source:(1, 66) loss:4.96208
step:1878500 source:(1, 66) loss:6.27483
step:1879000 source:(1, 66) loss:7.94347
step:1879500 source:(1, 66) loss:6.76699
step:1880000 source:(1, 66) loss:5.36214
step:1880500 source:(1, 46) loss:6.46042
step:1881000 source:(1, 66) loss:6.88436
step:1881500 source:(1, 66) loss:5.39667
step:1882000 source:(1, 50) loss:6.47665
step:1882500 source:(1, 44) loss:6.32682
step:1883000 source:(1, 30) loss:7.99961
step:1883500 source:(1, 66) loss:6.02626
step:1884000 source:(1, 66) loss:6.19954
step:1884500 source:(1, 66) loss:5.87571
step:1885000 source:(1, 66) loss:6.94678
step:1885500 source:(1, 23) loss:4.55527
step:1886000 source:(1, 66) loss:6.72042
step:1886500 source:(1, 34) loss:5.47308
step:1887000 source:(1, 66) loss:5.93714
step:1887500 source:(1, 66) loss:5.61859
step:1888000 source:(1, 66) loss:6.30443
step:1888500 source:(1, 46) loss:7.12039
step:1889000 source:(1, 66) loss:4.49877
step:1889500 source:(1, 37) loss:5.66584
step:1890000 source:(1, 66) loss:4.79791
step:1890500 source:(1, 36) loss:4.39448
step:1891000 source:(1, 33) loss:4.55515
step:1891500 source:(1, 55) loss:5.00681
step:1892000 source:(1, 59) loss:6.37602
step:1892500 source:(1, 28) loss:6.18109
step:1893000 source:(1, 33) loss:6.27384
step:1893500 source:(1, 32) loss:7.18317
step:1894000 source:(1, 66) loss:5.68004
step:1894500 source:(1, 53) loss:4.32366
step:1895000 source:(1, 66) loss:6.4138
step:1895500 source:(1, 66) loss:5.87759
step:1896000 source:(1, 66) loss:4.99506
step:1896500 source:(1, 66) loss:6.94191
step:1897000 source:(1, 66) loss:5.81009
step:1897500 source:(1, 26) loss:4.96088
step:1898000 source:(1, 66) loss:4.0665
step:1898500 source:(1, 66) loss:6.93775
step:1899000 source:(1, 66) loss:6.56849
step:1899500 source:(1, 47) loss:5.38625
step:1900000 source:(1, 46) loss:5.7056
step:1900500 source:(1, 66) loss:4.77669
step:1901000 source:(1, 66) loss:4.88823
step:1901500 source:(1, 63) loss:6.29178
step:1902000 source:(1, 31) loss:6.96679
step:1902500 source:(1, 33) loss:6.23692
step:1903000 source:(1, 53) loss:5.20052
step:1903500 source:(1, 66) loss:7.49616
step:1904000 source:(1, 31) loss:6.6994
step:1904500 source:(1, 66) loss:6.77171
step:1905000 source:(1, 31) loss:5.73482
step:1905500 source:(1, 66) loss:8.52791
step:1906000 source:(1, 66) loss:7.23688
step:1906500 source:(1, 66) loss:6.16758
step:1907000 source:(1, 66) loss:5.67828
step:1907500 source:(1, 57) loss:5.81317
step:1908000 source:(1, 24) loss:5.37474
step:1908500 source:(1, 59) loss:6.71232
step:1909000 source:(1, 39) loss:6.06623
step:1909500 source:(1, 35) loss:8.70521
step:1910000 source:(1, 66) loss:5.19234
step:1910500 source:(1, 43) loss:5.08864
step:1911000 source:(1, 54) loss:5.24982
step:1911500 source:(1, 41) loss:5.56017
step:1912000 source:(1, 66) loss:5.40883
step:1912500 source:(1, 66) loss:6.07692
step:1913000 source:(1, 66) loss:6.15342
step:1913500 source:(1, 44) loss:7.69238
step:1914000 source:(1, 66) loss:5.53995
step:1914500 source:(1, 66) loss:4.14747
step:1915000 source:(1, 66) loss:6.13471
step:1915500 source:(1, 51) loss:4.78579
step:1916000 source:(1, 57) loss:5.45543
step:1916500 source:(1, 66) loss:5.39088
step:1917000 source:(1, 34) loss:7.53511
step:1917500 source:(1, 66) loss:6.92479
step:1918000 source:(1, 66) loss:4.74816
step:1918500 source:(1, 35) loss:6.03101
step:1919000 source:(1, 66) loss:6.9349
step:1919500 source:(1, 35) loss:5.81013
step:1920000 source:(1, 66) loss:6.58391
step:1920500 source:(1, 66) loss:5.30959
step:1921000 source:(1, 36) loss:5.37895
step:1921500 source:(1, 66) loss:6.21494
step:1922000 source:(1, 48) loss:6.41625
step:1922500 source:(1, 51) loss:4.29455
step:1923000 source:(1, 66) loss:6.637
step:1923500 source:(1, 66) loss:6.95807
step:1924000 source:(1, 66) loss:7.10544
step:1924500 source:(1, 66) loss:6.74568
step:1925000 source:(1, 55) loss:6.7398
step:1925500 source:(1, 33) loss:5.88244
step:1926000 source:(1, 66) loss:5.15524
step:1926500 source:(1, 35) loss:6.20785
step:1927000 source:(1, 25) loss:5.45929
step:1927500 source:(1, 30) loss:6.31677
step:1928000 source:(1, 66) loss:5.75302
step:1928500 source:(1, 41) loss:5.75701
step:1929000 source:(1, 59) loss:5.91712
step:1929500 source:(1, 64) loss:5.24766
step:1930000 source:(1, 43) loss:5.60458
step:1930500 source:(1, 66) loss:6.41541
step:1931000 source:(1, 66) loss:4.09399
step:1931500 source:(1, 66) loss:4.28607
step:1932000 source:(1, 39) loss:7.83532
step:1932500 source:(1, 66) loss:6.15149
step:1933000 source:(1, 52) loss:5.4127
step:1933500 source:(1, 66) loss:6.04791
step:1934000 source:(1, 66) loss:7.05344
step:1934500 source:(1, 66) loss:6.7949
step:1935000 source:(1, 66) loss:5.92504
step:1935500 source:(1, 62) loss:5.57178
step:1936000 source:(1, 66) loss:4.8604
step:1936500 source:(1, 66) loss:6.40086
step:1937000 source:(1, 38) loss:5.46025
step:1937500 source:(1, 66) loss:6.31657
step:1938000 source:(1, 30) loss:6.18845
step:1938500 source:(1, 66) loss:6.0144
step:1939000 source:(1, 33) loss:5.73629
step:1939500 source:(1, 66) loss:6.10576
step:1940000 source:(1, 30) loss:7.25291
step:1940500 source:(1, 51) loss:4.70647
step:1941000 source:(1, 66) loss:4.21434
step:1941500 source:(1, 25) loss:5.66559
step:1942000 source:(1, 39) loss:5.25993
step:1942500 source:(1, 59) loss:5.42228
step:1943000 source:(1, 22) loss:6.4876
step:1943500 source:(1, 53) loss:6.07815
step:1944000 source:(1, 66) loss:5.32683
step:1944500 source:(1, 48) loss:4.94365
step:1945000 source:(1, 23) loss:6.22255
step:1945500 source:(1, 52) loss:5.35792
step:1946000 source:(1, 40) loss:5.61613
step:1946500 source:(1, 66) loss:6.34894
step:1947000 source:(1, 66) loss:5.70432
step:1947500 source:(1, 66) loss:5.54945
step:1948000 source:(1, 49) loss:6.43493
step:1948500 source:(1, 27) loss:7.43735
step:1949000 source:(1, 24) loss:6.1179
step:1949500 source:(1, 66) loss:6.54623
step:1950000 source:(1, 58) loss:6.11204
epoch:16 eval_bleu:53.735536336898804
2018-07-23 13:38:13.364837: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 13:38:13.364913: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 13:38:13.365973: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 13:38:13.366016: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:1950500 source:(1, 38) loss:5.93142
step:1951000 source:(1, 29) loss:4.15242
step:1951500 source:(1, 32) loss:5.25281
step:1952000 source:(1, 66) loss:5.671
step:1952500 source:(1, 66) loss:6.184
step:1953000 source:(1, 65) loss:6.02248
step:1953500 source:(1, 66) loss:6.09015
step:1954000 source:(1, 66) loss:5.73986
step:1954500 source:(1, 66) loss:6.84994
step:1955000 source:(1, 28) loss:4.2441
step:1955500 source:(1, 61) loss:5.56181
step:1956000 source:(1, 66) loss:5.24273
step:1956500 source:(1, 66) loss:5.70596
step:1957000 source:(1, 61) loss:6.02018
step:1957500 source:(1, 22) loss:6.24951
step:1958000 source:(1, 47) loss:5.15738
step:1958500 source:(1, 66) loss:6.33903
step:1959000 source:(1, 66) loss:8.37445
step:1959500 source:(1, 41) loss:5.79311
step:1960000 source:(1, 66) loss:4.95854
step:1960500 source:(1, 41) loss:7.95652
step:1961000 source:(1, 66) loss:4.8916
step:1961500 source:(1, 66) loss:6.0614
step:1962000 source:(1, 66) loss:4.94741
step:1962500 source:(1, 66) loss:4.59794
step:1963000 source:(1, 38) loss:7.13759
step:1963500 source:(1, 66) loss:5.31675
step:1964000 source:(1, 66) loss:6.62911
step:1964500 source:(1, 56) loss:6.52699
step:1965000 source:(1, 43) loss:6.20678
step:1965500 source:(1, 66) loss:4.62844
step:1966000 source:(1, 41) loss:4.45787
step:1966500 source:(1, 66) loss:6.64853
step:1967000 source:(1, 66) loss:7.31338
step:1967500 source:(1, 66) loss:5.30212
step:1968000 source:(1, 66) loss:5.66085
step:1968500 source:(1, 66) loss:4.86259
step:1969000 source:(1, 66) loss:5.32693
step:1969500 source:(1, 66) loss:5.80339
step:1970000 source:(1, 66) loss:5.62792
step:1970500 source:(1, 42) loss:7.58166
step:1971000 source:(1, 66) loss:5.63368
step:1971500 source:(1, 66) loss:4.00501
step:1972000 source:(1, 66) loss:6.72782
step:1972500 source:(1, 24) loss:6.89058
step:1973000 source:(1, 66) loss:5.73751
step:1973500 source:(1, 42) loss:7.59439
step:1974000 source:(1, 66) loss:6.07343
step:1974500 source:(1, 66) loss:4.93233
step:1975000 source:(1, 27) loss:4.92968
step:1975500 source:(1, 66) loss:6.08268
step:1976000 source:(1, 32) loss:4.99085
step:1976500 source:(1, 49) loss:5.64517
step:1977000 source:(1, 66) loss:6.51639
step:1977500 source:(1, 56) loss:5.57895
step:1978000 source:(1, 66) loss:5.13011
step:1978500 source:(1, 66) loss:6.78208
step:1979000 source:(1, 66) loss:6.10013
step:1979500 source:(1, 66) loss:6.90204
step:1980000 source:(1, 66) loss:5.46076
step:1980500 source:(1, 46) loss:5.43201
step:1981000 source:(1, 66) loss:6.64497
step:1981500 source:(1, 66) loss:5.44363
step:1982000 source:(1, 50) loss:5.92023
step:1982500 source:(1, 44) loss:5.63284
step:1983000 source:(1, 30) loss:7.15705
step:1983500 source:(1, 66) loss:5.7119
step:1984000 source:(1, 66) loss:5.82876
step:1984500 source:(1, 66) loss:5.39635
step:1985000 source:(1, 66) loss:5.60519
step:1985500 source:(1, 23) loss:4.70209
step:1986000 source:(1, 66) loss:6.35647
step:1986500 source:(1, 34) loss:5.52888
step:1987000 source:(1, 66) loss:5.55003
step:1987500 source:(1, 66) loss:5.64801
step:1988000 source:(1, 66) loss:6.35243
step:1988500 source:(1, 46) loss:5.81918
step:1989000 source:(1, 66) loss:6.60424
step:1989500 source:(1, 37) loss:5.84065
step:1990000 source:(1, 66) loss:5.54642
step:1990500 source:(1, 36) loss:5.0908
step:1991000 source:(1, 33) loss:4.99306
step:1991500 source:(1, 55) loss:4.18117
step:1992000 source:(1, 59) loss:6.81598
step:1992500 source:(1, 28) loss:6.18973
step:1993000 source:(1, 33) loss:6.15626
step:1993500 source:(1, 32) loss:5.73027
step:1994000 source:(1, 66) loss:5.93938
step:1994500 source:(1, 53) loss:5.18748
step:1995000 source:(1, 66) loss:4.39687
step:1995500 source:(1, 66) loss:5.17522
step:1996000 source:(1, 66) loss:5.39164
step:1996500 source:(1, 66) loss:6.76031
step:1997000 source:(1, 66) loss:6.42262
step:1997500 source:(1, 26) loss:6.39491
step:1998000 source:(1, 66) loss:5.43369
step:1998500 source:(1, 66) loss:6.19893
step:1999000 source:(1, 66) loss:5.99613
step:1999500 source:(1, 47) loss:6.53841
step:2000000 source:(1, 46) loss:6.61289
step:2000500 source:(1, 66) loss:6.21531
step:2001000 source:(1, 66) loss:5.2405
step:2001500 source:(1, 63) loss:7.03227
step:2002000 source:(1, 31) loss:7.82834
step:2002500 source:(1, 33) loss:4.88487
step:2003000 source:(1, 53) loss:4.2881
step:2003500 source:(1, 66) loss:6.24891
step:2004000 source:(1, 31) loss:7.48709
step:2004500 source:(1, 66) loss:5.69521
step:2005000 source:(1, 31) loss:5.12782
step:2005500 source:(1, 66) loss:6.64132
step:2006000 source:(1, 66) loss:6.26887
step:2006500 source:(1, 66) loss:5.16241
step:2007000 source:(1, 66) loss:5.82033
step:2007500 source:(1, 57) loss:5.37116
step:2008000 source:(1, 24) loss:5.11087
step:2008500 source:(1, 59) loss:7.23318
step:2009000 source:(1, 39) loss:5.03583
step:2009500 source:(1, 35) loss:6.14222
step:2010000 source:(1, 66) loss:6.3785
step:2010500 source:(1, 43) loss:5.90431
step:2011000 source:(1, 54) loss:5.60484
step:2011500 source:(1, 41) loss:6.35959
step:2012000 source:(1, 66) loss:5.51567
step:2012500 source:(1, 66) loss:5.62871
step:2013000 source:(1, 66) loss:6.58956
step:2013500 source:(1, 44) loss:5.32864
step:2014000 source:(1, 66) loss:6.96731
step:2014500 source:(1, 66) loss:5.44766
step:2015000 source:(1, 66) loss:5.78783
step:2015500 source:(1, 51) loss:6.09635
step:2016000 source:(1, 57) loss:4.60681
step:2016500 source:(1, 66) loss:6.58251
step:2017000 source:(1, 34) loss:7.36302
step:2017500 source:(1, 66) loss:7.16475
step:2018000 source:(1, 66) loss:4.20597
step:2018500 source:(1, 35) loss:6.30332
step:2019000 source:(1, 66) loss:6.53204
step:2019500 source:(1, 35) loss:6.5321
step:2020000 source:(1, 66) loss:6.96524
step:2020500 source:(1, 66) loss:6.63248
step:2021000 source:(1, 36) loss:6.06982
step:2021500 source:(1, 66) loss:6.2347
step:2022000 source:(1, 48) loss:5.49396
step:2022500 source:(1, 51) loss:4.00596
step:2023000 source:(1, 66) loss:6.64595
step:2023500 source:(1, 66) loss:6.74541
step:2024000 source:(1, 66) loss:8.11253
step:2024500 source:(1, 66) loss:5.86975
step:2025000 source:(1, 55) loss:5.77599
step:2025500 source:(1, 33) loss:6.01654
step:2026000 source:(1, 66) loss:6.66314
step:2026500 source:(1, 35) loss:5.4739
step:2027000 source:(1, 25) loss:5.20008
step:2027500 source:(1, 30) loss:5.89691
step:2028000 source:(1, 66) loss:4.96322
step:2028500 source:(1, 41) loss:5.46876
step:2029000 source:(1, 59) loss:5.32565
step:2029500 source:(1, 64) loss:5.92153
step:2030000 source:(1, 43) loss:6.25179
step:2030500 source:(1, 66) loss:5.0708
step:2031000 source:(1, 66) loss:7.38965
step:2031500 source:(1, 66) loss:4.75809
step:2032000 source:(1, 39) loss:7.3507
step:2032500 source:(1, 66) loss:5.2616
step:2033000 source:(1, 52) loss:6.93102
step:2033500 source:(1, 66) loss:6.92336
step:2034000 source:(1, 66) loss:7.78814
step:2034500 source:(1, 66) loss:6.49036
step:2035000 source:(1, 66) loss:6.675
step:2035500 source:(1, 62) loss:4.58245
step:2036000 source:(1, 66) loss:5.68338
step:2036500 source:(1, 66) loss:6.12911
step:2037000 source:(1, 38) loss:6.88654
step:2037500 source:(1, 66) loss:5.97684
step:2038000 source:(1, 30) loss:6.67178
step:2038500 source:(1, 66) loss:6.70849
step:2039000 source:(1, 33) loss:5.82759
step:2039500 source:(1, 66) loss:7.15715
step:2040000 source:(1, 30) loss:5.48892
step:2040500 source:(1, 51) loss:6.33016
step:2041000 source:(1, 66) loss:4.18917
step:2041500 source:(1, 25) loss:4.61486
step:2042000 source:(1, 39) loss:6.66043
step:2042500 source:(1, 59) loss:7.14664
step:2043000 source:(1, 22) loss:4.56683
step:2043500 source:(1, 53) loss:6.36397
step:2044000 source:(1, 66) loss:4.80957
step:2044500 source:(1, 48) loss:4.7519
step:2045000 source:(1, 23) loss:6.29685
step:2045500 source:(1, 52) loss:6.22579
step:2046000 source:(1, 40) loss:5.49101
step:2046500 source:(1, 66) loss:6.15925
step:2047000 source:(1, 66) loss:6.17595
step:2047500 source:(1, 66) loss:6.03433
step:2048000 source:(1, 49) loss:5.82084
step:2048500 source:(1, 27) loss:6.34117
step:2049000 source:(1, 24) loss:5.94102
step:2049500 source:(1, 66) loss:6.45053
step:2050000 source:(1, 58) loss:6.31831
epoch:17 eval_bleu:48.17664623260498
2018-07-23 16:39:14.987825: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 16:39:14.987844: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 16:39:14.987854: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 16:39:14.987826: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 16:39:14.988780: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 16:39:14.988830: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 16:54:25.080321: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 16:54:25.080395: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 16:54:25.080439: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:2050500 source:(1, 38) loss:4.4299
step:2051000 source:(1, 29) loss:4.37607
step:2051500 source:(1, 32) loss:5.66893
step:2052000 source:(1, 66) loss:4.26228
step:2052500 source:(1, 66) loss:5.91939
step:2053000 source:(1, 65) loss:5.66417
step:2053500 source:(1, 66) loss:7.0784
step:2054000 source:(1, 66) loss:7.21878
step:2054500 source:(1, 66) loss:6.49712
step:2055000 source:(1, 28) loss:4.63957
step:2055500 source:(1, 61) loss:6.18675
step:2056000 source:(1, 66) loss:6.14757
step:2056500 source:(1, 66) loss:5.57977
step:2057000 source:(1, 61) loss:5.55638
step:2057500 source:(1, 22) loss:6.30479
step:2058000 source:(1, 47) loss:5.87368
step:2058500 source:(1, 66) loss:5.51601
step:2059000 source:(1, 66) loss:7.77252
step:2059500 source:(1, 41) loss:6.31007
step:2060000 source:(1, 66) loss:5.17232
step:2060500 source:(1, 41) loss:6.80554
step:2061000 source:(1, 66) loss:6.6875
step:2061500 source:(1, 66) loss:6.12076
step:2062000 source:(1, 66) loss:5.39865
step:2062500 source:(1, 66) loss:5.43528
step:2063000 source:(1, 38) loss:5.68713
step:2063500 source:(1, 66) loss:6.23042
step:2064000 source:(1, 66) loss:6.41203
step:2064500 source:(1, 56) loss:5.78162
step:2065000 source:(1, 43) loss:5.71797
step:2065500 source:(1, 66) loss:5.2335
step:2066000 source:(1, 41) loss:4.50179
step:2066500 source:(1, 66) loss:7.38122
step:2067000 source:(1, 66) loss:5.34042
step:2067500 source:(1, 66) loss:6.01806
step:2068000 source:(1, 66) loss:5.90701
step:2068500 source:(1, 66) loss:6.63937
step:2069000 source:(1, 66) loss:6.83316
step:2069500 source:(1, 66) loss:5.6341
step:2070000 source:(1, 66) loss:7.27678
step:2070500 source:(1, 42) loss:6.47671
step:2071000 source:(1, 66) loss:6.56401
step:2071500 source:(1, 66) loss:6.41659
step:2072000 source:(1, 66) loss:6.03536
step:2072500 source:(1, 24) loss:6.12961
step:2073000 source:(1, 66) loss:6.07177
step:2073500 source:(1, 42) loss:7.21907
step:2074000 source:(1, 66) loss:5.62582
step:2074500 source:(1, 66) loss:6.93525
step:2075000 source:(1, 27) loss:5.86798
step:2075500 source:(1, 66) loss:6.43222
step:2076000 source:(1, 32) loss:5.92421
step:2076500 source:(1, 49) loss:5.83612
step:2077000 source:(1, 66) loss:5.75814
step:2077500 source:(1, 56) loss:6.51697
step:2078000 source:(1, 66) loss:6.23154
step:2078500 source:(1, 66) loss:7.85142
step:2079000 source:(1, 66) loss:6.37037
step:2079500 source:(1, 66) loss:6.06806
step:2080000 source:(1, 66) loss:6.39414
step:2080500 source:(1, 46) loss:6.64187
step:2081000 source:(1, 66) loss:7.08273
step:2081500 source:(1, 66) loss:6.14844
step:2082000 source:(1, 50) loss:7.03263
step:2082500 source:(1, 44) loss:6.35816
step:2083000 source:(1, 30) loss:7.67491
step:2083500 source:(1, 66) loss:6.25164
step:2084000 source:(1, 66) loss:6.23463
step:2084500 source:(1, 66) loss:5.12387
step:2085000 source:(1, 66) loss:8.24607
step:2085500 source:(1, 23) loss:4.31943
step:2086000 source:(1, 66) loss:6.16324
step:2086500 source:(1, 34) loss:5.65686
step:2087000 source:(1, 66) loss:5.62129
step:2087500 source:(1, 66) loss:5.7301
step:2088000 source:(1, 66) loss:7.01956
step:2088500 source:(1, 46) loss:4.96996
step:2089000 source:(1, 66) loss:5.61229
step:2089500 source:(1, 37) loss:5.97862
step:2090000 source:(1, 66) loss:4.82664
step:2090500 source:(1, 36) loss:5.15146
step:2091000 source:(1, 33) loss:5.22157
step:2091500 source:(1, 55) loss:4.45341
step:2092000 source:(1, 59) loss:6.60195
step:2092500 source:(1, 28) loss:5.1034
step:2093000 source:(1, 33) loss:6.05338
step:2093500 source:(1, 32) loss:6.4154
step:2094000 source:(1, 66) loss:5.55347
step:2094500 source:(1, 53) loss:5.4497
step:2095000 source:(1, 66) loss:5.33267
step:2095500 source:(1, 66) loss:5.48444
step:2096000 source:(1, 66) loss:5.48285
step:2096500 source:(1, 66) loss:5.90619
step:2097000 source:(1, 66) loss:6.13547
step:2097500 source:(1, 26) loss:6.69741
step:2098000 source:(1, 66) loss:5.89513
step:2098500 source:(1, 66) loss:6.21876
step:2099000 source:(1, 66) loss:7.27279
step:2099500 source:(1, 47) loss:5.46015
step:2100000 source:(1, 46) loss:6.3092
step:2100500 source:(1, 66) loss:5.50033
step:2101000 source:(1, 66) loss:6.24281
step:2101500 source:(1, 63) loss:6.91167
step:2102000 source:(1, 31) loss:6.59075
step:2102500 source:(1, 33) loss:5.05668
step:2103000 source:(1, 53) loss:5.98213
step:2103500 source:(1, 66) loss:5.70775
step:2104000 source:(1, 31) loss:7.23532
step:2104500 source:(1, 66) loss:6.21304
step:2105000 source:(1, 31) loss:5.1148
step:2105500 source:(1, 66) loss:7.95093
step:2106000 source:(1, 66) loss:5.54246
step:2106500 source:(1, 66) loss:6.07154
step:2107000 source:(1, 66) loss:6.47637
step:2107500 source:(1, 57) loss:6.06776
step:2108000 source:(1, 24) loss:5.17762
step:2108500 source:(1, 59) loss:6.43919
step:2109000 source:(1, 39) loss:5.0321
step:2109500 source:(1, 35) loss:7.36753
step:2110000 source:(1, 66) loss:5.17343
step:2110500 source:(1, 43) loss:6.1318
step:2111000 source:(1, 54) loss:6.20406
step:2111500 source:(1, 41) loss:5.88825
step:2112000 source:(1, 66) loss:6.48433
step:2112500 source:(1, 66) loss:6.85542
step:2113000 source:(1, 66) loss:6.78349
step:2113500 source:(1, 44) loss:6.68835
step:2114000 source:(1, 66) loss:5.98237
step:2114500 source:(1, 66) loss:5.15539
step:2115000 source:(1, 66) loss:4.83677
step:2115500 source:(1, 51) loss:5.17764
step:2116000 source:(1, 57) loss:5.13711
step:2116500 source:(1, 66) loss:6.03125
step:2117000 source:(1, 34) loss:6.19749
step:2117500 source:(1, 66) loss:5.93007
step:2118000 source:(1, 66) loss:5.14907
step:2118500 source:(1, 35) loss:6.52949
step:2119000 source:(1, 66) loss:7.84033
step:2119500 source:(1, 35) loss:6.0025
step:2120000 source:(1, 66) loss:6.06775
step:2120500 source:(1, 66) loss:5.31587
step:2121000 source:(1, 36) loss:5.8113
step:2121500 source:(1, 66) loss:4.42241
step:2122000 source:(1, 48) loss:5.67066
step:2122500 source:(1, 51) loss:7.34376
step:2123000 source:(1, 66) loss:5.62531
step:2123500 source:(1, 66) loss:7.07521
step:2124000 source:(1, 66) loss:6.81624
step:2124500 source:(1, 66) loss:5.98895
step:2125000 source:(1, 55) loss:5.4912
step:2125500 source:(1, 33) loss:6.23887
step:2126000 source:(1, 66) loss:5.2859
step:2126500 source:(1, 35) loss:6.14249
step:2127000 source:(1, 25) loss:4.96415
step:2127500 source:(1, 30) loss:5.56511
step:2128000 source:(1, 66) loss:4.8006
step:2128500 source:(1, 41) loss:6.57134
step:2129000 source:(1, 59) loss:6.18956
step:2129500 source:(1, 64) loss:3.89995
step:2130000 source:(1, 43) loss:5.23077
step:2130500 source:(1, 66) loss:6.30162
step:2131000 source:(1, 66) loss:6.30394
step:2131500 source:(1, 66) loss:5.15037
step:2132000 source:(1, 39) loss:7.10517
step:2132500 source:(1, 66) loss:5.1896
step:2133000 source:(1, 52) loss:5.74239
step:2133500 source:(1, 66) loss:6.79396
step:2134000 source:(1, 66) loss:6.7606
step:2134500 source:(1, 66) loss:6.587
step:2135000 source:(1, 66) loss:7.14364
step:2135500 source:(1, 62) loss:5.82223
step:2136000 source:(1, 66) loss:5.94807
step:2136500 source:(1, 66) loss:5.24693
step:2137000 source:(1, 38) loss:6.23712
step:2137500 source:(1, 66) loss:5.48397
step:2138000 source:(1, 30) loss:6.22337
step:2138500 source:(1, 66) loss:5.68326
step:2139000 source:(1, 33) loss:5.7033
step:2139500 source:(1, 66) loss:5.92895
step:2140000 source:(1, 30) loss:5.48574
step:2140500 source:(1, 51) loss:6.70025
step:2141000 source:(1, 66) loss:5.00629
step:2141500 source:(1, 25) loss:4.85678
step:2142000 source:(1, 39) loss:5.29944
step:2142500 source:(1, 59) loss:6.68941
step:2143000 source:(1, 22) loss:5.49381
step:2143500 source:(1, 53) loss:7.45164
step:2144000 source:(1, 66) loss:5.61062
step:2144500 source:(1, 48) loss:5.70376
step:2145000 source:(1, 23) loss:6.61489
step:2145500 source:(1, 52) loss:6.14039
step:2146000 source:(1, 40) loss:5.32396
step:2146500 source:(1, 66) loss:5.31532
step:2147000 source:(1, 66) loss:5.24912
step:2147500 source:(1, 66) loss:6.42878
step:2148000 source:(1, 49) loss:6.36767
step:2148500 source:(1, 27) loss:6.3943
step:2149000 source:(1, 24) loss:6.29631
step:2149500 source:(1, 66) loss:6.82553
step:2150000 source:(1, 58) loss:6.94543
epoch:18 eval_bleu:45.00493109226227
2018-07-23 19:41:19.566415: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 19:41:19.566477: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 19:56:39.744887: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 19:56:39.744964: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 19:56:39.745001: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:2150500 source:(1, 38) loss:5.29387
step:2151000 source:(1, 29) loss:3.95164
step:2151500 source:(1, 32) loss:5.24603
step:2152000 source:(1, 66) loss:5.6457
step:2152500 source:(1, 66) loss:6.11061
step:2153000 source:(1, 65) loss:6.63844
step:2153500 source:(1, 66) loss:5.75807
step:2154000 source:(1, 66) loss:6.32847
step:2154500 source:(1, 66) loss:7.53754
step:2155000 source:(1, 28) loss:6.20826
step:2155500 source:(1, 61) loss:6.7525
step:2156000 source:(1, 66) loss:5.83103
step:2156500 source:(1, 66) loss:6.09295
step:2157000 source:(1, 61) loss:5.18746
step:2157500 source:(1, 22) loss:5.94821
step:2158000 source:(1, 47) loss:5.31279
step:2158500 source:(1, 66) loss:5.18081
step:2159000 source:(1, 66) loss:8.41239
step:2159500 source:(1, 41) loss:5.81015
step:2160000 source:(1, 66) loss:6.66623
step:2160500 source:(1, 41) loss:6.69158
step:2161000 source:(1, 66) loss:5.20766
step:2161500 source:(1, 66) loss:5.20854
step:2162000 source:(1, 66) loss:5.34213
step:2162500 source:(1, 66) loss:6.7
step:2163000 source:(1, 38) loss:6.34966
step:2163500 source:(1, 66) loss:5.63833
step:2164000 source:(1, 66) loss:5.99603
step:2164500 source:(1, 56) loss:6.25709
step:2165000 source:(1, 43) loss:5.74849
step:2165500 source:(1, 66) loss:4.54409
step:2166000 source:(1, 41) loss:4.98854
step:2166500 source:(1, 66) loss:7.60146
step:2167000 source:(1, 66) loss:5.49077
step:2167500 source:(1, 66) loss:5.68303
step:2168000 source:(1, 66) loss:5.837
step:2168500 source:(1, 66) loss:6.49918
step:2169000 source:(1, 66) loss:6.70393
step:2169500 source:(1, 66) loss:4.76888
step:2170000 source:(1, 66) loss:6.0938
step:2170500 source:(1, 42) loss:7.27798
step:2171000 source:(1, 66) loss:6.86862
step:2171500 source:(1, 66) loss:6.11711
step:2172000 source:(1, 66) loss:6.5596
step:2172500 source:(1, 24) loss:5.88501
step:2173000 source:(1, 66) loss:6.83115
step:2173500 source:(1, 42) loss:7.41231
step:2174000 source:(1, 66) loss:7.53442
step:2174500 source:(1, 66) loss:7.08993
step:2175000 source:(1, 27) loss:5.78539
step:2175500 source:(1, 66) loss:6.90389
step:2176000 source:(1, 32) loss:6.02077
step:2176500 source:(1, 49) loss:5.51179
step:2177000 source:(1, 66) loss:4.2237
step:2177500 source:(1, 56) loss:6.3846
step:2178000 source:(1, 66) loss:5.83584
step:2178500 source:(1, 66) loss:6.49095
step:2179000 source:(1, 66) loss:7.08429
step:2179500 source:(1, 66) loss:5.84006
step:2180000 source:(1, 66) loss:6.3792
step:2180500 source:(1, 46) loss:5.25629
step:2181000 source:(1, 66) loss:5.82022
step:2181500 source:(1, 66) loss:6.54155
step:2182000 source:(1, 50) loss:7.67858
step:2182500 source:(1, 44) loss:5.68876
step:2183000 source:(1, 30) loss:8.50802
step:2183500 source:(1, 66) loss:6.478
step:2184000 source:(1, 66) loss:6.41978
step:2184500 source:(1, 66) loss:6.0457
step:2185000 source:(1, 66) loss:6.68298
step:2185500 source:(1, 23) loss:4.27095
step:2186000 source:(1, 66) loss:5.59035
step:2186500 source:(1, 34) loss:6.21993
step:2187000 source:(1, 66) loss:5.60419
step:2187500 source:(1, 66) loss:5.83557
step:2188000 source:(1, 66) loss:6.62071
step:2188500 source:(1, 46) loss:6.81327
step:2189000 source:(1, 66) loss:6.2009
step:2189500 source:(1, 37) loss:6.45121
step:2190000 source:(1, 66) loss:5.24524
step:2190500 source:(1, 36) loss:5.34242
step:2191000 source:(1, 33) loss:4.90485
step:2191500 source:(1, 55) loss:4.56213
step:2192000 source:(1, 59) loss:7.22205
step:2192500 source:(1, 28) loss:5.88926
step:2193000 source:(1, 33) loss:5.86194
step:2193500 source:(1, 32) loss:5.13226
step:2194000 source:(1, 66) loss:5.14046
step:2194500 source:(1, 53) loss:4.91413
step:2195000 source:(1, 66) loss:4.43359
step:2195500 source:(1, 66) loss:7.08801
step:2196000 source:(1, 66) loss:6.83493
step:2196500 source:(1, 66) loss:6.22987
step:2197000 source:(1, 66) loss:4.82479
step:2197500 source:(1, 26) loss:5.12967
step:2198000 source:(1, 66) loss:5.74031
step:2198500 source:(1, 66) loss:7.07669
step:2199000 source:(1, 66) loss:6.5262
step:2199500 source:(1, 47) loss:5.63598
step:2200000 source:(1, 46) loss:6.30735
step:2200500 source:(1, 66) loss:5.77343
step:2201000 source:(1, 66) loss:6.13058
step:2201500 source:(1, 63) loss:6.04796
step:2202000 source:(1, 31) loss:7.00245
step:2202500 source:(1, 33) loss:6.2392
step:2203000 source:(1, 53) loss:4.87765
step:2203500 source:(1, 66) loss:7.46069
step:2204000 source:(1, 31) loss:6.53217
step:2204500 source:(1, 66) loss:5.79537
step:2205000 source:(1, 31) loss:6.75002
step:2205500 source:(1, 66) loss:7.1512
step:2206000 source:(1, 66) loss:6.87442
step:2206500 source:(1, 66) loss:5.58748
step:2207000 source:(1, 66) loss:5.42487
step:2207500 source:(1, 57) loss:5.50198
step:2208000 source:(1, 24) loss:5.24874
step:2208500 source:(1, 59) loss:6.6218
step:2209000 source:(1, 39) loss:5.5086
step:2209500 source:(1, 35) loss:8.48457
step:2210000 source:(1, 66) loss:4.606
step:2210500 source:(1, 43) loss:4.67357
step:2211000 source:(1, 54) loss:4.98356
step:2211500 source:(1, 41) loss:5.1943
step:2212000 source:(1, 66) loss:6.09079
step:2212500 source:(1, 66) loss:6.96603
step:2213000 source:(1, 66) loss:7.14427
step:2213500 source:(1, 44) loss:7.20799
step:2214000 source:(1, 66) loss:5.70863
step:2214500 source:(1, 66) loss:4.57989
step:2215000 source:(1, 66) loss:5.72767
step:2215500 source:(1, 51) loss:5.55568
step:2216000 source:(1, 57) loss:6.11749
step:2216500 source:(1, 66) loss:4.97111
step:2217000 source:(1, 34) loss:6.79136
step:2217500 source:(1, 66) loss:6.27594
step:2218000 source:(1, 66) loss:5.07939
step:2218500 source:(1, 35) loss:6.428
step:2219000 source:(1, 66) loss:7.1695
step:2219500 source:(1, 35) loss:6.25249
step:2220000 source:(1, 66) loss:7.58462
step:2220500 source:(1, 66) loss:4.17865
step:2221000 source:(1, 36) loss:5.85902
step:2221500 source:(1, 66) loss:6.03504
step:2222000 source:(1, 48) loss:7.18812
step:2222500 source:(1, 51) loss:3.68797
step:2223000 source:(1, 66) loss:5.77909
step:2223500 source:(1, 66) loss:5.79594
step:2224000 source:(1, 66) loss:7.0505
step:2224500 source:(1, 66) loss:6.95352
step:2225000 source:(1, 55) loss:6.00275
step:2225500 source:(1, 33) loss:5.84007
step:2226000 source:(1, 66) loss:5.29136
step:2226500 source:(1, 35) loss:6.11703
step:2227000 source:(1, 25) loss:5.13413
step:2227500 source:(1, 30) loss:6.21028
step:2228000 source:(1, 66) loss:4.00804
step:2228500 source:(1, 41) loss:5.83196
step:2229000 source:(1, 59) loss:7.15734
step:2229500 source:(1, 64) loss:5.7381
step:2230000 source:(1, 43) loss:6.4797
step:2230500 source:(1, 66) loss:5.40565
step:2231000 source:(1, 66) loss:5.79608
step:2231500 source:(1, 66) loss:5.52079
step:2232000 source:(1, 39) loss:8.17926
step:2232500 source:(1, 66) loss:6.0616
step:2233000 source:(1, 52) loss:6.81079
step:2233500 source:(1, 66) loss:6.55959
step:2234000 source:(1, 66) loss:6.14425
step:2234500 source:(1, 66) loss:7.23493
step:2235000 source:(1, 66) loss:4.96267
step:2235500 source:(1, 62) loss:5.28785
step:2236000 source:(1, 66) loss:5.82823
step:2236500 source:(1, 66) loss:5.28793
step:2237000 source:(1, 38) loss:6.44744
step:2237500 source:(1, 66) loss:5.31755
step:2238000 source:(1, 30) loss:6.29529
step:2238500 source:(1, 66) loss:6.87279
step:2239000 source:(1, 33) loss:6.02322
step:2239500 source:(1, 66) loss:5.88405
step:2240000 source:(1, 30) loss:7.44802
step:2240500 source:(1, 51) loss:6.42078
step:2241000 source:(1, 66) loss:5.8814
step:2241500 source:(1, 25) loss:6.14242
step:2242000 source:(1, 39) loss:5.98048
step:2242500 source:(1, 59) loss:6.59586
step:2243000 source:(1, 22) loss:6.89035
step:2243500 source:(1, 53) loss:7.11083
step:2244000 source:(1, 66) loss:5.07197
step:2244500 source:(1, 48) loss:6.29179
step:2245000 source:(1, 23) loss:6.11668
step:2245500 source:(1, 52) loss:7.46692
step:2246000 source:(1, 40) loss:5.90372
step:2246500 source:(1, 66) loss:7.25689
step:2247000 source:(1, 66) loss:5.59208
step:2247500 source:(1, 66) loss:6.85007
step:2248000 source:(1, 49) loss:6.21618
step:2248500 source:(1, 27) loss:6.72545
step:2249000 source:(1, 24) loss:6.10079
step:2249500 source:(1, 66) loss:6.82286
step:2250000 source:(1, 58) loss:5.07655
epoch:19 eval_bleu:47.923487424850464
2018-07-23 22:45:00.912581: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 22:45:00.912646: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 23:00:26.598564: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 23:00:26.598653: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-23 23:00:26.598690: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:2250500 source:(1, 38) loss:5.81674
step:2251000 source:(1, 29) loss:4.67557
step:2251500 source:(1, 32) loss:5.98061
step:2252000 source:(1, 66) loss:6.0995
step:2252500 source:(1, 66) loss:7.46571
step:2253000 source:(1, 65) loss:4.99063
step:2253500 source:(1, 66) loss:5.54679
step:2254000 source:(1, 66) loss:4.81063
step:2254500 source:(1, 66) loss:6.75561
step:2255000 source:(1, 28) loss:5.39234
step:2255500 source:(1, 61) loss:6.72093
step:2256000 source:(1, 66) loss:6.05029
step:2256500 source:(1, 66) loss:5.42192
step:2257000 source:(1, 61) loss:5.06042
step:2257500 source:(1, 22) loss:6.34874
step:2258000 source:(1, 47) loss:5.04964
step:2258500 source:(1, 66) loss:6.19607
step:2259000 source:(1, 66) loss:6.37791
step:2259500 source:(1, 41) loss:5.60558
step:2260000 source:(1, 66) loss:6.23356
step:2260500 source:(1, 41) loss:6.01548
step:2261000 source:(1, 66) loss:5.20527
step:2261500 source:(1, 66) loss:5.53751
step:2262000 source:(1, 66) loss:4.47942
step:2262500 source:(1, 66) loss:6.14248
step:2263000 source:(1, 38) loss:5.40002
step:2263500 source:(1, 66) loss:5.78014
step:2264000 source:(1, 66) loss:5.85797
step:2264500 source:(1, 56) loss:6.16131
step:2265000 source:(1, 43) loss:5.76502
step:2265500 source:(1, 66) loss:5.63745
step:2266000 source:(1, 41) loss:4.45004
step:2266500 source:(1, 66) loss:7.0266
step:2267000 source:(1, 66) loss:6.15173
step:2267500 source:(1, 66) loss:5.08555
step:2268000 source:(1, 66) loss:4.93478
step:2268500 source:(1, 66) loss:6.83965
step:2269000 source:(1, 66) loss:6.40772
step:2269500 source:(1, 66) loss:5.82277
step:2270000 source:(1, 66) loss:5.13752
step:2270500 source:(1, 42) loss:6.6377
step:2271000 source:(1, 66) loss:6.81599
step:2271500 source:(1, 66) loss:5.99187
step:2272000 source:(1, 66) loss:6.49824
step:2272500 source:(1, 24) loss:5.76962
step:2273000 source:(1, 66) loss:5.97214
step:2273500 source:(1, 42) loss:7.06484
step:2274000 source:(1, 66) loss:6.29123
step:2274500 source:(1, 66) loss:6.06334
step:2275000 source:(1, 27) loss:6.33932
step:2275500 source:(1, 66) loss:7.2049
step:2276000 source:(1, 32) loss:5.24697
step:2276500 source:(1, 49) loss:5.1498
step:2277000 source:(1, 66) loss:5.46527
step:2277500 source:(1, 56) loss:6.24337
step:2278000 source:(1, 66) loss:6.43064
step:2278500 source:(1, 66) loss:6.71863
step:2279000 source:(1, 66) loss:6.55866
step:2279500 source:(1, 66) loss:5.83247
step:2280000 source:(1, 66) loss:5.78185
step:2280500 source:(1, 46) loss:7.32517
step:2281000 source:(1, 66) loss:5.85266
step:2281500 source:(1, 66) loss:6.65898
step:2282000 source:(1, 50) loss:6.73698
step:2282500 source:(1, 44) loss:5.59249
step:2283000 source:(1, 30) loss:7.96311
step:2283500 source:(1, 66) loss:6.17827
step:2284000 source:(1, 66) loss:6.37774
step:2284500 source:(1, 66) loss:4.68006
step:2285000 source:(1, 66) loss:4.78054
step:2285500 source:(1, 23) loss:4.46206
step:2286000 source:(1, 66) loss:5.80694
step:2286500 source:(1, 34) loss:5.96014
step:2287000 source:(1, 66) loss:6.19241
step:2287500 source:(1, 66) loss:6.11472
step:2288000 source:(1, 66) loss:6.76717
step:2288500 source:(1, 46) loss:4.35141
step:2289000 source:(1, 66) loss:7.18677
step:2289500 source:(1, 37) loss:7.02342
step:2290000 source:(1, 66) loss:5.28881
step:2290500 source:(1, 36) loss:5.08133
step:2291000 source:(1, 33) loss:5.94216
step:2291500 source:(1, 55) loss:4.54379
step:2292000 source:(1, 59) loss:7.08378
step:2292500 source:(1, 28) loss:5.2938
step:2293000 source:(1, 33) loss:5.95861
step:2293500 source:(1, 32) loss:5.97748
step:2294000 source:(1, 66) loss:4.54036
step:2294500 source:(1, 53) loss:5.04161
step:2295000 source:(1, 66) loss:4.63873
step:2295500 source:(1, 66) loss:6.80528
step:2296000 source:(1, 66) loss:4.35505
step:2296500 source:(1, 66) loss:5.95373
step:2297000 source:(1, 66) loss:6.63183
step:2297500 source:(1, 26) loss:5.18702
step:2298000 source:(1, 66) loss:5.67229
step:2298500 source:(1, 66) loss:7.1599
step:2299000 source:(1, 66) loss:7.53272
step:2299500 source:(1, 47) loss:5.66075
step:2300000 source:(1, 46) loss:6.23115
step:2300500 source:(1, 66) loss:5.64006
step:2301000 source:(1, 66) loss:4.25157
step:2301500 source:(1, 63) loss:6.20412
step:2302000 source:(1, 31) loss:6.66098
step:2302500 source:(1, 33) loss:5.6417
step:2303000 source:(1, 53) loss:5.81148
step:2303500 source:(1, 66) loss:4.88605
step:2304000 source:(1, 31) loss:6.13249
step:2304500 source:(1, 66) loss:5.90074
step:2305000 source:(1, 31) loss:5.09346
step:2305500 source:(1, 66) loss:8.1827
step:2306000 source:(1, 66) loss:7.19434
step:2306500 source:(1, 66) loss:5.59341
step:2307000 source:(1, 66) loss:5.43789
step:2307500 source:(1, 57) loss:6.90823
step:2308000 source:(1, 24) loss:4.94044
step:2308500 source:(1, 59) loss:7.75976
step:2309000 source:(1, 39) loss:6.0688
step:2309500 source:(1, 35) loss:8.29165
step:2310000 source:(1, 66) loss:5.34642
step:2310500 source:(1, 43) loss:5.42427
step:2311000 source:(1, 54) loss:5.11606
step:2311500 source:(1, 41) loss:5.01002
step:2312000 source:(1, 66) loss:4.5708
step:2312500 source:(1, 66) loss:5.62076
step:2313000 source:(1, 66) loss:6.54066
step:2313500 source:(1, 44) loss:6.41868
step:2314000 source:(1, 66) loss:6.40122
step:2314500 source:(1, 66) loss:4.65929
step:2315000 source:(1, 66) loss:7.13633
step:2315500 source:(1, 51) loss:4.8082
step:2316000 source:(1, 57) loss:4.72255
step:2316500 source:(1, 66) loss:6.02633
step:2317000 source:(1, 34) loss:6.31687
step:2317500 source:(1, 66) loss:6.95709
step:2318000 source:(1, 66) loss:5.27507
step:2318500 source:(1, 35) loss:4.9042
step:2319000 source:(1, 66) loss:7.56993
step:2319500 source:(1, 35) loss:5.41314
step:2320000 source:(1, 66) loss:6.9694
step:2320500 source:(1, 66) loss:4.67286
step:2321000 source:(1, 36) loss:6.03892
step:2321500 source:(1, 66) loss:5.25928
step:2322000 source:(1, 48) loss:7.17427
step:2322500 source:(1, 51) loss:5.38709
step:2323000 source:(1, 66) loss:5.05036
step:2323500 source:(1, 66) loss:6.55745
step:2324000 source:(1, 66) loss:6.7542
step:2324500 source:(1, 66) loss:7.08224
step:2325000 source:(1, 55) loss:6.42193
step:2325500 source:(1, 33) loss:6.65011
step:2326000 source:(1, 66) loss:4.97845
step:2326500 source:(1, 35) loss:5.84504
step:2327000 source:(1, 25) loss:4.72484
step:2327500 source:(1, 30) loss:6.50737
step:2328000 source:(1, 66) loss:4.74482
step:2328500 source:(1, 41) loss:5.99342
step:2329000 source:(1, 59) loss:6.96112
step:2329500 source:(1, 64) loss:5.29857
step:2330000 source:(1, 43) loss:6.9093
step:2330500 source:(1, 66) loss:5.80408
step:2331000 source:(1, 66) loss:5.63132
step:2331500 source:(1, 66) loss:5.1688
step:2332000 source:(1, 39) loss:7.56991
step:2332500 source:(1, 66) loss:4.86718
step:2333000 source:(1, 52) loss:5.65056
step:2333500 source:(1, 66) loss:5.90263
step:2334000 source:(1, 66) loss:4.3066
step:2334500 source:(1, 66) loss:7.14336
step:2335000 source:(1, 66) loss:5.28705
step:2335500 source:(1, 62) loss:4.4831
step:2336000 source:(1, 66) loss:7.09149
step:2336500 source:(1, 66) loss:5.72228
step:2337000 source:(1, 38) loss:5.75129
step:2337500 source:(1, 66) loss:5.65632
step:2338000 source:(1, 30) loss:7.22319
step:2338500 source:(1, 66) loss:7.64172
step:2339000 source:(1, 33) loss:5.03172
step:2339500 source:(1, 66) loss:6.08529
step:2340000 source:(1, 30) loss:7.692
step:2340500 source:(1, 51) loss:5.98926
step:2341000 source:(1, 66) loss:4.83585
step:2341500 source:(1, 25) loss:4.70678
step:2342000 source:(1, 39) loss:5.80555
step:2342500 source:(1, 59) loss:7.2591
step:2343000 source:(1, 22) loss:6.69975
step:2343500 source:(1, 53) loss:7.08277
step:2344000 source:(1, 66) loss:6.10428
step:2344500 source:(1, 48) loss:5.5802
step:2345000 source:(1, 23) loss:6.41954
step:2345500 source:(1, 52) loss:5.45302
step:2346000 source:(1, 40) loss:5.62912
step:2346500 source:(1, 66) loss:6.78184
step:2347000 source:(1, 66) loss:7.46039
step:2347500 source:(1, 66) loss:5.49602
step:2348000 source:(1, 49) loss:5.6705
step:2348500 source:(1, 27) loss:6.25522
step:2349000 source:(1, 24) loss:6.23673
step:2349500 source:(1, 66) loss:5.69327
step:2350000 source:(1, 58) loss:5.40284
epoch:20 eval_bleu:45.94692289829254
2018-07-24 02:21:32.196005: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
2018-07-24 02:21:32.196083: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?], [?,?]], output_types=[DT_INT32, DT_STRING, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
step:2350500 source:(1, 38) loss:5.97835
step:2351000 source:(1, 29) loss:3.97833
step:2351500 source:(1, 32) loss:6.69127
step:2352000 source:(1, 66) loss:6.51529
step:2352500 source:(1, 66) loss:6.17187
step:2353000 source:(1, 65) loss:5.16593
step:2353500 source:(1, 66) loss:5.79867
step:2354000 source:(1, 66) loss:6.64984
step:2354500 source:(1, 66) loss:8.3369
step:2355000 source:(1, 28) loss:4.94908
step:2355500 source:(1, 61) loss:6.84636
step:2356000 source:(1, 66) loss:6.13612
step:2356500 source:(1, 66) loss:6.67202
step:2357000 source:(1, 61) loss:6.90254
step:2357500 source:(1, 22) loss:6.84753
step:2358000 source:(1, 47) loss:5.86298
step:2358500 source:(1, 66) loss:5.7889
step:2359000 source:(1, 66) loss:8.64433
step:2359500 source:(1, 41) loss:6.43325
step:2360000 source:(1, 66) loss:4.86171
step:2360500 source:(1, 41) loss:5.56221
step:2361000 source:(1, 66) loss:4.97149
step:2361500 source:(1, 66) loss:5.81769
step:2362000 source:(1, 66) loss:5.76285
step:2362500 source:(1, 66) loss:6.6838
step:2363000 source:(1, 38) loss:6.17232
step:2363500 source:(1, 66) loss:6.43293
step:2364000 source:(1, 66) loss:5.95151
step:2364500 source:(1, 56) loss:5.80627
step:2365000 source:(1, 43) loss:6.91946
step:2365500 source:(1, 66) loss:6.07775
step:2366000 source:(1, 41) loss:4.37782
step:2366500 source:(1, 66) loss:6.78444
step:2367000 source:(1, 66) loss:7.04555
step:2367500 source:(1, 66) loss:5.06161
step:2368000 source:(1, 66) loss:6.41967
step:2368500 source:(1, 66) loss:6.67653
step:2369000 source:(1, 66) loss:6.75688
step:2369500 source:(1, 66) loss:6.34613
step:2370000 source:(1, 66) loss:6.42795
step:2370500 source:(1, 42) loss:7.68346
step:2371000 source:(1, 66) loss:6.59416
step:2371500 source:(1, 66) loss:5.1936
step:2372000 source:(1, 66) loss:6.02451
step:2372500 source:(1, 24) loss:6.22004
step:2373000 source:(1, 66) loss:5.75797
step:2373500 source:(1, 42) loss:7.15496
step:2374000 source:(1, 66) loss:4.13297
step:2374500 source:(1, 66) loss:5.5077
step:2375000 source:(1, 27) loss:5.70649
step:2375500 source:(1, 66) loss:6.83743
step:2376000 source:(1, 32) loss:4.99122
step:2376500 source:(1, 49) loss:5.67226
step:2377000 source:(1, 66) loss:4.72025
step:2377500 source:(1, 56) loss:5.69942
step:2378000 source:(1, 66) loss:4.59499
step:2378500 source:(1, 66) loss:5.51332
step:2379000 source:(1, 66) loss:7.39916
step:2379500 source:(1, 66) loss:5.49153
step:2380000 source:(1, 66) loss:6.67952
step:2380500 source:(1, 46) loss:6.9658
step:2381000 source:(1, 66) loss:6.27635
step:2381500 source:(1, 66) loss:5.34484
step:2382000 source:(1, 50) loss:7.56976
step:2382500 source:(1, 44) loss:5.44623
step:2383000 source:(1, 30) loss:8.09436
step:2383500 source:(1, 66) loss:5.97067
step:2384000 source:(1, 66) loss:6.93652
step:2384500 source:(1, 66) loss:5.73219
step:2385000 source:(1, 66) loss:5.94539
step:2385500 source:(1, 23) loss:4.33111
step:2386000 source:(1, 66) loss:5.26796
step:2386500 source:(1, 34) loss:5.52273
step:2387000 source:(1, 66) loss:5.3021
step:2387500 source:(1, 66) loss:6.45608
step:2388000 source:(1, 66) loss:6.12568
step:2388500 source:(1, 46) loss:6.40616
step:2389000 source:(1, 66) loss:5.55539
step:2389500 source:(1, 37) loss:6.32789
step:2390000 source:(1, 66) loss:4.6642
step:2390500 source:(1, 36) loss:4.29597
step:2391000 source:(1, 33) loss:4.56442
step:2391500 source:(1, 55) loss:3.97931
step:2392000 source:(1, 59) loss:5.47893
step:2392500 source:(1, 28) loss:6.1054
step:2393000 source:(1, 33) loss:6.40861
step:2393500 source:(1, 32) loss:6.60342
step:2394000 source:(1, 66) loss:6.17253
step:2394500 source:(1, 53) loss:5.17095
step:2395000 source:(1, 66) loss:5.90195
step:2395500 source:(1, 66) loss:5.69921
step:2396000 source:(1, 66) loss:4.99327
step:2396500 source:(1, 66) loss:6.42894
step:2397000 source:(1, 66) loss:6.28673
step:2397500 source:(1, 26) loss:5.85307
step:2398000 source:(1, 66) loss:5.29619
step:2398500 source:(1, 66) loss:6.34995
step:2399000 source:(1, 66) loss:7.97893
step:2399500 source:(1, 47) loss:5.92244
step:2400000 source:(1, 46) loss:6.30791
step:2400500 source:(1, 66) loss:6.12524
step:2401000 source:(1, 66) loss:6.89582
step:2401500 source:(1, 63) loss:5.78556
step:2402000 source:(1, 31) loss:6.53066
step:2402500 source:(1, 33) loss:5.03172
step:2403000 source:(1, 53) loss:6.55335
step:2403500 source:(1, 66) loss:5.97799
step:2404000 source:(1, 31) loss:7.92409
step:2404500 source:(1, 66) loss:5.52698
step:2405000 source:(1, 31) loss:6.4184
step:2405500 source:(1, 66) loss:8.55012
step:2406000 source:(1, 66) loss:6.25736
step:2406500 source:(1, 66) loss:7.25172
step:2407000 source:(1, 66) loss:4.91249
step:2407500 source:(1, 57) loss:7.3061
step:2408000 source:(1, 24) loss:5.14801
step:2408500 source:(1, 59) loss:5.72063
step:2409000 source:(1, 39) loss:5.1183
step:2409500 source:(1, 35) loss:8.29342
step:2410000 source:(1, 66) loss:5.43212
step:2410500 source:(1, 43) loss:6.21363
step:2411000 source:(1, 54) loss:6.12117
step:2411500 source:(1, 41) loss:5.65332
step:2412000 source:(1, 66) loss:5.70117
step:2412500 source:(1, 66) loss:6.71715
step:2413000 source:(1, 66) loss:6.77807
step:2413500 source:(1, 44) loss:7.58317
step:2414000 source:(1, 66) loss:6.74069
step:2414500 source:(1, 66) loss:4.20405
step:2415000 source:(1, 66) loss:4.72786
step:2415500 source:(1, 51) loss:4.77987
step:2416000 source:(1, 57) loss:7.50355
step:2416500 source:(1, 66) loss:6.66327
step:2417000 source:(1, 34) loss:6.96496
step:2417500 source:(1, 66) loss:5.68265
step:2418000 source:(1, 66) loss:5.96479
step:2418500 source:(1, 35) loss:6.23414
step:2419000 source:(1, 66) loss:8.55838
step:2419500 source:(1, 35) loss:5.74881
step:2420000 source:(1, 66) loss:7.02712
step:2420500 source:(1, 66) loss:5.6276
step:2421000 source:(1, 36) loss:5.59789
step:2421500 source:(1, 66) loss:5.77319
step:2422000 source:(1, 48) loss:6.38885
step:2422500 source:(1, 51) loss:3.96301
step:2423000 source:(1, 66) loss:5.79265
step:2423500 source:(1, 66) loss:6.14411
step:2424000 source:(1, 66) loss:5.97092
step:2424500 source:(1, 66) loss:6.47511
step:2425000 source:(1, 55) loss:4.93621
step:2425500 source:(1, 33) loss:5.72361
step:2426000 source:(1, 66) loss:5.1389
step:2426500 source:(1, 35) loss:6.87122
step:2427000 source:(1, 25) loss:5.56527
step:2427500 source:(1, 30) loss:8.04738
step:2428000 source:(1, 66) loss:4.20773
step:2428500 source:(1, 41) loss:5.77039
step:2429000 source:(1, 59) loss:5.46795
step:2429500 source:(1, 64) loss:4.26339
step:2430000 source:(1, 43) loss:5.05295
step:2430500 source:(1, 66) loss:5.64391
step:2431000 source:(1, 66) loss:6.94774
step:2431500 source:(1, 66) loss:6.14163
step:2432000 source:(1, 39) loss:5.41734
step:2432500 source:(1, 66) loss:5.28051
step:2433000 source:(1, 52) loss:6.68326
step:2433500 source:(1, 66) loss:6.5631
step:2434000 source:(1, 66) loss:6.49921
step:2434500 source:(1, 66) loss:5.13243
step:2435000 source:(1, 66) loss:5.0613
step:2435500 source:(1, 62) loss:5.4216
step:2436000 source:(1, 66) loss:5.97314
step:2436500 source:(1, 66) loss:6.51789
step:2437000 source:(1, 38) loss:6.01755
step:2437500 source:(1, 66) loss:6.39337
step:2438000 source:(1, 30) loss:7.31044
step:2438500 source:(1, 66) loss:6.14625
step:2439000 source:(1, 33) loss:5.33135
step:2439500 source:(1, 66) loss:5.5229
step:2440000 source:(1, 30) loss:7.1612
step:2440500 source:(1, 51) loss:7.38043
step:2441000 source:(1, 66) loss:5.26493
step:2441500 source:(1, 25) loss:6.37046
step:2442000 source:(1, 39) loss:5.96012
step:2442500 source:(1, 59) loss:5.72884
step:2443000 source:(1, 22) loss:6.90469
step:2443500 source:(1, 53) loss:7.26561
step:2444000 source:(1, 66) loss:6.29563
step:2444500 source:(1, 48) loss:5.75606
step:2445000 source:(1, 23) loss:6.44241
step:2445500 source:(1, 52) loss:6.87031
step:2446000 source:(1, 40) loss:5.72849
step:2446500 source:(1, 66) loss:5.5822
step:2447000 source:(1, 66) loss:6.8383
step:2447500 source:(1, 66) loss:5.63701
step:2448000 source:(1, 49) loss:6.07781
step:2448500 source:(1, 27) loss:7.01171
step:2449000 source:(1, 24) loss:6.19638
step:2449500 source:(1, 66) loss:5.98433
step:2450000 source:(1, 58) loss:5.78047
epoch:21 eval_bleu:50.63524842262268
/home/syp/miniconda2/envs/newtf/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
step:2450500 source:(1, 38) loss:5.56284
step:2451000 source:(1, 29) loss:2.85127
step:2451500 source:(1, 32) loss:4.3981
step:2452000 source:(1, 66) loss:5.03938
step:2452500 source:(1, 66) loss:6.21591
step:2453000 source:(1, 65) loss:5.91403
step:2453500 source:(1, 66) loss:6.10394
step:2454000 source:(1, 66) loss:5.74373
step:2454500 source:(1, 66) loss:8.5831
step:2455000 source:(1, 28) loss:5.42976
step:2455500 source:(1, 61) loss:6.65932
step:2456000 source:(1, 66) loss:6.26977
step:2456500 source:(1, 66) loss:5.6961
step:2457000 source:(1, 61) loss:5.9516
step:2457500 source:(1, 22) loss:6.32366
step:2458000 source:(1, 47) loss:6.45661
step:2458500 source:(1, 66) loss:5.22566
step:2459000 source:(1, 66) loss:7.24413
step:2459500 source:(1, 41) loss:5.3878
step:2460000 source:(1, 66) loss:5.92395
step:2460500 source:(1, 41) loss:6.86741
step:2461000 source:(1, 66) loss:5.02881
step:2461500 source:(1, 66) loss:4.59407
step:2462000 source:(1, 66) loss:4.23424
step:2462500 source:(1, 66) loss:5.41365
step:2463000 source:(1, 38) loss:7.12784
step:2463500 source:(1, 66) loss:8.27054
step:2464000 source:(1, 66) loss:5.5439
step:2464500 source:(1, 56) loss:6.71952
step:2465000 source:(1, 43) loss:7.3346
step:2465500 source:(1, 66) loss:6.01337
step:2466000 source:(1, 41) loss:4.49363
step:2466500 source:(1, 66) loss:6.61459
step:2467000 source:(1, 66) loss:7.5157
step:2467500 source:(1, 66) loss:5.94697
step:2468000 source:(1, 66) loss:5.96687
step:2468500 source:(1, 66) loss:5.68284
step:2469000 source:(1, 66) loss:6.91039
step:2469500 source:(1, 66) loss:5.31674
step:2470000 source:(1, 66) loss:6.79664
step:2470500 source:(1, 42) loss:6.5796
step:2471000 source:(1, 66) loss:5.75282
step:2471500 source:(1, 66) loss:4.53582
step:2472000 source:(1, 66) loss:6.58129
step:2472500 source:(1, 24) loss:6.35475
step:2473000 source:(1, 66) loss:6.98036
step:2473500 source:(1, 42) loss:6.70058
step:2474000 source:(1, 66) loss:7.31566
step:2474500 source:(1, 66) loss:6.16661
step:2475000 source:(1, 27) loss:6.07235
step:2475500 source:(1, 66) loss:6.1181
step:2476000 source:(1, 32) loss:5.92442
step:2476500 source:(1, 49) loss:6.82287
step:2477000 source:(1, 66) loss:4.89852
step:2477500 source:(1, 56) loss:6.10612
step:2478000 source:(1, 66) loss:5.25926
step:2478500 source:(1, 66) loss:6.77453
step:2479000 source:(1, 66) loss:5.56276
step:2479500 source:(1, 66) loss:6.17304
step:2480000 source:(1, 66) loss:5.78606
step:2480500 source:(1, 46) loss:6.07468
step:2481000 source:(1, 66) loss:6.86353
step:2481500 source:(1, 66) loss:5.447
step:2482000 source:(1, 50) loss:6.52921
step:2482500 source:(1, 44) loss:5.56899
step:2483000 source:(1, 30) loss:7.80089
step:2483500 source:(1, 66) loss:4.5759
step:2484000 source:(1, 66) loss:6.38155
step:2484500 source:(1, 66) loss:7.30937
step:2485000 source:(1, 66) loss:6.59291
step:2485500 source:(1, 23) loss:4.13755
step:2486000 source:(1, 66) loss:5.38374
step:2486500 source:(1, 34) loss:5.50539
step:2487000 source:(1, 66) loss:5.66009
step:2487500 source:(1, 66) loss:5.36237
step:2488000 source:(1, 66) loss:6.36886
step:2488500 source:(1, 46) loss:6.39601
step:2489000 source:(1, 66) loss:5.90835
step:2489500 source:(1, 37) loss:6.80426
step:2490000 source:(1, 66) loss:6.42369
step:2490500 source:(1, 36) loss:5.3841
step:2491000 source:(1, 33) loss:5.23168
step:2491500 source:(1, 55) loss:3.92359
step:2492000 source:(1, 59) loss:6.12865
step:2492500 source:(1, 28) loss:6.08188
step:2493000 source:(1, 33) loss:5.65443
step:2493500 source:(1, 32) loss:7.07267
step:2494000 source:(1, 66) loss:5.35872
step:2494500 source:(1, 53) loss:5.58036
step:2495000 source:(1, 66) loss:3.84999
step:2495500 source:(1, 66) loss:5.66571
step:2496000 source:(1, 66) loss:5.49023
step:2496500 source:(1, 66) loss:6.24494
step:2497000 source:(1, 66) loss:5.81678
step:2497500 source:(1, 26) loss:4.95921
step:2498000 source:(1, 66) loss:6.59585
step:2498500 source:(1, 66) loss:6.16685
step:2499000 source:(1, 66) loss:6.83784
step:2499500 source:(1, 47) loss:6.50932
step:2500000 source:(1, 46) loss:6.16516
reach max steps:2500000 loss:6.165158748626709
reached max training steps
epoch:22 eval_bleu:52.95292139053345
saving model for max training steps
Traceback (most recent call last):
  File "long_text_gen.py", line 224, in <module>
    tf.app.run(main=_main)
  File "/home/syp/miniconda2/envs/newtf/lib/python3.6/site-packages/tensorflow/python/platform/app.py", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "long_text_gen.py", line 216, in _main
    os.makedirs(args.log_dir + '/max/')
  File "/home/syp/miniconda2/envs/newtf/lib/python3.6/os.py", line 220, in makedirs
    mkdir(name, mode)
FileExistsError: [Errno 17] File exists: './log_dir/bsize32.epoch70.lr_c2warm16000//max/'
